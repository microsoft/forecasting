{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import keras\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import * \n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append TSPerf path to sys.path\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "tsperf_dir = os.path.dirname(os.path.dirname(os.path.dirname(nb_dir)))\n",
    "if tsperf_dir not in sys.path:\n",
    "    sys.path.append(tsperf_dir)\n",
    "\n",
    "from common.metrics import MAPE\n",
    "import retail_sales.OrangeJuice_Pt_3Weeks_Weekly.common.benchmark_settings as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "DATA_DIR = '../../data'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "# Data parameters\n",
    "MAX_STORE_ID = 137\n",
    "MAX_BRAND_ID = 11\n",
    "\n",
    "# Parameters of the model\n",
    "PRED_HORIZON = 3\n",
    "PRED_STEPS = 2\n",
    "SEQ_LEN = 8\n",
    "DYNAMIC_FEATURES = ['deal', 'feat']\n",
    "STATIC_FEATURES = ['store', 'brand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_cartesian_product(dict_in):\n",
    "    \"\"\"Generate a Pandas dataframe from Cartesian product of lists.\n",
    "    \n",
    "    Args: \n",
    "        dict_in (Dictionary): Dictionary containing multiple lists\n",
    "        \n",
    "    Returns:\n",
    "        df (Dataframe): Dataframe corresponding to the Caresian product of the lists\n",
    "    \"\"\"\n",
    "    from collections import OrderedDict\n",
    "    from itertools import product\n",
    "    od = OrderedDict(sorted(dict_in.items()))\n",
    "    cart = list(product(*od.values()))\n",
    "    df = pd.DataFrame(cart, columns=od.keys())\n",
    "    return df\n",
    "\n",
    "def gen_sequence(df, seq_len, seq_cols, start_timestep=0, end_timestep=None):\n",
    "    \"\"\"Reshape features into an array of dimension (time steps, features).  \n",
    "    \n",
    "    Args:\n",
    "        df (Dataframe): Time series data of a specific (store, brand) combination\n",
    "        seq_len (Integer): The number of previous time series values to use as input features\n",
    "        seq_cols (List): A list of names of the feature columns \n",
    "        start_timestep (Integer): First time step you can use to create feature sequences\n",
    "        end_timestep (Integer): Last time step you can use to create feature sequences\n",
    "        \n",
    "    Returns:\n",
    "        A generator object for iterating all the feature sequences\n",
    "    \"\"\"\n",
    "    data_array = df[seq_cols].values\n",
    "    if end_timestep is None:\n",
    "        end_timestep = df.shape[0]\n",
    "    for start, stop in zip(range(start_timestep, end_timestep-seq_len+1), range(start_timestep+seq_len, end_timestep+1)):\n",
    "        yield data_array[start:stop, :]\n",
    "\n",
    "\n",
    "def gen_sequence_array(df_all, seq_len, seq_cols, start_timestep=0, end_timestep=None):\n",
    "    \"\"\"Combine feature sequences for all the combinations of (store, brand) into an 3d array.\n",
    "    \n",
    "    Args:\n",
    "        df_all (Dataframe): Time series data of all stores and brands\n",
    "        seq_len (Integer): The number of previous time series values to use as input features\n",
    "        seq_cols (List): A list of names of the feature columns \n",
    "        start_timestep (Integer): First time step you can use to create feature sequences\n",
    "        end_timestep (Integer): Last time step you can use to create feature sequences\n",
    "        \n",
    "    Returns:\n",
    "        seq_array (Numpy Array): An array of the feature sequences of all stores and brands    \n",
    "    \"\"\"\n",
    "    seq_gen = (list(gen_sequence(df_all[(df_all['store']==cur_store) & (df_all['brand']==cur_brand)], \\\n",
    "                                 seq_len, seq_cols, start_timestep, end_timestep)) \\\n",
    "              for cur_store, cur_brand in itertools.product(df_all['store'].unique(), df_all['brand'].unique()))\n",
    "    seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "    return seq_array\n",
    "\n",
    "def static_feature_array(df_all, total_timesteps, seq_cols):\n",
    "    \"\"\"Generate an arary which encodes all the static features.\n",
    "    \"\"\"\n",
    "    fea_df = data_filled.groupby(['store', 'brand']). \\\n",
    "                         apply(lambda x: x.iloc[:total_timesteps,:]). \\\n",
    "                         reset_index(drop=True)\n",
    "    fea_array = fea_df[seq_cols].values\n",
    "    return fea_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Round 1 ----\n",
      "   store  brand  week  constant    price1    price2    price3    price4  \\\n",
      "0      2      1    40         1  0.060469  0.060497  0.042031  0.029531   \n",
      "1      2      1    46         1  0.060469  0.060312  0.045156  0.046719   \n",
      "2      2      1    47         1  0.060469  0.060312  0.045156  0.046719   \n",
      "\n",
      "     price5    price6    price7    price8    price9   price10   price11  deal  \\\n",
      "0  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844  0.038984     1   \n",
      "1  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031  0.038984     0   \n",
      "2  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656  0.038984     0   \n",
      "\n",
      "   feat     profit  move  \n",
      "0   0.0  37.992326  8256  \n",
      "1   0.0  30.126667  6144  \n",
      "2   0.0  30.000000  3840  \n",
      "\n",
      "Number of missing rows is 6204\n",
      "\n",
      "   brand  store  week  constant    price1    price2    price3    price4  \\\n",
      "0      1      2    40       1.0  0.060469  0.060497  0.042031  0.029531   \n",
      "1      1      2    41       1.0  0.060469  0.060497  0.042031  0.029531   \n",
      "2      1      2    42       1.0  0.060469  0.060497  0.042031  0.029531   \n",
      "\n",
      "     price5    price6    price7    price8    price9   price10   price11  deal  \\\n",
      "0  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844  0.038984   1.0   \n",
      "1  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844  0.038984   1.0   \n",
      "2  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844  0.038984   1.0   \n",
      "\n",
      "   feat     profit    move  \n",
      "0   0.0  37.992326  8256.0  \n",
      "1   0.0  37.992326  8256.0  \n",
      "2   0.0  37.992326  8256.0  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = 0\n",
    "print('---- Round ' + str(r+1) + ' ----')\n",
    "train_df = pd.read_csv(os.path.join(TRAIN_DIR, 'train_round_'+str(r+1)+'.csv'))\n",
    "train_df['move'] = train_df['logmove'].apply(lambda x: round(math.exp(x)))\n",
    "train_df.drop('logmove', axis=1, inplace=True)\n",
    "print(train_df.head(3))\n",
    "print('')\n",
    "# Fill missing values\n",
    "store_list = train_df['store'].unique()\n",
    "brand_list = train_df['brand'].unique()\n",
    "week_list = range(bs.TRAIN_START_WEEK, bs.TEST_END_WEEK_LIST[r]+1)\n",
    "d = {'store': store_list,\n",
    "     'brand': brand_list,\n",
    "     'week': week_list}        \n",
    "data_grid = df_from_cartesian_product(d)\n",
    "data_filled = pd.merge(data_grid, train_df, how='left', \n",
    "                        on=['store', 'brand', 'week'])\n",
    "print('Number of missing rows is {}'.format(data_filled[data_filled.isnull().any(axis=1)].shape[0]))\n",
    "print('')\n",
    "data_filled = data_filled.groupby(['store', 'brand']). \\\n",
    "                          apply(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "print(data_filled.head(3))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>constant</th>\n",
       "      <th>deal</th>\n",
       "      <th>feat</th>\n",
       "      <th>move</th>\n",
       "      <th>price1</th>\n",
       "      <th>price10</th>\n",
       "      <th>price11</th>\n",
       "      <th>price2</th>\n",
       "      <th>price3</th>\n",
       "      <th>price4</th>\n",
       "      <th>price5</th>\n",
       "      <th>price6</th>\n",
       "      <th>price7</th>\n",
       "      <th>price8</th>\n",
       "      <th>price9</th>\n",
       "      <th>profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.493088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>0.686433</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.485356</td>\n",
       "      <td>0.38567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.493088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>0.686433</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.485356</td>\n",
       "      <td>0.38567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.493088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>0.686433</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.485356</td>\n",
       "      <td>0.38567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.493088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>0.686433</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.485356</td>\n",
       "      <td>0.38567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.493088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>0.686433</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.485356</td>\n",
       "      <td>0.38567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   constant  deal  feat      move  price1   price10  price11    price2  \\\n",
       "0       0.0   1.0   0.0  0.011436     1.0  0.493088      1.0  0.993939   \n",
       "1       0.0   1.0   0.0  0.011436     1.0  0.493088      1.0  0.993939   \n",
       "2       0.0   1.0   0.0  0.011436     1.0  0.493088      1.0  0.993939   \n",
       "3       0.0   1.0   0.0  0.011436     1.0  0.493088      1.0  0.993939   \n",
       "4       0.0   1.0   0.0  0.011436     1.0  0.493088      1.0  0.993939   \n",
       "\n",
       "     price3    price4  price5  price6    price7    price8    price9   profit  \n",
       "0  0.686433  0.434783     1.0     1.0  0.731481  0.977528  0.485356  0.38567  \n",
       "1  0.686433  0.434783     1.0     1.0  0.731481  0.977528  0.485356  0.38567  \n",
       "2  0.686433  0.434783     1.0     1.0  0.731481  0.977528  0.485356  0.38567  \n",
       "3  0.686433  0.434783     1.0     1.0  0.731481  0.977528  0.485356  0.38567  \n",
       "4  0.686433  0.434783     1.0     1.0  0.731481  0.977528  0.485356  0.38567  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the dataframe of features\n",
    "cols_normalize = data_filled.columns.difference(['store','brand','week'])\n",
    "min_max_scaler = MinMaxScaler()\n",
    "data_filled_scaled = pd.DataFrame(min_max_scaler.fit_transform(data_filled[cols_normalize]), \n",
    "                                  columns=cols_normalize, \n",
    "                                  index=data_filled.index)\n",
    "data_filled_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 19)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sub = data_filled[(data_filled.brand==1) & (data_filled.store==2)]\n",
    "data_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78518, 8, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_timestep = 0\n",
    "end_timestep = bs.TRAIN_END_WEEK_LIST[r]-bs.TRAIN_START_WEEK-PRED_HORIZON+1\n",
    "\n",
    "train_input1 = gen_sequence_array(data_filled, SEQ_LEN, ['move'], start_timestep, end_timestep)\n",
    "\n",
    "train_input1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78518, 8, 2)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_timestep = PRED_HORIZON\n",
    "end_timestep = bs.TRAIN_END_WEEK_LIST[r]-bs.TRAIN_START_WEEK+1\n",
    "\n",
    "train_input2 = gen_sequence_array(data_filled, SEQ_LEN, DYNAMIC_FEATURES, start_timestep, end_timestep)\n",
    "\n",
    "train_input2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78518, 8, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = np.concatenate((train_input1, train_input2), axis=2)\n",
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78518, 2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_output\n",
    "start_timestep = SEQ_LEN+1\n",
    "end_timestep = bs.TRAIN_END_WEEK_LIST[r]-bs.TRAIN_START_WEEK+1\n",
    "train_output = gen_sequence_array(data_filled, PRED_STEPS, ['move'], start_timestep, end_timestep)\n",
    "train_output = np.squeeze(train_output)\n",
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78518, 2)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_col = np.full([train_input1.shape[0],1], 1)\n",
    "brand_col = np.full([train_input1.shape[0],1], 2)\n",
    "np.concatenate((store_col, brand_col), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.TRAIN_END_WEEK_LIST[r]-bs.TRAIN_START_WEEK-sequence_len+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.73084502,\n",
       "        0.11508554],\n",
       "       [0.        , 0.        , 0.01020408, ..., 0.        , 0.73084502,\n",
       "        0.11508554],\n",
       "       [0.        , 0.        , 0.02040816, ..., 0.        , 0.73084502,\n",
       "        0.11508554],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.97959184, ..., 0.        , 0.3278259 ,\n",
       "        0.21617418],\n",
       "       [0.        , 0.        , 0.98979592, ..., 0.        , 0.3278259 ,\n",
       "        0.21617418],\n",
       "       [0.        , 0.        , 1.        , ..., 0.        , 0.3278259 ,\n",
       "        0.21617418]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_sub)\n",
    "data_sub_scaled = scaler.transform(data_sub)\n",
    "data_sub_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       1.        , 0.62921348, 0.25477707, 1.        , 1.        ,\n",
       "       0.70103093, 0.97435897, 0.41148325, 0.35294118, 1.        ,\n",
       "       1.        , 0.        , 0.73084502, 0.11508554])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sub_scaled[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 8, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = gen_sequence_array(data_sub, sequence_length, sequence_cols)\n",
    "sa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = bs.TRAIN_END_WEEK_LIST[r]-bs.TRAIN_START_WEEK-SEQ_LEN-PRED_HORIZON+2\n",
    "train_input2 = static_feature_array(data_filled, total_timesteps, ['store', 'brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78518, 2)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "def create_dcnn_model(seq_len, kernel_size=2, n_filters=4, n_input_series=1, n_outputs=1):\n",
    "    # Sequential input\n",
    "    seq_in = Input(shape=(seq_len, n_input_series), name='input_1')\n",
    "    # Categorical input\n",
    "    cat_fea_in = Input(shape=(2,), dtype='uint8', name='input_2')\n",
    "    store_id = Lambda(lambda x: x[:, 0, None])(cat_fea_in)\n",
    "    brand_id = Lambda(lambda x: x[:, 1, None])(cat_fea_in)\n",
    "    store_embed = Embedding(MAX_STORE_ID+1, 7, input_length=1)(store_id)\n",
    "    brand_embed = Embedding(MAX_BRAND_ID+1, 4, input_length=1)(brand_id)\n",
    "    \n",
    "    # Dilated convolutional layers\n",
    "    c1 = Conv1D(filters=n_filters, kernel_size=kernel_size, dilation_rate=1, \n",
    "                padding='causal', activation='relu', name='conv1d_1')(seq_in)\n",
    "    c2 = Conv1D(filters=n_filters, kernel_size=kernel_size, dilation_rate=2, \n",
    "                padding='causal', activation='relu', name='conv1d_2')(c1)\n",
    "    c3 = Conv1D(filters=n_filters, kernel_size=kernel_size, dilation_rate=4, \n",
    "                padding='causal', activation='relu', name='conv1d_3')(c2)\n",
    "    # Skip connections\n",
    "    c4 = concatenate([c1, c3])\n",
    "    # Output of convolutional layers \n",
    "    conv_out = Conv1D(8, 1, activation='relu')(c4)\n",
    "    conv_out = Dropout(0.25)(conv_out)\n",
    "    conv_out = Flatten()(conv_out)\n",
    "    \n",
    "    # Concatenate with categorical features\n",
    "    x = concatenate([conv_out, Flatten()(store_embed), Flatten()(brand_embed)])\n",
    "    #x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(n_outputs, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=[seq_in, cat_fea_in], outputs=output)\n",
    "    adam = optimizers.Adam(lr=0.01)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 8, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 8, 4)         12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 8, 4)         36          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 8, 4)         36          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8)         0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 8, 8)         72          concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 1)            0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 1)            0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 8, 8)         0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 1, 7)         966         lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 1, 4)         48          lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 64)           0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 7)            0           embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 4)            0           embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 75)           0           flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "                                                                 flatten_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 64)           4864        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 64)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 32)           2080        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 2)            66          dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,180\n",
      "Trainable params: 8,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_dcnn_model(seq_len=SEQ_LEN, n_outputs=PRED_STEPS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78518 samples, validate on 78518 samples\n",
      "Epoch 1/25\n",
      "78518/78518 [==============================] - 133s 2ms/step - loss: 386293512.0778 - mean_absolute_error: 8087.8466 - val_loss: 374295882.4036 - val_mean_absolute_error: 7858.6402\n",
      "Epoch 2/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 378488205.3290 - mean_absolute_error: 7961.6788 - val_loss: 373397352.7377 - val_mean_absolute_error: 7849.8510\n",
      "Epoch 3/25\n",
      "78518/78518 [==============================] - 133s 2ms/step - loss: 377414270.3836 - mean_absolute_error: 7909.0688 - val_loss: 376472132.8982 - val_mean_absolute_error: 7422.8482\n",
      "Epoch 4/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 376378657.4870 - mean_absolute_error: 7900.8502 - val_loss: 378515494.8343 - val_mean_absolute_error: 7220.1104\n",
      "Epoch 5/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 375873679.0266 - mean_absolute_error: 7861.7805 - val_loss: 386010663.2796 - val_mean_absolute_error: 7027.4053\n",
      "Epoch 6/25\n",
      "78518/78518 [==============================] - 133s 2ms/step - loss: 375240348.4050 - mean_absolute_error: 7864.7784 - val_loss: 382448148.4892 - val_mean_absolute_error: 7008.8999\n",
      "Epoch 7/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 374340559.5836 - mean_absolute_error: 7864.7836 - val_loss: 376293203.8697 - val_mean_absolute_error: 7142.7492\n",
      "Epoch 8/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 374832763.6571 - mean_absolute_error: 7891.0146 - val_loss: 378109925.6912 - val_mean_absolute_error: 7088.6418\n",
      "Epoch 9/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 373905533.9406 - mean_absolute_error: 7887.4436 - val_loss: 379338443.8989 - val_mean_absolute_error: 7029.6998\n",
      "Epoch 10/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 374827880.4667 - mean_absolute_error: 7836.7817 - val_loss: 372654305.0220 - val_mean_absolute_error: 7177.1328\n",
      "Epoch 11/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 372842536.2809 - mean_absolute_error: 7796.8387 - val_loss: 374284267.5426 - val_mean_absolute_error: 7103.9646\n",
      "Epoch 12/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 372904233.3724 - mean_absolute_error: 7799.7770 - val_loss: 382036128.4855 - val_mean_absolute_error: 6727.6834\n",
      "Epoch 13/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 373165798.6954 - mean_absolute_error: 7776.8522 - val_loss: 374636732.5426 - val_mean_absolute_error: 7062.9692\n",
      "Epoch 14/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 372713932.9769 - mean_absolute_error: 7784.3286 - val_loss: 372542927.3464 - val_mean_absolute_error: 7163.7675\n",
      "Epoch 15/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 372627624.2712 - mean_absolute_error: 7775.4448 - val_loss: 380599769.5342 - val_mean_absolute_error: 6986.7845\n",
      "Epoch 16/25\n",
      "78518/78518 [==============================] - 133s 2ms/step - loss: 372915734.1477 - mean_absolute_error: 7778.9765 - val_loss: 375991420.5629 - val_mean_absolute_error: 7303.9684\n",
      "Epoch 17/25\n",
      "78518/78518 [==============================] - 133s 2ms/step - loss: 372037804.6605 - mean_absolute_error: 7763.7428 - val_loss: 373232721.6625 - val_mean_absolute_error: 7296.1479\n",
      "Epoch 18/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 371884164.2826 - mean_absolute_error: 7775.8774 - val_loss: 372443901.9346 - val_mean_absolute_error: 7070.9893\n",
      "Epoch 19/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 371743094.8000 - mean_absolute_error: 7782.3006 - val_loss: 373037879.4233 - val_mean_absolute_error: 7155.2981\n",
      "Epoch 20/25\n",
      "78518/78518 [==============================] - 133s 2ms/step - loss: 372242769.9491 - mean_absolute_error: 7765.5824 - val_loss: 372672354.1129 - val_mean_absolute_error: 7183.5290\n",
      "Epoch 21/25\n",
      "78518/78518 [==============================] - 133s 2ms/step - loss: 372434204.5997 - mean_absolute_error: 7745.0527 - val_loss: 375440966.3196 - val_mean_absolute_error: 7093.8917\n",
      "Epoch 22/25\n",
      "78518/78518 [==============================] - 133s 2ms/step - loss: 371717314.1502 - mean_absolute_error: 7743.1649 - val_loss: 370800542.7543 - val_mean_absolute_error: 7195.4007\n",
      "Epoch 23/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 371767363.0636 - mean_absolute_error: 7748.9430 - val_loss: 373500449.8376 - val_mean_absolute_error: 7198.8633\n",
      "Epoch 24/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 372383249.6081 - mean_absolute_error: 7744.5226 - val_loss: 370028823.1909 - val_mean_absolute_error: 7359.4896\n",
      "Epoch 25/25\n",
      "78518/78518 [==============================] - 132s 2ms/step - loss: 372735835.3348 - mean_absolute_error: 7741.3913 - val_loss: 373244022.6506 - val_mean_absolute_error: 7064.1489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75b850a470>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_input1, train_input2], train_output, nb_epoch=25, batch_size=2, validation_data=([train_input1, train_input2], train_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9905.389"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using single GPU or CPU..\n",
      "Train on 78518 samples, validate on 78518 samples\n",
      "Epoch 1/5\n",
      "78518/78518 [==============================] - 129s 2ms/step - loss: 371783279.8227 - mean_absolute_error: 7720.2254 - val_loss: 375786319.9688 - val_mean_absolute_error: 6926.7757\n",
      "Epoch 2/5\n",
      "78518/78518 [==============================] - 130s 2ms/step - loss: 371858775.8101 - mean_absolute_error: 7743.6835 - val_loss: 374005735.5173 - val_mean_absolute_error: 7079.4063\n",
      "Epoch 3/5\n",
      "78518/78518 [==============================] - 130s 2ms/step - loss: 371711579.5102 - mean_absolute_error: 7737.9117 - val_loss: 374819726.8688 - val_mean_absolute_error: 6959.9123\n",
      "Epoch 4/5\n",
      "78518/78518 [==============================] - 130s 2ms/step - loss: 372115131.8486 - mean_absolute_error: 7743.9186 - val_loss: 372810391.6923 - val_mean_absolute_error: 7005.5653\n",
      "Epoch 5/5\n",
      "78518/78518 [==============================] - 130s 2ms/step - loss: 372776029.6781 - mean_absolute_error: 7742.7033 - val_loss: 374814529.7087 - val_mean_absolute_error: 6901.9189\n",
      "648.030503988266\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "try:\n",
    "    model = multi_gpu_model(model, cpu_merge=False)\n",
    "    print(\"Training using multiple GPUs..\")\n",
    "except:\n",
    "    print(\"Training using single GPU or CPU..\")\n",
    "start_time = time.time()\n",
    "model.fit([train_input1, train_input2], train_output, epochs=5, batch_size=2, validation_data=([train_input1, train_input2], train_output))\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
