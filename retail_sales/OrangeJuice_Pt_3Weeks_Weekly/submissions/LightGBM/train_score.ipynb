{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and score a boosted decision tree model using [LightGBM Python package](https://github.com/Microsoft/LightGBM) from Microsoft, which is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append TSPerf path to sys.path\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "tsperf_dir = os.path.dirname(os.path.dirname(os.path.dirname(nb_dir)))\n",
    "if tsperf_dir not in sys.path:\n",
    "    sys.path.append(tsperf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.metrics import MAPE\n",
    "import retail_sales.OrangeJuice_Pt_3Weeks_Weekly.common.benchmark_settings as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Define parameters\n",
    "# NUM_ROUNDS = 12\n",
    "# TRAIN_START_WEEK = 40\n",
    "# TRAIN_END_WEEK_LIST = range(135, 159, 2)\n",
    "# TEST_START_WEEK_LIST = range(137, 161, 2)\n",
    "# TEST_END_WEEK_LIST = range(138, 162, 2)\n",
    "DATA_DIR = '../../data'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "def df_from_cartesian_product(dict_in):\n",
    "    \"\"\"Generate a Pandas dataframe from Cartesian product of lists\n",
    "    \n",
    "    Args: \n",
    "        dict_in: Dictionary containing multiple lists\n",
    "        \n",
    "    Returns:\n",
    "        df: Pandas dataframe corresponding to the Caresian product of the lists\n",
    "    \"\"\"\n",
    "    from collections import OrderedDict\n",
    "    from itertools import product\n",
    "    od = OrderedDict(sorted(dict_in.items()))\n",
    "    cart = list(product(*od.values()))\n",
    "    df = pd.DataFrame(cart, columns=od.keys())\n",
    "    return df\n",
    "\n",
    "def lagged_features(df, lags):\n",
    "    \"\"\"Create lagged features based on time series data\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Input time series data sorted by time\n",
    "        lags (list): Lag lengths\n",
    "        \n",
    "    Returns:\n",
    "        fea (dataframe): Lagged features \n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for lag in lags:\n",
    "        df_shifted = df.shift(lag)\n",
    "        df_shifted.columns = [x + '_lag' + str(lag) for x in df_shifted.columns]\n",
    "        df_list.append(df_shifted)\n",
    "    fea = pd.concat(df_list, axis=1)\n",
    "    return fea\n",
    "\n",
    "def create_features(df):\n",
    "    lagged_fea = lagged_features(df.drop(['brand','store','week'], axis=1), lags)\n",
    "    return pd.concat([df[['brand' , 'store', 'week', 'logmove']], lagged_fea], axis=1)\n",
    "\n",
    "def make_predictions(df, model):\n",
    "    predictions = pd.DataFrame({'logmove': model.predict(df, num_iteration=bst.best_iteration or MAX_ROUNDS)})\n",
    "    return pd.concat([df[['brand', 'store', 'week']].reset_index(drop=True), predictions], axis=1)\n",
    "\n",
    "def evaluate(pred, actual):\n",
    "    pred['pred_units'] = pred['logmove'].apply(lambda x: round(math.exp(x)))\n",
    "    actual['actual_units'] = actual['logmove'].apply(lambda x: round(math.exp(x)))\n",
    "    return MAPE(pred['pred_units'], actual['actual_units'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "params = {\n",
    "    'num_leaves': 80,\n",
    "    'objective': 'regression',\n",
    "    'min_data_in_leaf': 100, #200,\n",
    "    'learning_rate': 0.1, #0.02,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'l2',\n",
    "    'num_threads': 16\n",
    "}\n",
    "MAX_ROUNDS = 5000 #5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lags = [2] #[2,3,4]\n",
    "lags_str = [str(x) for x in lags]\n",
    "categ_fea = [\"\".join(res) for res in itertools.product(['deal_lag', 'feat_lag'], lags_str)]\n",
    "categ_fea = ['store', 'brand'] + categ_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Round 1 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6204\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/lightgbm/basic.py:988: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['brand', 'deal_lag2', 'feat_lag2', 'store']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.266488\n",
      "[200]\ttraining's l2: 0.19229\n",
      "[300]\ttraining's l2: 0.158313\n",
      "[400]\ttraining's l2: 0.137848\n",
      "[500]\ttraining's l2: 0.122654\n",
      "[600]\ttraining's l2: 0.112029\n",
      "[700]\ttraining's l2: 0.102161\n",
      "[800]\ttraining's l2: 0.0944019\n",
      "[900]\ttraining's l2: 0.0876973\n",
      "[1000]\ttraining's l2: 0.0821083\n",
      "[1100]\ttraining's l2: 0.0771434\n",
      "[1200]\ttraining's l2: 0.0728598\n",
      "[1300]\ttraining's l2: 0.0690778\n",
      "[1400]\ttraining's l2: 0.0656885\n",
      "[1500]\ttraining's l2: 0.0626349\n",
      "[1600]\ttraining's l2: 0.0598821\n",
      "[1700]\ttraining's l2: 0.0572827\n",
      "[1800]\ttraining's l2: 0.0549692\n",
      "[1900]\ttraining's l2: 0.0528099\n",
      "[2000]\ttraining's l2: 0.0508134\n",
      "[2100]\ttraining's l2: 0.0489353\n",
      "[2200]\ttraining's l2: 0.0471932\n",
      "[2300]\ttraining's l2: 0.0456095\n",
      "[2400]\ttraining's l2: 0.0440873\n",
      "[2500]\ttraining's l2: 0.0426476\n",
      "[2600]\ttraining's l2: 0.0413572\n",
      "[2700]\ttraining's l2: 0.0401451\n",
      "[2800]\ttraining's l2: 0.0389514\n",
      "[2900]\ttraining's l2: 0.0378191\n",
      "[3000]\ttraining's l2: 0.0367658\n",
      "[3100]\ttraining's l2: 0.0357497\n",
      "[3200]\ttraining's l2: 0.0347654\n",
      "[3300]\ttraining's l2: 0.0338339\n",
      "[3400]\ttraining's l2: 0.0329159\n",
      "[3500]\ttraining's l2: 0.0320624\n",
      "[3600]\ttraining's l2: 0.0312617\n",
      "[3700]\ttraining's l2: 0.0305011\n",
      "[3800]\ttraining's l2: 0.0297848\n",
      "[3900]\ttraining's l2: 0.0290716\n",
      "[4000]\ttraining's l2: 0.0284112\n",
      "[4100]\ttraining's l2: 0.0277635\n",
      "[4200]\ttraining's l2: 0.027139\n",
      "[4300]\ttraining's l2: 0.0265626\n",
      "[4400]\ttraining's l2: 0.0259884\n",
      "[4500]\ttraining's l2: 0.0254483\n",
      "[4600]\ttraining's l2: 0.0249234\n",
      "[4700]\ttraining's l2: 0.0244249\n",
      "[4800]\ttraining's l2: 0.0239198\n",
      "[4900]\ttraining's l2: 0.0234568\n",
      "[5000]\ttraining's l2: 0.0229778\n",
      "      brand  store  week   logmove\n",
      "0         1      2   137  8.752499\n",
      "1         1      2   138  8.752499\n",
      "2         2      2   137  8.839296\n",
      "3         2      2   138  8.839296\n",
      "4         3      2   137  7.708008\n",
      "5         3      2   138  7.708008\n",
      "6         4      2   137  9.626168\n",
      "7         4      2   138  9.626168\n",
      "8         5      2   137  8.788684\n",
      "9         5      2   138  8.788684\n",
      "10        6      2   137  7.414288\n",
      "11        6      2   138  7.414288\n",
      "12        7      2   137  7.566473\n",
      "13        7      2   138  7.566473\n",
      "14        8      2   137  7.346101\n",
      "15        8      2   138  7.346101\n",
      "16        9      2   137  7.783215\n",
      "17        9      2   138  7.783215\n",
      "18       10      2   137  9.201576\n",
      "19       10      2   138  9.201576\n",
      "20       11      2   137  8.828379\n",
      "21       11      2   138  8.828379\n",
      "22        1      5   137  9.005968\n",
      "23        1      5   138  9.005968\n",
      "24        2      5   137  8.980986\n",
      "25        2      5   138  8.980986\n",
      "26        3      5   137  8.155858\n",
      "27        3      5   138  8.155858\n",
      "28        4      5   137  9.590604\n",
      "29        4      5   138  9.590604\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   137  6.566221\n",
      "1797      8    134   138  6.566221\n",
      "1798      9    134   137  7.566476\n",
      "1799      9    134   138  7.566476\n",
      "1800     10    134   137  8.823543\n",
      "1801     10    134   138  8.823543\n",
      "1802     11    134   137  9.044830\n",
      "1803     11    134   138  9.044830\n",
      "1804      1    137   137  9.674182\n",
      "1805      1    137   138  9.674182\n",
      "1806      2    137   137  9.338666\n",
      "1807      2    137   138  9.338666\n",
      "1808      3    137   137  8.072473\n",
      "1809      3    137   138  8.072473\n",
      "1810      4    137   137  9.745859\n",
      "1811      4    137   138  9.745859\n",
      "1812      5    137   137  9.153879\n",
      "1813      5    137   138  9.153879\n",
      "1814      6    137   137  8.228406\n",
      "1815      6    137   138  8.228406\n",
      "1816      7    137   137  8.217851\n",
      "1817      7    137   138  8.217851\n",
      "1818      8    137   137  7.475456\n",
      "1819      8    137   138  7.475456\n",
      "1820      9    137   137  7.599235\n",
      "1821      9    137   138  7.599235\n",
      "1822     10    137   137  9.525244\n",
      "1823     10    137   138  9.525244\n",
      "1824     11    137   137  9.272826\n",
      "1825     11    137   138  9.272826\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 78.2321096879806\n",
      "\n",
      "---- Round 2 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6215\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/lightgbm/basic.py:988: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['brand', 'deal_lag2', 'feat_lag2', 'store']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.266806\n",
      "[200]\ttraining's l2: 0.192031\n",
      "[300]\ttraining's l2: 0.157181\n",
      "[400]\ttraining's l2: 0.13667\n",
      "[500]\ttraining's l2: 0.121956\n",
      "[600]\ttraining's l2: 0.111358\n",
      "[700]\ttraining's l2: 0.101954\n",
      "[800]\ttraining's l2: 0.0940483\n",
      "[900]\ttraining's l2: 0.0874615\n",
      "[1000]\ttraining's l2: 0.0820078\n",
      "[1100]\ttraining's l2: 0.0770957\n",
      "[1200]\ttraining's l2: 0.0728044\n",
      "[1300]\ttraining's l2: 0.069053\n",
      "[1400]\ttraining's l2: 0.0656927\n",
      "[1500]\ttraining's l2: 0.0626422\n",
      "[1600]\ttraining's l2: 0.0599039\n",
      "[1700]\ttraining's l2: 0.0573587\n",
      "[1800]\ttraining's l2: 0.0550271\n",
      "[1900]\ttraining's l2: 0.0528682\n",
      "[2000]\ttraining's l2: 0.0508982\n",
      "[2100]\ttraining's l2: 0.0490132\n",
      "[2200]\ttraining's l2: 0.0472384\n",
      "[2300]\ttraining's l2: 0.0456362\n",
      "[2400]\ttraining's l2: 0.0440776\n",
      "[2500]\ttraining's l2: 0.042674\n",
      "[2600]\ttraining's l2: 0.0413326\n",
      "[2700]\ttraining's l2: 0.0400966\n",
      "[2800]\ttraining's l2: 0.0389417\n",
      "[2900]\ttraining's l2: 0.0378183\n",
      "[3000]\ttraining's l2: 0.0367726\n",
      "[3100]\ttraining's l2: 0.0358086\n",
      "[3200]\ttraining's l2: 0.0348266\n",
      "[3300]\ttraining's l2: 0.0339081\n",
      "[3400]\ttraining's l2: 0.0330447\n",
      "[3500]\ttraining's l2: 0.0322076\n",
      "[3600]\ttraining's l2: 0.03141\n",
      "[3700]\ttraining's l2: 0.0306389\n",
      "[3800]\ttraining's l2: 0.0299028\n",
      "[3900]\ttraining's l2: 0.0292353\n",
      "[4000]\ttraining's l2: 0.0285757\n",
      "[4100]\ttraining's l2: 0.027934\n",
      "[4200]\ttraining's l2: 0.0273219\n",
      "[4300]\ttraining's l2: 0.0267392\n",
      "[4400]\ttraining's l2: 0.0261754\n",
      "[4500]\ttraining's l2: 0.0256142\n",
      "[4600]\ttraining's l2: 0.0251001\n",
      "[4700]\ttraining's l2: 0.024619\n",
      "[4800]\ttraining's l2: 0.0241047\n",
      "[4900]\ttraining's l2: 0.0236347\n",
      "[5000]\ttraining's l2: 0.0231648\n",
      "      brand  store  week   logmove\n",
      "0         1      2   139  8.547048\n",
      "1         1      2   140  8.547048\n",
      "2         2      2   139  8.911706\n",
      "3         2      2   140  8.911706\n",
      "4         3      2   139  7.634681\n",
      "5         3      2   140  7.634681\n",
      "6         4      2   139  7.902951\n",
      "7         4      2   140  7.902951\n",
      "8         5      2   139  8.623374\n",
      "9         5      2   140  8.623374\n",
      "10        6      2   139  8.347332\n",
      "11        6      2   140  8.347332\n",
      "12        7      2   139  7.216214\n",
      "13        7      2   140  7.216214\n",
      "14        8      2   139  7.563365\n",
      "15        8      2   140  7.563365\n",
      "16        9      2   139  7.759967\n",
      "17        9      2   140  7.759967\n",
      "18       10      2   139  8.088385\n",
      "19       10      2   140  8.088385\n",
      "20       11      2   139  7.745976\n",
      "21       11      2   140  7.745976\n",
      "22        1      5   139  8.895038\n",
      "23        1      5   140  8.895038\n",
      "24        2      5   139  9.231881\n",
      "25        2      5   140  9.231881\n",
      "26        3      5   139  7.949432\n",
      "27        3      5   140  7.949432\n",
      "28        4      5   139  7.636602\n",
      "29        4      5   140  7.636602\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   139  6.224947\n",
      "1797      8    134   140  6.224947\n",
      "1798      9    134   139  7.088386\n",
      "1799      9    134   140  7.088386\n",
      "1800     10    134   139  8.420583\n",
      "1801     10    134   140  8.420583\n",
      "1802     11    134   139  8.740758\n",
      "1803     11    134   140  8.740758\n",
      "1804      1    137   139  9.522415\n",
      "1805      1    137   140  9.522415\n",
      "1806      2    137   139  9.708500\n",
      "1807      2    137   140  9.708500\n",
      "1808      3    137   139  7.929205\n",
      "1809      3    137   140  7.929205\n",
      "1810      4    137   139  8.048497\n",
      "1811      4    137   140  8.048497\n",
      "1812      5    137   139  9.305704\n",
      "1813      5    137   140  9.305704\n",
      "1814      6    137   139  8.729130\n",
      "1815      6    137   140  8.729130\n",
      "1816      7    137   139  7.786111\n",
      "1817      7    137   140  7.786111\n",
      "1818      8    137   139  7.285984\n",
      "1819      8    137   140  7.285984\n",
      "1820      9    137   139  7.685598\n",
      "1821      9    137   140  7.685598\n",
      "1822     10    137   139  8.823619\n",
      "1823     10    137   140  8.823619\n",
      "1824     11    137   139  8.493598\n",
      "1825     11    137   140  8.493598\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 76.06054868990037\n",
      "\n",
      "---- Round 3 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6237\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.272807\n",
      "[200]\ttraining's l2: 0.196774\n",
      "[300]\ttraining's l2: 0.161289\n",
      "[400]\ttraining's l2: 0.139701\n",
      "[500]\ttraining's l2: 0.124203\n",
      "[600]\ttraining's l2: 0.113421\n",
      "[700]\ttraining's l2: 0.103629\n",
      "[800]\ttraining's l2: 0.0955162\n",
      "[900]\ttraining's l2: 0.0887797\n",
      "[1000]\ttraining's l2: 0.0832212\n",
      "[1100]\ttraining's l2: 0.0782626\n",
      "[1200]\ttraining's l2: 0.0737725\n",
      "[1300]\ttraining's l2: 0.0700133\n",
      "[1400]\ttraining's l2: 0.0665202\n",
      "[1500]\ttraining's l2: 0.0634637\n",
      "[1600]\ttraining's l2: 0.0606145\n",
      "[1700]\ttraining's l2: 0.0580796\n",
      "[1800]\ttraining's l2: 0.0557852\n",
      "[1900]\ttraining's l2: 0.0535666\n",
      "[2000]\ttraining's l2: 0.0515486\n",
      "[2100]\ttraining's l2: 0.0496409\n",
      "[2200]\ttraining's l2: 0.04789\n",
      "[2300]\ttraining's l2: 0.0462586\n",
      "[2400]\ttraining's l2: 0.0447313\n",
      "[2500]\ttraining's l2: 0.0433161\n",
      "[2600]\ttraining's l2: 0.0419958\n",
      "[2700]\ttraining's l2: 0.0407205\n",
      "[2800]\ttraining's l2: 0.0395487\n",
      "[2900]\ttraining's l2: 0.038443\n",
      "[3000]\ttraining's l2: 0.0373653\n",
      "[3100]\ttraining's l2: 0.0363749\n",
      "[3200]\ttraining's l2: 0.0353991\n",
      "[3300]\ttraining's l2: 0.0344036\n",
      "[3400]\ttraining's l2: 0.033516\n",
      "[3500]\ttraining's l2: 0.0326852\n",
      "[3600]\ttraining's l2: 0.0318541\n",
      "[3700]\ttraining's l2: 0.0310767\n",
      "[3800]\ttraining's l2: 0.0303498\n",
      "[3900]\ttraining's l2: 0.0296588\n",
      "[4000]\ttraining's l2: 0.0289823\n",
      "[4100]\ttraining's l2: 0.0283232\n",
      "[4200]\ttraining's l2: 0.0277064\n",
      "[4300]\ttraining's l2: 0.0271067\n",
      "[4400]\ttraining's l2: 0.0265466\n",
      "[4500]\ttraining's l2: 0.02598\n",
      "[4600]\ttraining's l2: 0.0254511\n",
      "[4700]\ttraining's l2: 0.0249267\n",
      "[4800]\ttraining's l2: 0.0244179\n",
      "[4900]\ttraining's l2: 0.0239434\n",
      "[5000]\ttraining's l2: 0.0234683\n",
      "      brand  store  week   logmove\n",
      "0         1      2   141  8.446384\n",
      "1         1      2   142  8.446384\n",
      "2         2      2   141  8.366609\n",
      "3         2      2   142  8.366609\n",
      "4         3      2   141  8.199342\n",
      "5         3      2   142  8.199342\n",
      "6         4      2   141  8.114703\n",
      "7         4      2   142  8.114703\n",
      "8         5      2   141  8.467964\n",
      "9         5      2   142  8.467964\n",
      "10        6      2   141  7.474067\n",
      "11        6      2   142  7.474067\n",
      "12        7      2   141  8.173049\n",
      "13        7      2   142  8.173049\n",
      "14        8      2   141  7.248930\n",
      "15        8      2   142  7.248930\n",
      "16        9      2   141  7.033662\n",
      "17        9      2   142  7.033662\n",
      "18       10      2   141  8.489805\n",
      "19       10      2   142  8.489805\n",
      "20       11      2   141  7.867967\n",
      "21       11      2   142  7.867967\n",
      "22        1      5   141  8.449739\n",
      "23        1      5   142  8.449739\n",
      "24        2      5   141  8.493761\n",
      "25        2      5   142  8.493761\n",
      "26        3      5   141  8.875921\n",
      "27        3      5   142  8.875921\n",
      "28        4      5   141  8.388685\n",
      "29        4      5   142  8.388685\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   141  6.541471\n",
      "1797      8    134   142  6.541471\n",
      "1798      9    134   141  7.313158\n",
      "1799      9    134   142  7.313158\n",
      "1800     10    134   141  8.629642\n",
      "1801     10    134   142  8.629642\n",
      "1802     11    134   141  8.131597\n",
      "1803     11    134   142  8.131597\n",
      "1804      1    137   141  9.342575\n",
      "1805      1    137   142  9.342575\n",
      "1806      2    137   141  9.450666\n",
      "1807      2    137   142  9.450666\n",
      "1808      3    137   141  8.964413\n",
      "1809      3    137   142  8.964413\n",
      "1810      4    137   141  7.579882\n",
      "1811      4    137   142  7.579882\n",
      "1812      5    137   141  8.773677\n",
      "1813      5    137   142  8.773677\n",
      "1814      6    137   141  7.999558\n",
      "1815      6    137   142  7.999558\n",
      "1816      7    137   141  8.979153\n",
      "1817      7    137   142  8.979153\n",
      "1818      8    137   141  7.513035\n",
      "1819      8    137   142  7.513035\n",
      "1820      9    137   141  7.234108\n",
      "1821      9    137   142  7.234108\n",
      "1822     10    137   141  9.215173\n",
      "1823     10    137   142  9.215173\n",
      "1824     11    137   141  8.655894\n",
      "1825     11    137   142  8.655894\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 100.13694699100049\n",
      "\n",
      "---- Round 4 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6248\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.270879\n",
      "[200]\ttraining's l2: 0.197685\n",
      "[300]\ttraining's l2: 0.16119\n",
      "[400]\ttraining's l2: 0.140365\n",
      "[500]\ttraining's l2: 0.124507\n",
      "[600]\ttraining's l2: 0.113504\n",
      "[700]\ttraining's l2: 0.1041\n",
      "[800]\ttraining's l2: 0.0958492\n",
      "[900]\ttraining's l2: 0.0892195\n",
      "[1000]\ttraining's l2: 0.0835521\n",
      "[1100]\ttraining's l2: 0.078599\n",
      "[1200]\ttraining's l2: 0.0742012\n",
      "[1300]\ttraining's l2: 0.070369\n",
      "[1400]\ttraining's l2: 0.0669964\n",
      "[1500]\ttraining's l2: 0.0639183\n",
      "[1600]\ttraining's l2: 0.0610915\n",
      "[1700]\ttraining's l2: 0.058485\n",
      "[1800]\ttraining's l2: 0.0561885\n",
      "[1900]\ttraining's l2: 0.053994\n",
      "[2000]\ttraining's l2: 0.0519188\n",
      "[2100]\ttraining's l2: 0.0500222\n",
      "[2200]\ttraining's l2: 0.0482478\n",
      "[2300]\ttraining's l2: 0.0466068\n",
      "[2400]\ttraining's l2: 0.0450473\n",
      "[2500]\ttraining's l2: 0.0435952\n",
      "[2600]\ttraining's l2: 0.0422714\n",
      "[2700]\ttraining's l2: 0.0410175\n",
      "[2800]\ttraining's l2: 0.0398345\n",
      "[2900]\ttraining's l2: 0.0387228\n",
      "[3000]\ttraining's l2: 0.0376624\n",
      "[3100]\ttraining's l2: 0.0366565\n",
      "[3200]\ttraining's l2: 0.0356623\n",
      "[3300]\ttraining's l2: 0.0347157\n",
      "[3400]\ttraining's l2: 0.0338145\n",
      "[3500]\ttraining's l2: 0.0329885\n",
      "[3600]\ttraining's l2: 0.0321831\n",
      "[3700]\ttraining's l2: 0.0314146\n",
      "[3800]\ttraining's l2: 0.0306536\n",
      "[3900]\ttraining's l2: 0.0299597\n",
      "[4000]\ttraining's l2: 0.0293016\n",
      "[4100]\ttraining's l2: 0.028635\n",
      "[4200]\ttraining's l2: 0.0280297\n",
      "[4300]\ttraining's l2: 0.0274272\n",
      "[4400]\ttraining's l2: 0.0268251\n",
      "[4500]\ttraining's l2: 0.0262672\n",
      "[4600]\ttraining's l2: 0.025717\n",
      "[4700]\ttraining's l2: 0.025215\n",
      "[4800]\ttraining's l2: 0.0246871\n",
      "[4900]\ttraining's l2: 0.0241995\n",
      "[5000]\ttraining's l2: 0.0237201\n",
      "      brand  store  week   logmove\n",
      "0         1      2   143  8.390728\n",
      "1         1      2   144  8.390728\n",
      "2         2      2   143  8.404218\n",
      "3         2      2   144  8.404218\n",
      "4         3      2   143  8.215595\n",
      "5         3      2   144  8.215595\n",
      "6         4      2   143  7.807160\n",
      "7         4      2   144  7.807160\n",
      "8         5      2   143  8.630204\n",
      "9         5      2   144  8.630204\n",
      "10        6      2   143  8.494222\n",
      "11        6      2   144  8.494222\n",
      "12        7      2   143  7.706343\n",
      "13        7      2   144  7.706343\n",
      "14        8      2   143  7.401170\n",
      "15        8      2   144  7.401170\n",
      "16        9      2   143  6.548523\n",
      "17        9      2   144  6.548523\n",
      "18       10      2   143  8.207848\n",
      "19       10      2   144  8.207848\n",
      "20       11      2   143  8.158590\n",
      "21       11      2   144  8.158590\n",
      "22        1      5   143  8.010714\n",
      "23        1      5   144  8.010714\n",
      "24        2      5   143  8.843226\n",
      "25        2      5   144  8.843226\n",
      "26        3      5   143  8.379723\n",
      "27        3      5   144  8.379723\n",
      "28        4      5   143  6.956285\n",
      "29        4      5   144  6.956285\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   143  6.255971\n",
      "1797      8    134   144  6.255971\n",
      "1798      9    134   143  6.265686\n",
      "1799      9    134   144  6.265686\n",
      "1800     10    134   143  7.500185\n",
      "1801     10    134   144  7.500185\n",
      "1802     11    134   143  8.635775\n",
      "1803     11    134   144  8.635775\n",
      "1804      1    137   143  9.612370\n",
      "1805      1    137   144  9.612370\n",
      "1806      2    137   143  9.169726\n",
      "1807      2    137   144  9.169726\n",
      "1808      3    137   143  8.357064\n",
      "1809      3    137   144  8.357064\n",
      "1810      4    137   143  7.634642\n",
      "1811      4    137   144  7.634642\n",
      "1812      5    137   143  9.017243\n",
      "1813      5    137   144  9.017243\n",
      "1814      6    137   143  8.875079\n",
      "1815      6    137   144  8.875079\n",
      "1816      7    137   143  8.280689\n",
      "1817      7    137   144  8.280689\n",
      "1818      8    137   143  7.406874\n",
      "1819      8    137   144  7.406874\n",
      "1820      9    137   143  6.902507\n",
      "1821      9    137   144  6.902507\n",
      "1822     10    137   143  8.359860\n",
      "1823     10    137   144  8.359860\n",
      "1824     11    137   143  8.721068\n",
      "1825     11    137   144  8.721068\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 84.50803699032221\n",
      "\n",
      "---- Round 5 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6358\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.27436\n",
      "[200]\ttraining's l2: 0.200613\n",
      "[300]\ttraining's l2: 0.162928\n",
      "[400]\ttraining's l2: 0.141598\n",
      "[500]\ttraining's l2: 0.126236\n",
      "[600]\ttraining's l2: 0.115331\n",
      "[700]\ttraining's l2: 0.105414\n",
      "[800]\ttraining's l2: 0.0972252\n",
      "[900]\ttraining's l2: 0.0903066\n",
      "[1000]\ttraining's l2: 0.0846729\n",
      "[1100]\ttraining's l2: 0.0796875\n",
      "[1200]\ttraining's l2: 0.0751824\n",
      "[1300]\ttraining's l2: 0.0714497\n",
      "[1400]\ttraining's l2: 0.0679219\n",
      "[1500]\ttraining's l2: 0.0647559\n",
      "[1600]\ttraining's l2: 0.0619049\n",
      "[1700]\ttraining's l2: 0.0592789\n",
      "[1800]\ttraining's l2: 0.0568971\n",
      "[1900]\ttraining's l2: 0.0546313\n",
      "[2000]\ttraining's l2: 0.0525524\n",
      "[2100]\ttraining's l2: 0.0506383\n",
      "[2200]\ttraining's l2: 0.0488273\n",
      "[2300]\ttraining's l2: 0.0471629\n",
      "[2400]\ttraining's l2: 0.0455827\n",
      "[2500]\ttraining's l2: 0.0441309\n",
      "[2600]\ttraining's l2: 0.0428197\n",
      "[2700]\ttraining's l2: 0.04156\n",
      "[2800]\ttraining's l2: 0.0403446\n",
      "[2900]\ttraining's l2: 0.0391878\n",
      "[3000]\ttraining's l2: 0.0380995\n",
      "[3100]\ttraining's l2: 0.0370795\n",
      "[3200]\ttraining's l2: 0.0360689\n",
      "[3300]\ttraining's l2: 0.0351099\n",
      "[3400]\ttraining's l2: 0.0342026\n",
      "[3500]\ttraining's l2: 0.03335\n",
      "[3600]\ttraining's l2: 0.0325426\n",
      "[3700]\ttraining's l2: 0.0317649\n",
      "[3800]\ttraining's l2: 0.0309741\n",
      "[3900]\ttraining's l2: 0.0302706\n",
      "[4000]\ttraining's l2: 0.0295742\n",
      "[4100]\ttraining's l2: 0.0288965\n",
      "[4200]\ttraining's l2: 0.0282686\n",
      "[4300]\ttraining's l2: 0.0276543\n",
      "[4400]\ttraining's l2: 0.0270847\n",
      "[4500]\ttraining's l2: 0.0265084\n",
      "[4600]\ttraining's l2: 0.0259815\n",
      "[4700]\ttraining's l2: 0.0254745\n",
      "[4800]\ttraining's l2: 0.0249576\n",
      "[4900]\ttraining's l2: 0.02448\n",
      "[5000]\ttraining's l2: 0.0239833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      brand  store  week   logmove\n",
      "0         1      2   145  9.305906\n",
      "1         1      2   146  9.305906\n",
      "2         2      2   145  8.662221\n",
      "3         2      2   146  8.662221\n",
      "4         3      2   145  7.494201\n",
      "5         3      2   146  7.494201\n",
      "6         4      2   145  9.422921\n",
      "7         4      2   146  9.422921\n",
      "8         5      2   145  8.269323\n",
      "9         5      2   146  8.269323\n",
      "10        6      2   145  7.552031\n",
      "11        6      2   146  7.552031\n",
      "12        7      2   145  7.020245\n",
      "13        7      2   146  7.020245\n",
      "14        8      2   145  7.233719\n",
      "15        8      2   146  7.233719\n",
      "16        9      2   145  6.633515\n",
      "17        9      2   146  6.633515\n",
      "18       10      2   145  8.083717\n",
      "19       10      2   146  8.083717\n",
      "20       11      2   145  8.669253\n",
      "21       11      2   146  8.669253\n",
      "22        1      5   145  9.037954\n",
      "23        1      5   146  9.037954\n",
      "24        2      5   145  8.742600\n",
      "25        2      5   146  8.742600\n",
      "26        3      5   145  8.229528\n",
      "27        3      5   146  8.229528\n",
      "28        4      5   145  8.526882\n",
      "29        4      5   146  8.526882\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   145  6.472135\n",
      "1797      8    134   146  6.472135\n",
      "1798      9    134   145  6.804037\n",
      "1799      9    134   146  6.804037\n",
      "1800     10    134   145  7.795157\n",
      "1801     10    134   146  7.795157\n",
      "1802     11    134   145  9.178574\n",
      "1803     11    134   146  9.178574\n",
      "1804      1    137   145  9.911953\n",
      "1805      1    137   146  9.911953\n",
      "1806      2    137   145  9.519152\n",
      "1807      2    137   146  9.519152\n",
      "1808      3    137   145  8.182890\n",
      "1809      3    137   146  8.182890\n",
      "1810      4    137   145  8.496007\n",
      "1811      4    137   146  8.496007\n",
      "1812      5    137   145  8.401290\n",
      "1813      5    137   146  8.401290\n",
      "1814      6    137   145  8.471065\n",
      "1815      6    137   146  8.471065\n",
      "1816      7    137   145  8.255087\n",
      "1817      7    137   146  8.255087\n",
      "1818      8    137   145  7.274651\n",
      "1819      8    137   146  7.274651\n",
      "1820      9    137   145  7.078355\n",
      "1821      9    137   146  7.078355\n",
      "1822     10    137   145  9.067918\n",
      "1823     10    137   146  9.067918\n",
      "1824     11    137   145  9.226035\n",
      "1825     11    137   146  9.226035\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 147.68133658648387\n",
      "\n",
      "---- Round 6 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6446\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.276846\n",
      "[200]\ttraining's l2: 0.204247\n",
      "[300]\ttraining's l2: 0.164667\n",
      "[400]\ttraining's l2: 0.142357\n",
      "[500]\ttraining's l2: 0.126622\n",
      "[600]\ttraining's l2: 0.115739\n",
      "[700]\ttraining's l2: 0.105917\n",
      "[800]\ttraining's l2: 0.097755\n",
      "[900]\ttraining's l2: 0.0909212\n",
      "[1000]\ttraining's l2: 0.0851774\n",
      "[1100]\ttraining's l2: 0.0800716\n",
      "[1200]\ttraining's l2: 0.0755718\n",
      "[1300]\ttraining's l2: 0.0716735\n",
      "[1400]\ttraining's l2: 0.0681845\n",
      "[1500]\ttraining's l2: 0.065004\n",
      "[1600]\ttraining's l2: 0.0621063\n",
      "[1700]\ttraining's l2: 0.0594683\n",
      "[1800]\ttraining's l2: 0.0571032\n",
      "[1900]\ttraining's l2: 0.0548191\n",
      "[2000]\ttraining's l2: 0.0527741\n",
      "[2100]\ttraining's l2: 0.0508736\n",
      "[2200]\ttraining's l2: 0.0490907\n",
      "[2300]\ttraining's l2: 0.0474265\n",
      "[2400]\ttraining's l2: 0.0458663\n",
      "[2500]\ttraining's l2: 0.0443992\n",
      "[2600]\ttraining's l2: 0.0430226\n",
      "[2700]\ttraining's l2: 0.0417581\n",
      "[2800]\ttraining's l2: 0.0405214\n",
      "[2900]\ttraining's l2: 0.0393471\n",
      "[3000]\ttraining's l2: 0.0382254\n",
      "[3100]\ttraining's l2: 0.0371921\n",
      "[3200]\ttraining's l2: 0.0362069\n",
      "[3300]\ttraining's l2: 0.0352299\n",
      "[3400]\ttraining's l2: 0.0343259\n",
      "[3500]\ttraining's l2: 0.0334704\n",
      "[3600]\ttraining's l2: 0.0326282\n",
      "[3700]\ttraining's l2: 0.0318435\n",
      "[3800]\ttraining's l2: 0.0310697\n",
      "[3900]\ttraining's l2: 0.0303385\n",
      "[4000]\ttraining's l2: 0.0296615\n",
      "[4100]\ttraining's l2: 0.028986\n",
      "[4200]\ttraining's l2: 0.0283427\n",
      "[4300]\ttraining's l2: 0.0277313\n",
      "[4400]\ttraining's l2: 0.0271232\n",
      "[4500]\ttraining's l2: 0.0265491\n",
      "[4600]\ttraining's l2: 0.0260098\n",
      "[4700]\ttraining's l2: 0.0254925\n",
      "[4800]\ttraining's l2: 0.0249586\n",
      "[4900]\ttraining's l2: 0.0244454\n",
      "[5000]\ttraining's l2: 0.0239755\n",
      "      brand  store  week    logmove\n",
      "0         1      2   147   9.429036\n",
      "1         1      2   148   9.429036\n",
      "2         2      2   147   9.071755\n",
      "3         2      2   148   9.071755\n",
      "4         3      2   147   8.226101\n",
      "5         3      2   148   8.226101\n",
      "6         4      2   147   9.011766\n",
      "7         4      2   148   9.011766\n",
      "8         5      2   147   8.675526\n",
      "9         5      2   148   8.675526\n",
      "10        6      2   147   8.165105\n",
      "11        6      2   148   8.165105\n",
      "12        7      2   147   8.029626\n",
      "13        7      2   148   8.029626\n",
      "14        8      2   147   8.065923\n",
      "15        8      2   148   8.065923\n",
      "16        9      2   147   7.292042\n",
      "17        9      2   148   7.292042\n",
      "18       10      2   147   8.925086\n",
      "19       10      2   148   8.925086\n",
      "20       11      2   147   8.878674\n",
      "21       11      2   148   8.878674\n",
      "22        1      5   147   9.105926\n",
      "23        1      5   148   9.105926\n",
      "24        2      5   147   8.934270\n",
      "25        2      5   148   8.934270\n",
      "26        3      5   147   7.946444\n",
      "27        3      5   148   7.946444\n",
      "28        4      5   147   8.539866\n",
      "29        4      5   148   8.539866\n",
      "...     ...    ...   ...        ...\n",
      "1796      8    134   147   6.848965\n",
      "1797      8    134   148   6.848965\n",
      "1798      9    134   147   7.525259\n",
      "1799      9    134   148   7.525259\n",
      "1800     10    134   147   8.635445\n",
      "1801     10    134   148   8.635445\n",
      "1802     11    134   147   8.915708\n",
      "1803     11    134   148   8.915708\n",
      "1804      1    137   147  10.144797\n",
      "1805      1    137   148  10.144797\n",
      "1806      2    137   147   9.730898\n",
      "1807      2    137   148   9.730898\n",
      "1808      3    137   147   8.538417\n",
      "1809      3    137   148   8.538417\n",
      "1810      4    137   147   8.108216\n",
      "1811      4    137   148   8.108216\n",
      "1812      5    137   147   8.746659\n",
      "1813      5    137   148   8.746659\n",
      "1814      6    137   147   8.442432\n",
      "1815      6    137   148   8.442432\n",
      "1816      7    137   147   8.182238\n",
      "1817      7    137   148   8.182238\n",
      "1818      8    137   147   7.916286\n",
      "1819      8    137   148   7.916286\n",
      "1820      9    137   147   6.719944\n",
      "1821      9    137   148   6.719944\n",
      "1822     10    137   147   8.793896\n",
      "1823     10    137   148   8.793896\n",
      "1824     11    137   147   9.245416\n",
      "1825     11    137   148   9.245416\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 145.3756747180339\n",
      "\n",
      "---- Round 7 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6501\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.274998\n",
      "[200]\ttraining's l2: 0.201866\n",
      "[300]\ttraining's l2: 0.164721\n",
      "[400]\ttraining's l2: 0.143641\n",
      "[500]\ttraining's l2: 0.127554\n",
      "[600]\ttraining's l2: 0.116536\n",
      "[700]\ttraining's l2: 0.106387\n",
      "[800]\ttraining's l2: 0.0982117\n",
      "[900]\ttraining's l2: 0.0916526\n",
      "[1000]\ttraining's l2: 0.0860022\n",
      "[1100]\ttraining's l2: 0.0811725\n",
      "[1200]\ttraining's l2: 0.0765429\n",
      "[1300]\ttraining's l2: 0.072733\n",
      "[1400]\ttraining's l2: 0.0692327\n",
      "[1500]\ttraining's l2: 0.0660416\n",
      "[1600]\ttraining's l2: 0.0632431\n",
      "[1700]\ttraining's l2: 0.0605802\n",
      "[1800]\ttraining's l2: 0.0581969\n",
      "[1900]\ttraining's l2: 0.0559421\n",
      "[2000]\ttraining's l2: 0.0538886\n",
      "[2100]\ttraining's l2: 0.0519167\n",
      "[2200]\ttraining's l2: 0.0500547\n",
      "[2300]\ttraining's l2: 0.0483785\n",
      "[2400]\ttraining's l2: 0.0468033\n",
      "[2500]\ttraining's l2: 0.0453103\n",
      "[2600]\ttraining's l2: 0.0438918\n",
      "[2700]\ttraining's l2: 0.0425778\n",
      "[2800]\ttraining's l2: 0.0413113\n",
      "[2900]\ttraining's l2: 0.0401459\n",
      "[3000]\ttraining's l2: 0.0389943\n",
      "[3100]\ttraining's l2: 0.0379337\n",
      "[3200]\ttraining's l2: 0.0369092\n",
      "[3300]\ttraining's l2: 0.0359381\n",
      "[3400]\ttraining's l2: 0.0350247\n",
      "[3500]\ttraining's l2: 0.0341685\n",
      "[3600]\ttraining's l2: 0.0332892\n",
      "[3700]\ttraining's l2: 0.032469\n",
      "[3800]\ttraining's l2: 0.0317083\n",
      "[3900]\ttraining's l2: 0.0309856\n",
      "[4000]\ttraining's l2: 0.0302849\n",
      "[4100]\ttraining's l2: 0.0296071\n",
      "[4200]\ttraining's l2: 0.028956\n",
      "[4300]\ttraining's l2: 0.0283413\n",
      "[4400]\ttraining's l2: 0.0277408\n",
      "[4500]\ttraining's l2: 0.0271518\n",
      "[4600]\ttraining's l2: 0.0265979\n",
      "[4700]\ttraining's l2: 0.026064\n",
      "[4800]\ttraining's l2: 0.0255512\n",
      "[4900]\ttraining's l2: 0.0250702\n",
      "[5000]\ttraining's l2: 0.0245876\n",
      "      brand  store  week   logmove\n",
      "0         1      2   149  9.121873\n",
      "1         1      2   150  9.121873\n",
      "2         2      2   149  8.428233\n",
      "3         2      2   150  8.428233\n",
      "4         3      2   149  8.116572\n",
      "5         3      2   150  8.116572\n",
      "6         4      2   149  8.760687\n",
      "7         4      2   150  8.760687\n",
      "8         5      2   149  8.916945\n",
      "9         5      2   150  8.916945\n",
      "10        6      2   149  7.669378\n",
      "11        6      2   150  7.669378\n",
      "12        7      2   149  7.082112\n",
      "13        7      2   150  7.082112\n",
      "14        8      2   149  7.473029\n",
      "15        8      2   150  7.473029\n",
      "16        9      2   149  6.890062\n",
      "17        9      2   150  6.890062\n",
      "18       10      2   149  8.725159\n",
      "19       10      2   150  8.725159\n",
      "20       11      2   149  8.573419\n",
      "21       11      2   150  8.573419\n",
      "22        1      5   149  9.299053\n",
      "23        1      5   150  9.299053\n",
      "24        2      5   149  8.599556\n",
      "25        2      5   150  8.599556\n",
      "26        3      5   149  7.782562\n",
      "27        3      5   150  7.782562\n",
      "28        4      5   149  8.368915\n",
      "29        4      5   150  8.368915\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   149  6.902556\n",
      "1797      8    134   150  6.902556\n",
      "1798      9    134   149  7.192132\n",
      "1799      9    134   150  7.192132\n",
      "1800     10    134   149  8.558502\n",
      "1801     10    134   150  8.558502\n",
      "1802     11    134   149  8.507612\n",
      "1803     11    134   150  8.507612\n",
      "1804      1    137   149  9.860582\n",
      "1805      1    137   150  9.860582\n",
      "1806      2    137   149  9.095061\n",
      "1807      2    137   150  9.095061\n",
      "1808      3    137   149  8.440423\n",
      "1809      3    137   150  8.440423\n",
      "1810      4    137   149  8.222793\n",
      "1811      4    137   150  8.222793\n",
      "1812      5    137   149  9.062575\n",
      "1813      5    137   150  9.062575\n",
      "1814      6    137   149  8.260260\n",
      "1815      6    137   150  8.260260\n",
      "1816      7    137   149  7.776985\n",
      "1817      7    137   150  7.776985\n",
      "1818      8    137   149  7.344992\n",
      "1819      8    137   150  7.344992\n",
      "1820      9    137   149  6.625965\n",
      "1821      9    137   150  6.625965\n",
      "1822     10    137   149  8.930394\n",
      "1823     10    137   150  8.930394\n",
      "1824     11    137   149  8.747657\n",
      "1825     11    137   150  8.747657\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 107.86563394849149\n",
      "\n",
      "---- Round 8 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6578\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.279784\n",
      "[200]\ttraining's l2: 0.206251\n",
      "[300]\ttraining's l2: 0.168201\n",
      "[400]\ttraining's l2: 0.146831\n",
      "[500]\ttraining's l2: 0.130942\n",
      "[600]\ttraining's l2: 0.119965\n",
      "[700]\ttraining's l2: 0.109504\n",
      "[800]\ttraining's l2: 0.100747\n",
      "[900]\ttraining's l2: 0.0939115\n",
      "[1000]\ttraining's l2: 0.0878414\n",
      "[1100]\ttraining's l2: 0.082716\n",
      "[1200]\ttraining's l2: 0.0780835\n",
      "[1300]\ttraining's l2: 0.0740083\n",
      "[1400]\ttraining's l2: 0.0703479\n",
      "[1500]\ttraining's l2: 0.0670321\n",
      "[1600]\ttraining's l2: 0.0641118\n",
      "[1700]\ttraining's l2: 0.0614604\n",
      "[1800]\ttraining's l2: 0.0590982\n",
      "[1900]\ttraining's l2: 0.0568413\n",
      "[2000]\ttraining's l2: 0.0547208\n",
      "[2100]\ttraining's l2: 0.0527128\n",
      "[2200]\ttraining's l2: 0.0508543\n",
      "[2300]\ttraining's l2: 0.0491847\n",
      "[2400]\ttraining's l2: 0.047577\n",
      "[2500]\ttraining's l2: 0.0460058\n",
      "[2600]\ttraining's l2: 0.0446058\n",
      "[2700]\ttraining's l2: 0.0432994\n",
      "[2800]\ttraining's l2: 0.0420581\n",
      "[2900]\ttraining's l2: 0.0408521\n",
      "[3000]\ttraining's l2: 0.0397025\n",
      "[3100]\ttraining's l2: 0.0386104\n",
      "[3200]\ttraining's l2: 0.0375516\n",
      "[3300]\ttraining's l2: 0.0365194\n",
      "[3400]\ttraining's l2: 0.0355963\n",
      "[3500]\ttraining's l2: 0.0346789\n",
      "[3600]\ttraining's l2: 0.0337866\n",
      "[3700]\ttraining's l2: 0.0329958\n",
      "[3800]\ttraining's l2: 0.0322189\n",
      "[3900]\ttraining's l2: 0.0314939\n",
      "[4000]\ttraining's l2: 0.0307754\n",
      "[4100]\ttraining's l2: 0.0300735\n",
      "[4200]\ttraining's l2: 0.0294161\n",
      "[4300]\ttraining's l2: 0.0287888\n",
      "[4400]\ttraining's l2: 0.0281846\n",
      "[4500]\ttraining's l2: 0.0275942\n",
      "[4600]\ttraining's l2: 0.0270227\n",
      "[4700]\ttraining's l2: 0.0265143\n",
      "[4800]\ttraining's l2: 0.0259953\n",
      "[4900]\ttraining's l2: 0.0254992\n",
      "[5000]\ttraining's l2: 0.0250112\n",
      "      brand  store  week   logmove\n",
      "0         1      2   151  8.676863\n",
      "1         1      2   152  8.676863\n",
      "2         2      2   151  8.399612\n",
      "3         2      2   152  8.399612\n",
      "4         3      2   151  8.344234\n",
      "5         3      2   152  8.344234\n",
      "6         4      2   151  8.535976\n",
      "7         4      2   152  8.535976\n",
      "8         5      2   151  9.000602\n",
      "9         5      2   152  9.000602\n",
      "10        6      2   151  7.959219\n",
      "11        6      2   152  7.959219\n",
      "12        7      2   151  7.430994\n",
      "13        7      2   152  7.430994\n",
      "14        8      2   151  7.257054\n",
      "15        8      2   152  7.257054\n",
      "16        9      2   151  7.048550\n",
      "17        9      2   152  7.048550\n",
      "18       10      2   151  8.832609\n",
      "19       10      2   152  8.832609\n",
      "20       11      2   151  8.069286\n",
      "21       11      2   152  8.069286\n",
      "22        1      5   151  8.813828\n",
      "23        1      5   152  8.813828\n",
      "24        2      5   151  9.102779\n",
      "25        2      5   152  9.102779\n",
      "26        3      5   151  8.445983\n",
      "27        3      5   152  8.445983\n",
      "28        4      5   151  8.154402\n",
      "29        4      5   152  8.154402\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   151  6.677196\n",
      "1797      8    134   152  6.677196\n",
      "1798      9    134   151  7.297109\n",
      "1799      9    134   152  7.297109\n",
      "1800     10    134   151  8.104588\n",
      "1801     10    134   152  8.104588\n",
      "1802     11    134   151  8.583747\n",
      "1803     11    134   152  8.583747\n",
      "1804      1    137   151  9.537545\n",
      "1805      1    137   152  9.537545\n",
      "1806      2    137   151  9.413727\n",
      "1807      2    137   152  9.413727\n",
      "1808      3    137   151  8.907343\n",
      "1809      3    137   152  8.907343\n",
      "1810      4    137   151  8.638700\n",
      "1811      4    137   152  8.638700\n",
      "1812      5    137   151  9.419717\n",
      "1813      5    137   152  9.419717\n",
      "1814      6    137   151  8.364494\n",
      "1815      6    137   152  8.364494\n",
      "1816      7    137   151  8.063020\n",
      "1817      7    137   152  8.063020\n",
      "1818      8    137   151  7.551948\n",
      "1819      8    137   152  7.551948\n",
      "1820      9    137   151  6.969196\n",
      "1821      9    137   152  6.969196\n",
      "1822     10    137   151  9.289112\n",
      "1823     10    137   152  9.289112\n",
      "1824     11    137   151  8.564649\n",
      "1825     11    137   152  8.564649\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 68.94518028854979\n",
      "\n",
      "---- Round 9 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6655\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.278834\n",
      "[200]\ttraining's l2: 0.20628\n",
      "[300]\ttraining's l2: 0.169426\n",
      "[400]\ttraining's l2: 0.14604\n",
      "[500]\ttraining's l2: 0.129886\n",
      "[600]\ttraining's l2: 0.118645\n",
      "[700]\ttraining's l2: 0.108524\n",
      "[800]\ttraining's l2: 0.100078\n",
      "[900]\ttraining's l2: 0.0932192\n",
      "[1000]\ttraining's l2: 0.0874319\n",
      "[1100]\ttraining's l2: 0.0824471\n",
      "[1200]\ttraining's l2: 0.0778573\n",
      "[1300]\ttraining's l2: 0.0738747\n",
      "[1400]\ttraining's l2: 0.0702867\n",
      "[1500]\ttraining's l2: 0.0671533\n",
      "[1600]\ttraining's l2: 0.0642034\n",
      "[1700]\ttraining's l2: 0.0615522\n",
      "[1800]\ttraining's l2: 0.0591636\n",
      "[1900]\ttraining's l2: 0.0568947\n",
      "[2000]\ttraining's l2: 0.0548315\n",
      "[2100]\ttraining's l2: 0.0528831\n",
      "[2200]\ttraining's l2: 0.0509737\n",
      "[2300]\ttraining's l2: 0.049258\n",
      "[2400]\ttraining's l2: 0.0476783\n",
      "[2500]\ttraining's l2: 0.0461739\n",
      "[2600]\ttraining's l2: 0.044779\n",
      "[2700]\ttraining's l2: 0.0434611\n",
      "[2800]\ttraining's l2: 0.0422218\n",
      "[2900]\ttraining's l2: 0.0410426\n",
      "[3000]\ttraining's l2: 0.0399032\n",
      "[3100]\ttraining's l2: 0.0388275\n",
      "[3200]\ttraining's l2: 0.0377868\n",
      "[3300]\ttraining's l2: 0.0367877\n",
      "[3400]\ttraining's l2: 0.0358538\n",
      "[3500]\ttraining's l2: 0.0349683\n",
      "[3600]\ttraining's l2: 0.0341255\n",
      "[3700]\ttraining's l2: 0.0333071\n",
      "[3800]\ttraining's l2: 0.0325174\n",
      "[3900]\ttraining's l2: 0.0317927\n",
      "[4000]\ttraining's l2: 0.03107\n",
      "[4100]\ttraining's l2: 0.0303469\n",
      "[4200]\ttraining's l2: 0.0296746\n",
      "[4300]\ttraining's l2: 0.0290541\n",
      "[4400]\ttraining's l2: 0.0284407\n",
      "[4500]\ttraining's l2: 0.027861\n",
      "[4600]\ttraining's l2: 0.0273004\n",
      "[4700]\ttraining's l2: 0.0267643\n",
      "[4800]\ttraining's l2: 0.0262362\n",
      "[4900]\ttraining's l2: 0.0257385\n",
      "[5000]\ttraining's l2: 0.0252413\n",
      "      brand  store  week   logmove\n",
      "0         1      2   153  8.710543\n",
      "1         1      2   154  8.710543\n",
      "2         2      2   153  8.578180\n",
      "3         2      2   154  8.578180\n",
      "4         3      2   153  8.183289\n",
      "5         3      2   154  8.183289\n",
      "6         4      2   153  8.258077\n",
      "7         4      2   154  8.258077\n",
      "8         5      2   153  9.020809\n",
      "9         5      2   154  9.020809\n",
      "10        6      2   153  7.709813\n",
      "11        6      2   154  7.709813\n",
      "12        7      2   153  7.826880\n",
      "13        7      2   154  7.826880\n",
      "14        8      2   153  7.540005\n",
      "15        8      2   154  7.540005\n",
      "16        9      2   153  6.962927\n",
      "17        9      2   154  6.962927\n",
      "18       10      2   153  9.382975\n",
      "19       10      2   154  9.382975\n",
      "20       11      2   153  8.217267\n",
      "21       11      2   154  8.217267\n",
      "22        1      5   153  9.000278\n",
      "23        1      5   154  9.000278\n",
      "24        2      5   153  8.909407\n",
      "25        2      5   154  8.909407\n",
      "26        3      5   153  8.181189\n",
      "27        3      5   154  8.181189\n",
      "28        4      5   153  7.888562\n",
      "29        4      5   154  7.888562\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   153  6.647145\n",
      "1797      8    134   154  6.647145\n",
      "1798      9    134   153  6.919037\n",
      "1799      9    134   154  6.919037\n",
      "1800     10    134   153  8.674372\n",
      "1801     10    134   154  8.674372\n",
      "1802     11    134   153  8.476369\n",
      "1803     11    134   154  8.476369\n",
      "1804      1    137   153  9.438046\n",
      "1805      1    137   154  9.438046\n",
      "1806      2    137   153  9.634244\n",
      "1807      2    137   154  9.634244\n",
      "1808      3    137   153  8.586658\n",
      "1809      3    137   154  8.586658\n",
      "1810      4    137   153  7.755448\n",
      "1811      4    137   154  7.755448\n",
      "1812      5    137   153  9.403432\n",
      "1813      5    137   154  9.403432\n",
      "1814      6    137   153  8.151990\n",
      "1815      6    137   154  8.151990\n",
      "1816      7    137   153  7.975138\n",
      "1817      7    137   154  7.975138\n",
      "1818      8    137   153  7.251721\n",
      "1819      8    137   154  7.251721\n",
      "1820      9    137   153  6.913380\n",
      "1821      9    137   154  6.913380\n",
      "1822     10    137   153  9.632805\n",
      "1823     10    137   154  9.632805\n",
      "1824     11    137   153  8.397082\n",
      "1825     11    137   154  8.397082\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 117.93093552051434\n",
      "\n",
      "---- Round 10 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6765\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.279321\n",
      "[200]\ttraining's l2: 0.206181\n",
      "[300]\ttraining's l2: 0.168628\n",
      "[400]\ttraining's l2: 0.146493\n",
      "[500]\ttraining's l2: 0.129897\n",
      "[600]\ttraining's l2: 0.118811\n",
      "[700]\ttraining's l2: 0.108624\n",
      "[800]\ttraining's l2: 0.100627\n",
      "[900]\ttraining's l2: 0.0934629\n",
      "[1000]\ttraining's l2: 0.0877534\n",
      "[1100]\ttraining's l2: 0.0828157\n",
      "[1200]\ttraining's l2: 0.0782545\n",
      "[1300]\ttraining's l2: 0.0743553\n",
      "[1400]\ttraining's l2: 0.070794\n",
      "[1500]\ttraining's l2: 0.067674\n",
      "[1600]\ttraining's l2: 0.0647757\n",
      "[1700]\ttraining's l2: 0.0620707\n",
      "[1800]\ttraining's l2: 0.0597411\n",
      "[1900]\ttraining's l2: 0.057406\n",
      "[2000]\ttraining's l2: 0.0552358\n",
      "[2100]\ttraining's l2: 0.0532458\n",
      "[2200]\ttraining's l2: 0.0513958\n",
      "[2300]\ttraining's l2: 0.0496988\n",
      "[2400]\ttraining's l2: 0.0480973\n",
      "[2500]\ttraining's l2: 0.0466114\n",
      "[2600]\ttraining's l2: 0.0452478\n",
      "[2700]\ttraining's l2: 0.0439415\n",
      "[2800]\ttraining's l2: 0.0426481\n",
      "[2900]\ttraining's l2: 0.0414313\n",
      "[3000]\ttraining's l2: 0.0402604\n",
      "[3100]\ttraining's l2: 0.0391809\n",
      "[3200]\ttraining's l2: 0.0381574\n",
      "[3300]\ttraining's l2: 0.0371587\n",
      "[3400]\ttraining's l2: 0.0361924\n",
      "[3500]\ttraining's l2: 0.0352882\n",
      "[3600]\ttraining's l2: 0.034424\n",
      "[3700]\ttraining's l2: 0.0335877\n",
      "[3800]\ttraining's l2: 0.0327812\n",
      "[3900]\ttraining's l2: 0.0320617\n",
      "[4000]\ttraining's l2: 0.031355\n",
      "[4100]\ttraining's l2: 0.0306283\n",
      "[4200]\ttraining's l2: 0.0299631\n",
      "[4300]\ttraining's l2: 0.0293315\n",
      "[4400]\ttraining's l2: 0.0287254\n",
      "[4500]\ttraining's l2: 0.0281372\n",
      "[4600]\ttraining's l2: 0.0275653\n",
      "[4700]\ttraining's l2: 0.0270233\n",
      "[4800]\ttraining's l2: 0.0264891\n",
      "[4900]\ttraining's l2: 0.025984\n",
      "[5000]\ttraining's l2: 0.0254781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      brand  store  week   logmove\n",
      "0         1      2   155  8.970168\n",
      "1         1      2   156  8.970168\n",
      "2         2      2   155  8.251628\n",
      "3         2      2   156  8.251628\n",
      "4         3      2   155  8.068856\n",
      "5         3      2   156  8.068856\n",
      "6         4      2   155  7.999592\n",
      "7         4      2   156  7.999592\n",
      "8         5      2   155  8.889869\n",
      "9         5      2   156  8.889869\n",
      "10        6      2   155  7.637332\n",
      "11        6      2   156  7.637332\n",
      "12        7      2   155  7.378734\n",
      "13        7      2   156  7.378734\n",
      "14        8      2   155  7.530430\n",
      "15        8      2   156  7.530430\n",
      "16        9      2   155  7.243917\n",
      "17        9      2   156  7.243917\n",
      "18       10      2   155  9.237232\n",
      "19       10      2   156  9.237232\n",
      "20       11      2   155  7.819761\n",
      "21       11      2   156  7.819761\n",
      "22        1      5   155  8.420922\n",
      "23        1      5   156  8.420922\n",
      "24        2      5   155  8.842637\n",
      "25        2      5   156  8.842637\n",
      "26        3      5   155  8.161023\n",
      "27        3      5   156  8.161023\n",
      "28        4      5   155  8.121393\n",
      "29        4      5   156  8.121393\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   155  6.891883\n",
      "1797      8    134   156  6.891883\n",
      "1798      9    134   155  7.335613\n",
      "1799      9    134   156  7.335613\n",
      "1800     10    134   155  8.780511\n",
      "1801     10    134   156  8.780511\n",
      "1802     11    134   155  8.612905\n",
      "1803     11    134   156  8.612905\n",
      "1804      1    137   155  9.692638\n",
      "1805      1    137   156  9.692638\n",
      "1806      2    137   155  9.479602\n",
      "1807      2    137   156  9.479602\n",
      "1808      3    137   155  8.358450\n",
      "1809      3    137   156  8.358450\n",
      "1810      4    137   155  8.566636\n",
      "1811      4    137   156  8.566636\n",
      "1812      5    137   155  9.544585\n",
      "1813      5    137   156  9.544585\n",
      "1814      6    137   155  8.264811\n",
      "1815      6    137   156  8.264811\n",
      "1816      7    137   155  8.028035\n",
      "1817      7    137   156  8.028035\n",
      "1818      8    137   155  7.675353\n",
      "1819      8    137   156  7.675353\n",
      "1820      9    137   155  6.537801\n",
      "1821      9    137   156  6.537801\n",
      "1822     10    137   155  9.127811\n",
      "1823     10    137   156  9.127811\n",
      "1824     11    137   155  8.472752\n",
      "1825     11    137   156  8.472752\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 118.31807915081072\n",
      "\n",
      "---- Round 11 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6875\n",
      "\n",
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.282437\n",
      "[200]\ttraining's l2: 0.208092\n",
      "[300]\ttraining's l2: 0.170503\n",
      "[400]\ttraining's l2: 0.147837\n",
      "[500]\ttraining's l2: 0.131307\n",
      "[600]\ttraining's l2: 0.119939\n",
      "[700]\ttraining's l2: 0.109894\n",
      "[800]\ttraining's l2: 0.101739\n",
      "[900]\ttraining's l2: 0.0946329\n",
      "[1000]\ttraining's l2: 0.0887889\n",
      "[1100]\ttraining's l2: 0.0837515\n",
      "[1200]\ttraining's l2: 0.0792369\n",
      "[1300]\ttraining's l2: 0.0753329\n",
      "[1400]\ttraining's l2: 0.0717576\n",
      "[1500]\ttraining's l2: 0.0684171\n",
      "[1600]\ttraining's l2: 0.0654274\n",
      "[1700]\ttraining's l2: 0.0627182\n",
      "[1800]\ttraining's l2: 0.0602897\n",
      "[1900]\ttraining's l2: 0.0579145\n",
      "[2000]\ttraining's l2: 0.055776\n",
      "[2100]\ttraining's l2: 0.053755\n",
      "[2200]\ttraining's l2: 0.0518544\n",
      "[2300]\ttraining's l2: 0.050096\n",
      "[2400]\ttraining's l2: 0.0485137\n",
      "[2500]\ttraining's l2: 0.0469571\n",
      "[2600]\ttraining's l2: 0.0455348\n",
      "[2700]\ttraining's l2: 0.0442201\n",
      "[2800]\ttraining's l2: 0.0429389\n",
      "[2900]\ttraining's l2: 0.0417316\n",
      "[3000]\ttraining's l2: 0.0405755\n",
      "[3100]\ttraining's l2: 0.0395098\n",
      "[3200]\ttraining's l2: 0.0384613\n",
      "[3300]\ttraining's l2: 0.0374572\n",
      "[3400]\ttraining's l2: 0.0364864\n",
      "[3500]\ttraining's l2: 0.0355859\n",
      "[3600]\ttraining's l2: 0.0347181\n",
      "[3700]\ttraining's l2: 0.0338899\n",
      "[3800]\ttraining's l2: 0.033112\n",
      "[3900]\ttraining's l2: 0.0323711\n",
      "[4000]\ttraining's l2: 0.0316371\n",
      "[4100]\ttraining's l2: 0.0309347\n",
      "[4200]\ttraining's l2: 0.0302774\n",
      "[4300]\ttraining's l2: 0.0296569\n",
      "[4400]\ttraining's l2: 0.0290271\n",
      "[4500]\ttraining's l2: 0.0284421\n",
      "[4600]\ttraining's l2: 0.0278631\n",
      "[4700]\ttraining's l2: 0.0273077\n",
      "[4800]\ttraining's l2: 0.0267655\n",
      "[4900]\ttraining's l2: 0.0262576\n",
      "[5000]\ttraining's l2: 0.0257622\n",
      "      brand  store  week   logmove\n",
      "0         1      2   157  8.700540\n",
      "1         1      2   158  8.700540\n",
      "2         2      2   157  8.042926\n",
      "3         2      2   158  8.042926\n",
      "4         3      2   157  7.886294\n",
      "5         3      2   158  7.886294\n",
      "6         4      2   157  8.988583\n",
      "7         4      2   158  8.988583\n",
      "8         5      2   157  8.534326\n",
      "9         5      2   158  8.534326\n",
      "10        6      2   157  7.584058\n",
      "11        6      2   158  7.584058\n",
      "12        7      2   157  6.936598\n",
      "13        7      2   158  6.936598\n",
      "14        8      2   157  7.420591\n",
      "15        8      2   158  7.420591\n",
      "16        9      2   157  6.409729\n",
      "17        9      2   158  6.409729\n",
      "18       10      2   157  8.239650\n",
      "19       10      2   158  8.239650\n",
      "20       11      2   157  7.461961\n",
      "21       11      2   158  7.461961\n",
      "22        1      5   157  8.959026\n",
      "23        1      5   158  8.959026\n",
      "24        2      5   157  8.966487\n",
      "25        2      5   158  8.966487\n",
      "26        3      5   157  8.582676\n",
      "27        3      5   158  8.582676\n",
      "28        4      5   157  8.951127\n",
      "29        4      5   158  8.951127\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   157  6.885533\n",
      "1797      8    134   158  6.885533\n",
      "1798      9    134   157  6.927660\n",
      "1799      9    134   158  6.927660\n",
      "1800     10    134   157  8.110945\n",
      "1801     10    134   158  8.110945\n",
      "1802     11    134   157  8.394675\n",
      "1803     11    134   158  8.394675\n",
      "1804      1    137   157  9.793272\n",
      "1805      1    137   158  9.793272\n",
      "1806      2    137   157  9.379908\n",
      "1807      2    137   158  9.379908\n",
      "1808      3    137   157  8.583528\n",
      "1809      3    137   158  8.583528\n",
      "1810      4    137   157  9.291739\n",
      "1811      4    137   158  9.291739\n",
      "1812      5    137   157  9.668289\n",
      "1813      5    137   158  9.668289\n",
      "1814      6    137   157  8.175699\n",
      "1815      6    137   158  8.175699\n",
      "1816      7    137   157  7.919279\n",
      "1817      7    137   158  7.919279\n",
      "1818      8    137   157  7.421682\n",
      "1819      8    137   158  7.421682\n",
      "1820      9    137   157  5.343521\n",
      "1821      9    137   158  5.343521\n",
      "1822     10    137   157  9.222117\n",
      "1823     10    137   158  9.222117\n",
      "1824     11    137   157  8.207510\n",
      "1825     11    137   158  8.207510\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 94.7643136827172\n",
      "\n",
      "---- Round 12 ----\n",
      "   store  brand  week   logmove  constant    price1    price2    price3  \\\n",
      "0      2      1    40  9.018695         1  0.060469  0.060497  0.042031   \n",
      "1      2      1    46  8.723231         1  0.060469  0.060312  0.045156   \n",
      "2      2      1    47  8.253228         1  0.060469  0.060312  0.045156   \n",
      "\n",
      "     price4    price5    price6    price7    price8    price9   price10  \\\n",
      "0  0.029531  0.049531  0.053021  0.038906  0.041406  0.028906  0.024844   \n",
      "1  0.046719  0.049531  0.047813  0.045781  0.027969  0.042969  0.042031   \n",
      "2  0.046719  0.037344  0.053021  0.045781  0.041406  0.048125  0.032656   \n",
      "\n",
      "    price11  deal  feat     profit  \n",
      "0  0.038984     1   0.0  37.992326  \n",
      "1  0.038984     0   0.0  30.126667  \n",
      "2  0.038984     0   0.0  30.000000  \n",
      "\n",
      "Number of missing rows is 6985\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   brand  store  week   logmove  logmove_lag2  constant_lag2  price1_lag2  \\\n",
      "0      1      2    40  9.018695           NaN            NaN          NaN   \n",
      "\n",
      "   price2_lag2  price3_lag2  price4_lag2  price5_lag2  price6_lag2  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   price7_lag2  price8_lag2  price9_lag2  price10_lag2  price11_lag2  \\\n",
      "0          NaN          NaN          NaN           NaN           NaN   \n",
      "\n",
      "   deal_lag2  feat_lag2  profit_lag2  \n",
      "0        NaN        NaN          NaN  \n",
      "\n",
      "Training and predicting models...\n",
      "Training until validation scores don't improve for 125 rounds.\n",
      "[100]\ttraining's l2: 0.279949\n",
      "[200]\ttraining's l2: 0.209411\n",
      "[300]\ttraining's l2: 0.170882\n",
      "[400]\ttraining's l2: 0.148751\n",
      "[500]\ttraining's l2: 0.132091\n",
      "[600]\ttraining's l2: 0.121086\n",
      "[700]\ttraining's l2: 0.110973\n",
      "[800]\ttraining's l2: 0.102311\n",
      "[900]\ttraining's l2: 0.095037\n",
      "[1000]\ttraining's l2: 0.0892981\n",
      "[1100]\ttraining's l2: 0.0842091\n",
      "[1200]\ttraining's l2: 0.0794915\n",
      "[1300]\ttraining's l2: 0.0755011\n",
      "[1400]\ttraining's l2: 0.0719202\n",
      "[1500]\ttraining's l2: 0.0687103\n",
      "[1600]\ttraining's l2: 0.0657267\n",
      "[1700]\ttraining's l2: 0.0629904\n",
      "[1800]\ttraining's l2: 0.0606123\n",
      "[1900]\ttraining's l2: 0.0582692\n",
      "[2000]\ttraining's l2: 0.0561102\n",
      "[2100]\ttraining's l2: 0.054111\n",
      "[2200]\ttraining's l2: 0.0522296\n",
      "[2300]\ttraining's l2: 0.0505002\n",
      "[2400]\ttraining's l2: 0.0489217\n",
      "[2500]\ttraining's l2: 0.047402\n",
      "[2600]\ttraining's l2: 0.0459853\n",
      "[2700]\ttraining's l2: 0.044693\n",
      "[2800]\ttraining's l2: 0.0434071\n",
      "[2900]\ttraining's l2: 0.0421719\n",
      "[3000]\ttraining's l2: 0.0409929\n",
      "[3100]\ttraining's l2: 0.0399128\n",
      "[3200]\ttraining's l2: 0.0388825\n",
      "[3300]\ttraining's l2: 0.0378659\n",
      "[3400]\ttraining's l2: 0.0368896\n",
      "[3500]\ttraining's l2: 0.0360153\n",
      "[3600]\ttraining's l2: 0.0351372\n",
      "[3700]\ttraining's l2: 0.0343215\n",
      "[3800]\ttraining's l2: 0.0334981\n",
      "[3900]\ttraining's l2: 0.0327414\n",
      "[4000]\ttraining's l2: 0.0320062\n",
      "[4100]\ttraining's l2: 0.0312898\n",
      "[4200]\ttraining's l2: 0.0306001\n",
      "[4300]\ttraining's l2: 0.0299391\n",
      "[4400]\ttraining's l2: 0.029318\n",
      "[4500]\ttraining's l2: 0.0286962\n",
      "[4600]\ttraining's l2: 0.0280946\n",
      "[4700]\ttraining's l2: 0.0275584\n",
      "[4800]\ttraining's l2: 0.0270166\n",
      "[4900]\ttraining's l2: 0.0265021\n",
      "[5000]\ttraining's l2: 0.0259876\n",
      "      brand  store  week   logmove\n",
      "0         1      2   159  9.079488\n",
      "1         1      2   160  9.079488\n",
      "2         2      2   159  8.138813\n",
      "3         2      2   160  8.138813\n",
      "4         3      2   159  7.468866\n",
      "5         3      2   160  7.468866\n",
      "6         4      2   159  8.695110\n",
      "7         4      2   160  8.695110\n",
      "8         5      2   159  8.654415\n",
      "9         5      2   160  8.654415\n",
      "10        6      2   159  7.131029\n",
      "11        6      2   160  7.131029\n",
      "12        7      2   159  6.529311\n",
      "13        7      2   160  6.529311\n",
      "14        8      2   159  7.402959\n",
      "15        8      2   160  7.402959\n",
      "16        9      2   159  6.381606\n",
      "17        9      2   160  6.381606\n",
      "18       10      2   159  8.408419\n",
      "19       10      2   160  8.408419\n",
      "20       11      2   159  7.916009\n",
      "21       11      2   160  7.916009\n",
      "22        1      5   159  9.161731\n",
      "23        1      5   160  9.161731\n",
      "24        2      5   159  8.869293\n",
      "25        2      5   160  8.869293\n",
      "26        3      5   159  7.890637\n",
      "27        3      5   160  7.890637\n",
      "28        4      5   159  8.761906\n",
      "29        4      5   160  8.761906\n",
      "...     ...    ...   ...       ...\n",
      "1796      8    134   159  6.273938\n",
      "1797      8    134   160  6.273938\n",
      "1798      9    134   159  6.494975\n",
      "1799      9    134   160  6.494975\n",
      "1800     10    134   159  7.950208\n",
      "1801     10    134   160  7.950208\n",
      "1802     11    134   159  8.721616\n",
      "1803     11    134   160  8.721616\n",
      "1804      1    137   159  9.703238\n",
      "1805      1    137   160  9.703238\n",
      "1806      2    137   159  9.307212\n",
      "1807      2    137   160  9.307212\n",
      "1808      3    137   159  7.906730\n",
      "1809      3    137   160  7.906730\n",
      "1810      4    137   159  8.627333\n",
      "1811      4    137   160  8.627333\n",
      "1812      5    137   159  8.748087\n",
      "1813      5    137   160  8.748087\n",
      "1814      6    137   159  8.020193\n",
      "1815      6    137   160  8.020193\n",
      "1816      7    137   159  7.524165\n",
      "1817      7    137   160  7.524165\n",
      "1818      8    137   159  7.250819\n",
      "1819      8    137   160  7.250819\n",
      "1820      9    137   159  6.039066\n",
      "1821      9    137   160  6.039066\n",
      "1822     10    137   159  8.718396\n",
      "1823     10    137   160  8.718396\n",
      "1824     11    137   159  8.694926\n",
      "1825     11    137   160  8.694926\n",
      "\n",
      "[1826 rows x 4 columns]\n",
      "\n",
      "\n",
      "MAPE of current round is 85.38085750184624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_all = []\n",
    "metric_all = []\n",
    "for r in range(bs.NUM_ROUNDS): #range(bs.NUM_ROUNDS):\n",
    "    print('---- Round ' + str(r+1) + ' ----')\n",
    "    train_df = pd.read_csv(os.path.join(TRAIN_DIR, 'train_round_'+str(r+1)+'.csv'))\n",
    "    print(train_df.head(3))\n",
    "    print('')\n",
    "    # Fill missing values\n",
    "    store_list = train_df['store'].unique()\n",
    "    brand_list = train_df['brand'].unique()\n",
    "    #week_list = range(bs.TRAIN_START_WEEK, bs.TRAIN_END_WEEK_LIST[r]+1)\n",
    "    week_list = range(bs.TRAIN_START_WEEK, bs.TEST_END_WEEK_LIST[r]+1)\n",
    "    d = {'store': store_list,\n",
    "         'brand': brand_list,\n",
    "         'week': week_list}        \n",
    "    data_grid = df_from_cartesian_product(d)\n",
    "    data_filled = pd.merge(data_grid, train_df, how='left', \n",
    "                            on=['store', 'brand', 'week'])\n",
    "    print('Number of missing rows is {}'.format(data_filled[data_filled.isnull().any(axis=1)].shape[0]))\n",
    "    print('')\n",
    "    data_filled = data_filled.groupby(['store', 'brand']). \\\n",
    "                                apply(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "    \n",
    "    # Create lagged features\n",
    "    features = data_filled.groupby(['store','brand']). \\\n",
    "                            apply(lambda x: create_features(x))\n",
    "    train_fea = features[features.week <= bs.TRAIN_END_WEEK_LIST[r]].reset_index(drop=True)\n",
    "    print(train_fea.head(1))\n",
    "    print('')\n",
    "    print('Training and predicting models...')\n",
    "    evals_result = {} # to record eval results for plotting\n",
    "    dtrain = lgb.Dataset(\n",
    "                train_fea.drop('logmove', axis=1, inplace=False), \n",
    "                label = train_fea['logmove']\n",
    "    )\n",
    "    bst = lgb.train(\n",
    "        params, \n",
    "        dtrain, \n",
    "        num_boost_round = MAX_ROUNDS,\n",
    "        valid_sets = [dtrain], \n",
    "        categorical_feature = categ_fea,\n",
    "        early_stopping_rounds = 125, \n",
    "        evals_result = evals_result,\n",
    "        verbose_eval = 100\n",
    "    )\n",
    "    # Generate forecasts\n",
    "    test_fea = features[features.week >= bs.TEST_START_WEEK_LIST[r]].reset_index(drop=True)\n",
    "    pred = test_fea.groupby(['store','brand']). \\\n",
    "                    apply(lambda x: make_predictions(x, bst)). \\\n",
    "                    reset_index(drop=True)\n",
    "    print(pred)\n",
    "    print('')\n",
    "    # Evaluate prediction accuracy\n",
    "    test_df = pd.read_csv(os.path.join(TEST_DIR, 'test_round_'+str(r+1)+'.csv'))\n",
    "    metric_value = evaluate(pred, test_df)\n",
    "    print('')\n",
    "    print('MAPE of current round is {}'.format(metric_value))\n",
    "    print('')\n",
    "    # Keep the predictions and accuracy\n",
    "    pred_all.append(pred)\n",
    "    metric_all.append(metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot metrics recorded during training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e8027aa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Plot metrics recorded during training...')\n",
    "ax = lgb.plot_metric(evals_result, metric='l2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot feature importances...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e7fe0f860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Plot feature importances...')\n",
    "ax = lgb.plot_importance(bst, max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78.2321096879806,\n",
       " 76.06054868990037,\n",
       " 100.13694699100049,\n",
       " 84.50803699032221,\n",
       " 147.68133658648387,\n",
       " 145.3756747180339,\n",
       " 107.86563394849149,\n",
       " 68.94518028854979,\n",
       " 117.93093552051434,\n",
       " 118.31807915081072,\n",
       " 94.7643136827172,\n",
       " 85.38085750184624]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.0999711463876"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metric_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
