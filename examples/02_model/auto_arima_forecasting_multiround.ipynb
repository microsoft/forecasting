{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA: Autoregressive Integrated Moving Average\n",
    "\n",
    "This notebook provides an example of how to train an ARIMA model to generate point forecasts of product sales in retail. We will train an ARIMA based model on the Orange Juice dataset.\n",
    "\n",
    "An ARIMA, which stands for AutoRegressive Integrated Moving Average, model can be created using an `ARIMA(p,d,q)` model within `statsmodels` library. In this notebook, we will be using an alternative library `pmdarima`, which allows us to automatically search for optimal ARIMA parameters, within a specified range. More specifically, we will be using `auto_arima` function within `pmdarima` to automatically discover the optimal parameters for an ARIMA model. This function wraps `ARIMA` and `SARIMAX` models of `statsmodels` library, that correspond to non-seasonal and seasonal model space, respectively.\n",
    "\n",
    "In an ARIMA model there are 3 parameters that are used to help model the major aspects of a times series: seasonality, trend, and noise. These parameters are:\n",
    "- **p** is the parameter associated with the auto-regressive aspect of the model, which incorporates past values.\n",
    "- **d** is the parameter associated with the integrated part of the model, which effects the amount of differencing to apply to a time series.\n",
    "- **q** is the parameter associated with the moving average part of the model.,\n",
    "\n",
    "If our data has a seasonal component, we use a seasonal ARIMA model or `ARIMA(p,d,q)(P,D,Q)m`. In that case, we have an additional set of parameters: `P`, `D`, and `Q` which describe the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model, and `m` refers to the number of periods in each season.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scrapbook as sb\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "from fclib.common.utils import git_repo_path\n",
    "from fclib.common.plot import plot_predictions_with_history\n",
    "from fclib.evaluation.evaluation_utils import MAPE\n",
    "from fclib.dataset.ojdata import download_ojdata, split_train_test, complete_and_fill_df\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "np.set_printoptions(precision=2)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Next, we define global settings related to the model. We will use historical weekly sales data only, without any covariate features to train the ARIMA model. The model parameter ranges are provided in params. These are later used by the `auto_arima()` function to search the space for the optimal set of parameters. To increase the space of models to search over, increase the `max_p` and `max_q` parameters.\n",
    "\n",
    "> NOTE: Our data does not show a strong seasonal component (as demonstrated in data exploration example notebook), so we will not be searching over the seasonal ARIMA models. To search over the seasonal models, set `seasonal` to `True` and include `start_P`, `start_Q`, `max_P`, and `max_Q` parameters in the auto_arima() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use False if you've already downloaded and split the data\n",
    "DOWNLOAD_SPLIT_DATA = True\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = os.path.join(git_repo_path(), \"ojdata\")\n",
    "\n",
    "# Forecasting settings\n",
    "N_SPLITS = 5\n",
    "HORIZON = 2\n",
    "GAP = 2\n",
    "FIRST_WEEK = 40\n",
    "LAST_WEEK = 156\n",
    "\n",
    "# Parameters of ARIMA model\n",
    "params = {\n",
    "    \"seasonal\": False,\n",
    "    \"start_p\": 0,\n",
    "    \"start_q\": 0,\n",
    "    \"max_p\": 5,\n",
    "    \"max_q\": 5,\n",
    "    \"m\": 52,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We need to download the Orange Juice data and split it into training and test sets. By default, the following cell will download and spit the data. If you've already done so, you may skip this part by switching `DOWNLOAD_SPLIT_DATA` to `False`.\n",
    "\n",
    "We store the training data and test data using dataframes. The training data includes `train_df` and `aux_df` with `train_df` containing the historical sales up to week 135 (the time we make forecasts) and `aux_df` containing price/promotion information up until week 138. Here we assume that future price and promotion information up to a certain number of weeks ahead is predetermined and known. In our example, we will be using historical sales only, and will not be using the `aux_df` data. The test data is stored in `test_df` which contains the sales of each product in week 137 and 138. Assuming the current week is week 135, our goal is to forecast the sales in week 137 and 138 using the training data. There is a one-week gap between the current week and the first target week of forecasting as we want to leave time for planning inventory in practice.\n",
    "\n",
    "The setting of the forecast problem are defined in `fclib.dataset.ojdata.split_train_test` function. We can change this setting (e.g., modify the horizon of the forecast or the range of the historical data) by passing different parameters to this functions. Below, we split the data into `n_splits=N_SPLITS` splits, using the forecasting settings listed above in the **Parameters** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists at the specified location.\n",
      "Finished data downloading and splitting.\n"
     ]
    }
   ],
   "source": [
    "if DOWNLOAD_SPLIT_DATA:\n",
    "    download_ojdata(DATA_DIR)\n",
    "    train_df_list, test_df_list, _ = split_train_test(\n",
    "        DATA_DIR,\n",
    "        n_splits=N_SPLITS,\n",
    "        horizon=HORIZON,\n",
    "        gap=GAP,\n",
    "        first_week=FIRST_WEEK,\n",
    "        last_week=LAST_WEEK,\n",
    "        write_csv=True,\n",
    "    )\n",
    "\n",
    "    print(\"Finished data downloading and splitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create training data and test data for multi-round forecasting, we pass a number greater than `1` to `n_splits` parameter in `split_train_test()` function. Note that the forecasting periods we generate in each test round are **non-overlapping**. This allows us to evaluate the forecasting model on multiple rounds of data, and get a more robust estimate of our model's performance.\n",
    "\n",
    "For visual demonstration, this is what the time series splits would look like for `N_SPLITS = 5`, and using other settings as above:\n",
    "\n",
    "![Multi split](../../assets/time_series_split_multiround.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our time series data is not complete, since we have missing sales for some stores/products and weeks. We will fill in those missing values by propagating the last valid observation forward to next available value.\n",
    "\n",
    "Note that our time series are grouped by `store` and `brand`, while `week` represents a time step, and `logmove` represents the value to predict.\n",
    "\n",
    "We will define functions for data frame processing, then use these functions within a loop that loops over each forecasting rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training_df(train_df):\n",
    "    \"\"\"Process training data frame.\"\"\"\n",
    "    train_df = train_df[[\"store\", \"brand\", \"week\", \"logmove\"]]\n",
    "    store_list = train_df[\"store\"].unique()\n",
    "    brand_list = train_df[\"brand\"].unique()\n",
    "    train_week_list = range(FIRST_WEEK, max(train_df.week))\n",
    "\n",
    "    train_filled = complete_and_fill_df(train_df, stores=store_list, brands=brand_list, weeks=train_week_list)\n",
    "\n",
    "    return train_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now process the test data. Note that the test data runs from `LAST_WEEK - HORIZON + 1` to `LAST_WEEK`. Note that, in addition to filling out missing values, we also convert unit sales from logarithmic scale to the counts. We will do model training on the log scale, due to improved performance, however, we will transfrom the test data back into the unit scale (counts) by applying `math.exp()`, so that we can evaluate the performance on the unit scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_df(test_df):\n",
    "    \"\"\"Process test data frame.\"\"\"\n",
    "    test_df[\"actuals\"] = test_df.logmove.apply(lambda x: round(math.exp(x)))\n",
    "    test_df = test_df[[\"store\", \"brand\", \"week\", \"actuals\"]]\n",
    "    store_list = test_df[\"store\"].unique()\n",
    "    brand_list = test_df[\"brand\"].unique()\n",
    "\n",
    "    test_week_list = range(min(test_df.week), max(test_df.week) + 1)\n",
    "    test_filled = complete_and_fill_df(test_df, stores=store_list, brands=brand_list, weeks=test_week_list)\n",
    "\n",
    "    return test_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run model training across all the stores and brands, and across all rounds. We will re-run the same code to automatically search for the best parameters, simply wrapped in a for loop iterating over stores and brands.\n",
    "\n",
    "> NOTE: Since we are building a model for each time series sequentially, it will take ~11 minutes for a single round, to iterate over 900+ time series for each store and brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Round 1 ----------\n",
      "Training ARIMA model...\n",
      "Forecasting for store: 2\n",
      "Forecasting for store: 5\n",
      "Forecasting for store: 8\n",
      "Forecasting for store: 9\n",
      "Forecasting for store: 12\n",
      "Forecasting for store: 14\n",
      "Forecasting for store: 18\n",
      "Forecasting for store: 21\n",
      "Forecasting for store: 28\n",
      "Forecasting for store: 32\n",
      "Forecasting for store: 33\n",
      "Forecasting for store: 40\n",
      "Forecasting for store: 44\n",
      "Forecasting for store: 45\n",
      "Forecasting for store: 47\n",
      "Forecasting for store: 48\n",
      "Forecasting for store: 49\n",
      "Forecasting for store: 50\n",
      "Forecasting for store: 51\n",
      "Forecasting for store: 52\n",
      "Forecasting for store: 53\n",
      "Forecasting for store: 54\n",
      "Forecasting for store: 56\n",
      "Forecasting for store: 59\n",
      "Forecasting for store: 62\n",
      "Forecasting for store: 64\n",
      "Forecasting for store: 67\n",
      "Forecasting for store: 68\n",
      "Forecasting for store: 70\n",
      "Forecasting for store: 71\n",
      "Forecasting for store: 72\n",
      "Forecasting for store: 73\n",
      "Forecasting for store: 74\n",
      "Forecasting for store: 75\n",
      "Forecasting for store: 76\n",
      "Forecasting for store: 77\n",
      "Forecasting for store: 78\n",
      "Forecasting for store: 80\n",
      "Forecasting for store: 81\n",
      "Forecasting for store: 83\n",
      "Forecasting for store: 84\n",
      "Forecasting for store: 86\n",
      "Forecasting for store: 88\n",
      "Forecasting for store: 89\n",
      "Forecasting for store: 90\n",
      "Forecasting for store: 91\n",
      "Forecasting for store: 92\n",
      "Forecasting for store: 93\n",
      "Forecasting for store: 94\n",
      "Forecasting for store: 95\n",
      "Forecasting for store: 97\n",
      "Forecasting for store: 98\n",
      "Forecasting for store: 100\n",
      "Forecasting for store: 101\n",
      "Forecasting for store: 102\n",
      "Forecasting for store: 103\n",
      "Forecasting for store: 104\n",
      "Forecasting for store: 105\n",
      "Forecasting for store: 106\n",
      "Forecasting for store: 107\n",
      "Forecasting for store: 109\n",
      "Forecasting for store: 110\n",
      "Forecasting for store: 111\n",
      "Forecasting for store: 112\n",
      "Forecasting for store: 113\n",
      "Forecasting for store: 114\n",
      "Forecasting for store: 115\n",
      "Forecasting for store: 116\n",
      "Forecasting for store: 117\n",
      "Forecasting for store: 118\n",
      "Forecasting for store: 119\n",
      "Forecasting for store: 121\n",
      "Forecasting for store: 122\n",
      "Forecasting for store: 123\n",
      "Forecasting for store: 124\n",
      "Forecasting for store: 126\n",
      "Forecasting for store: 128\n",
      "Forecasting for store: 129\n",
      "Forecasting for store: 130\n",
      "Forecasting for store: 131\n",
      "Forecasting for store: 132\n",
      "Forecasting for store: 134\n",
      "Forecasting for store: 137\n",
      "---------- Round 2 ----------\n",
      "Training ARIMA model...\n",
      "Forecasting for store: 2\n",
      "Forecasting for store: 5\n",
      "Forecasting for store: 8\n",
      "Forecasting for store: 9\n",
      "Forecasting for store: 12\n",
      "Forecasting for store: 14\n",
      "Forecasting for store: 18\n",
      "Forecasting for store: 21\n",
      "Forecasting for store: 28\n",
      "Forecasting for store: 32\n",
      "Forecasting for store: 33\n",
      "Forecasting for store: 40\n",
      "Forecasting for store: 44\n",
      "Forecasting for store: 45\n",
      "Forecasting for store: 47\n",
      "Forecasting for store: 48\n",
      "Forecasting for store: 49\n",
      "Forecasting for store: 50\n",
      "Forecasting for store: 51\n",
      "Forecasting for store: 52\n",
      "Forecasting for store: 53\n",
      "Forecasting for store: 54\n",
      "Forecasting for store: 56\n",
      "Forecasting for store: 59\n",
      "Forecasting for store: 62\n",
      "Forecasting for store: 64\n",
      "Forecasting for store: 67\n",
      "Forecasting for store: 68\n",
      "Forecasting for store: 70\n",
      "Forecasting for store: 71\n",
      "Forecasting for store: 72\n",
      "Forecasting for store: 73\n",
      "Forecasting for store: 74\n",
      "Forecasting for store: 75\n",
      "Forecasting for store: 76\n",
      "Forecasting for store: 77\n",
      "Forecasting for store: 78\n",
      "Forecasting for store: 80\n",
      "Forecasting for store: 81\n",
      "Forecasting for store: 83\n",
      "Forecasting for store: 84\n",
      "Forecasting for store: 97\n",
      "Forecasting for store: 98\n",
      "Forecasting for store: 100\n",
      "Forecasting for store: 101\n",
      "Forecasting for store: 102\n",
      "Forecasting for store: 103\n",
      "Forecasting for store: 104\n",
      "Forecasting for store: 105\n",
      "Forecasting for store: 106\n",
      "Forecasting for store: 107\n",
      "Forecasting for store: 109\n",
      "Forecasting for store: 110\n",
      "Forecasting for store: 111\n",
      "Forecasting for store: 112\n",
      "Forecasting for store: 113\n",
      "Forecasting for store: 114\n",
      "Forecasting for store: 115\n",
      "Forecasting for store: 116\n",
      "Forecasting for store: 117\n",
      "Forecasting for store: 118\n",
      "Forecasting for store: 119\n",
      "Forecasting for store: 121\n",
      "Forecasting for store: 122\n",
      "Forecasting for store: 123\n",
      "Forecasting for store: 124\n",
      "Forecasting for store: 126\n",
      "Forecasting for store: 128\n",
      "Forecasting for store: 129\n",
      "Forecasting for store: 130\n",
      "Forecasting for store: 131\n",
      "Forecasting for store: 132\n",
      "Forecasting for store: 134\n",
      "Forecasting for store: 137\n",
      "---------- Round 3 ----------\n",
      "Training ARIMA model...\n",
      "Forecasting for store: 2\n",
      "Forecasting for store: 5\n",
      "Forecasting for store: 8\n",
      "Forecasting for store: 9\n",
      "Forecasting for store: 12\n",
      "Forecasting for store: 14\n",
      "Forecasting for store: 18\n",
      "Forecasting for store: 21\n",
      "Forecasting for store: 28\n",
      "Forecasting for store: 32\n",
      "Forecasting for store: 33\n",
      "Forecasting for store: 40\n",
      "Forecasting for store: 44\n",
      "Forecasting for store: 45\n",
      "Forecasting for store: 47\n",
      "Forecasting for store: 48\n",
      "Forecasting for store: 49\n",
      "Forecasting for store: 50\n",
      "Forecasting for store: 51\n",
      "Forecasting for store: 52\n",
      "Forecasting for store: 53\n",
      "Forecasting for store: 54\n",
      "Forecasting for store: 56\n",
      "Forecasting for store: 59\n",
      "Forecasting for store: 62\n",
      "Forecasting for store: 64\n",
      "Forecasting for store: 67\n",
      "Forecasting for store: 68\n",
      "Forecasting for store: 70\n",
      "Forecasting for store: 71\n",
      "Forecasting for store: 72\n",
      "Forecasting for store: 73\n",
      "Forecasting for store: 74\n",
      "Forecasting for store: 75\n",
      "Forecasting for store: 76\n",
      "Forecasting for store: 77\n",
      "Forecasting for store: 78\n",
      "Forecasting for store: 80\n",
      "Forecasting for store: 81\n",
      "Forecasting for store: 83\n",
      "Forecasting for store: 84\n",
      "Forecasting for store: 86\n",
      "Forecasting for store: 88\n",
      "Forecasting for store: 89\n",
      "Forecasting for store: 90\n",
      "Forecasting for store: 91\n",
      "Forecasting for store: 92\n",
      "Forecasting for store: 93\n",
      "Forecasting for store: 94\n",
      "Forecasting for store: 95\n",
      "Forecasting for store: 97\n",
      "Forecasting for store: 98\n",
      "Forecasting for store: 100\n",
      "Forecasting for store: 101\n",
      "Forecasting for store: 102\n",
      "Forecasting for store: 103\n",
      "Forecasting for store: 104\n",
      "Forecasting for store: 105\n",
      "Forecasting for store: 106\n",
      "Forecasting for store: 107\n",
      "Forecasting for store: 109\n",
      "Forecasting for store: 110\n",
      "Forecasting for store: 111\n",
      "Forecasting for store: 112\n",
      "Forecasting for store: 113\n",
      "Forecasting for store: 114\n",
      "Forecasting for store: 115\n",
      "Forecasting for store: 116\n",
      "Forecasting for store: 117\n",
      "Forecasting for store: 118\n",
      "Forecasting for store: 119\n",
      "Forecasting for store: 121\n",
      "Forecasting for store: 122\n",
      "Forecasting for store: 123\n",
      "Forecasting for store: 124\n",
      "Forecasting for store: 126\n",
      "Forecasting for store: 128\n",
      "Forecasting for store: 129\n",
      "Forecasting for store: 130\n",
      "Forecasting for store: 131\n",
      "Forecasting for store: 132\n",
      "Forecasting for store: 134\n",
      "Forecasting for store: 137\n",
      "---------- Round 4 ----------\n",
      "Training ARIMA model...\n",
      "Forecasting for store: 2\n",
      "Forecasting for store: 5\n",
      "Forecasting for store: 8\n",
      "Forecasting for store: 9\n",
      "Forecasting for store: 12\n",
      "Forecasting for store: 14\n",
      "Forecasting for store: 18\n",
      "Forecasting for store: 21\n",
      "Forecasting for store: 28\n",
      "Forecasting for store: 32\n",
      "Forecasting for store: 33\n",
      "Forecasting for store: 40\n",
      "Forecasting for store: 44\n",
      "Forecasting for store: 45\n",
      "Forecasting for store: 47\n",
      "Forecasting for store: 48\n",
      "Forecasting for store: 49\n",
      "Forecasting for store: 50\n",
      "Forecasting for store: 51\n",
      "Forecasting for store: 52\n",
      "Forecasting for store: 53\n",
      "Forecasting for store: 54\n",
      "Forecasting for store: 56\n",
      "Forecasting for store: 59\n",
      "Forecasting for store: 62\n",
      "Forecasting for store: 64\n",
      "Forecasting for store: 67\n",
      "Forecasting for store: 68\n",
      "Forecasting for store: 70\n",
      "Forecasting for store: 71\n",
      "Forecasting for store: 72\n",
      "Forecasting for store: 73\n",
      "Forecasting for store: 74\n",
      "Forecasting for store: 75\n",
      "Forecasting for store: 76\n",
      "Forecasting for store: 77\n",
      "Forecasting for store: 78\n",
      "Forecasting for store: 80\n",
      "Forecasting for store: 81\n",
      "Forecasting for store: 83\n",
      "Forecasting for store: 84\n",
      "Forecasting for store: 86\n",
      "Forecasting for store: 88\n",
      "Forecasting for store: 89\n",
      "Forecasting for store: 90\n",
      "Forecasting for store: 91\n",
      "Forecasting for store: 92\n",
      "Forecasting for store: 93\n",
      "Forecasting for store: 94\n",
      "Forecasting for store: 95\n",
      "Forecasting for store: 97\n",
      "Forecasting for store: 98\n",
      "Forecasting for store: 100\n",
      "Forecasting for store: 101\n",
      "Forecasting for store: 102\n",
      "Forecasting for store: 103\n",
      "Forecasting for store: 104\n",
      "Forecasting for store: 105\n",
      "Forecasting for store: 106\n",
      "Forecasting for store: 107\n",
      "Forecasting for store: 109\n",
      "Forecasting for store: 110\n",
      "Forecasting for store: 111\n",
      "Forecasting for store: 112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasting for store: 113\n",
      "Forecasting for store: 114\n",
      "Forecasting for store: 115\n",
      "Forecasting for store: 116\n",
      "Forecasting for store: 117\n",
      "Forecasting for store: 118\n",
      "Forecasting for store: 119\n",
      "Forecasting for store: 121\n",
      "Forecasting for store: 122\n",
      "Forecasting for store: 123\n",
      "Forecasting for store: 124\n",
      "Forecasting for store: 126\n",
      "Forecasting for store: 128\n",
      "Forecasting for store: 129\n",
      "Forecasting for store: 130\n",
      "Forecasting for store: 131\n",
      "Forecasting for store: 132\n",
      "Forecasting for store: 134\n",
      "Forecasting for store: 137\n",
      "---------- Round 5 ----------\n",
      "Training ARIMA model...\n",
      "Forecasting for store: 2\n",
      "Forecasting for store: 5\n",
      "Forecasting for store: 8\n",
      "Forecasting for store: 9\n",
      "Forecasting for store: 12\n",
      "Forecasting for store: 14\n",
      "Forecasting for store: 18\n",
      "Forecasting for store: 21\n",
      "Forecasting for store: 28\n",
      "Forecasting for store: 32\n",
      "Forecasting for store: 33\n",
      "Forecasting for store: 40\n",
      "Forecasting for store: 44\n",
      "Forecasting for store: 45\n",
      "Forecasting for store: 47\n",
      "Forecasting for store: 48\n",
      "Forecasting for store: 49\n",
      "Forecasting for store: 50\n",
      "Forecasting for store: 51\n",
      "Forecasting for store: 52\n",
      "Forecasting for store: 53\n",
      "Forecasting for store: 54\n",
      "Forecasting for store: 56\n",
      "Forecasting for store: 59\n",
      "Forecasting for store: 62\n",
      "Forecasting for store: 64\n",
      "Forecasting for store: 67\n",
      "Forecasting for store: 68\n",
      "Forecasting for store: 70\n",
      "Forecasting for store: 71\n",
      "Forecasting for store: 72\n",
      "Forecasting for store: 73\n",
      "Forecasting for store: 74\n",
      "Forecasting for store: 75\n",
      "Forecasting for store: 76\n",
      "Forecasting for store: 77\n",
      "Forecasting for store: 78\n",
      "Forecasting for store: 80\n",
      "Forecasting for store: 81\n",
      "Forecasting for store: 83\n",
      "Forecasting for store: 84\n",
      "Forecasting for store: 86\n",
      "Forecasting for store: 88\n",
      "Forecasting for store: 89\n",
      "Forecasting for store: 90\n",
      "Forecasting for store: 91\n",
      "Forecasting for store: 92\n",
      "Forecasting for store: 93\n",
      "Forecasting for store: 94\n",
      "Forecasting for store: 95\n",
      "Forecasting for store: 97\n",
      "Forecasting for store: 98\n",
      "Forecasting for store: 100\n",
      "Forecasting for store: 101\n",
      "Forecasting for store: 102\n",
      "Forecasting for store: 103\n",
      "Forecasting for store: 104\n",
      "Forecasting for store: 105\n",
      "Forecasting for store: 106\n",
      "Forecasting for store: 107\n",
      "Forecasting for store: 109\n",
      "Forecasting for store: 110\n",
      "Forecasting for store: 111\n",
      "Forecasting for store: 112\n",
      "Forecasting for store: 113\n",
      "Forecasting for store: 114\n",
      "Forecasting for store: 115\n",
      "Forecasting for store: 116\n",
      "Forecasting for store: 117\n",
      "Forecasting for store: 118\n",
      "Forecasting for store: 119\n",
      "Forecasting for store: 121\n",
      "Forecasting for store: 122\n",
      "Forecasting for store: 123\n",
      "Forecasting for store: 124\n",
      "Forecasting for store: 126\n",
      "Forecasting for store: 128\n",
      "Forecasting for store: 129\n",
      "Forecasting for store: 130\n",
      "Forecasting for store: 131\n",
      "Forecasting for store: 132\n",
      "Forecasting for store: 134\n",
      "Forecasting for store: 137\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train and predict for all forecast rounds\n",
    "\n",
    "# Create an empty df to store predictions\n",
    "result_df = pd.DataFrame(None, columns=[\"predictions\", \"store\", \"brand\", \"week\", \"actuals\", \"round\"])\n",
    "\n",
    "for r in range(N_SPLITS):\n",
    "    print(\"---------- Round \" + str(r + 1) + \" ----------\")\n",
    "\n",
    "    # Process training data set\n",
    "    train_df = train_df_list[r].reset_index()\n",
    "    train_filled = process_training_df(train_df)\n",
    "\n",
    "    # Process test data set\n",
    "    test_df = test_df_list[r].reset_index()\n",
    "    test_filled = process_test_df(test_df)\n",
    "\n",
    "    print(\"Training ARIMA model...\")\n",
    "    store_list = train_filled[\"store\"].unique()\n",
    "    brand_list = train_filled[\"brand\"].unique()\n",
    "    # store_list = store_list[0:10]\n",
    "    for store, brand in itertools.product(store_list, brand_list):\n",
    "\n",
    "        if brand == 1:\n",
    "            print(f\"Forecasting for store: {store}\")\n",
    "\n",
    "        train_ts = train_filled.loc[(train_filled.store == store) & (train_filled.brand == brand)]\n",
    "        train_ts = np.array(train_ts[\"logmove\"])\n",
    "\n",
    "        model = auto_arima(\n",
    "            train_ts,\n",
    "            seasonal=params[\"seasonal\"],\n",
    "            start_p=params[\"start_p\"],\n",
    "            start_q=params[\"start_q\"],\n",
    "            max_p=params[\"max_p\"],\n",
    "            max_q=params[\"max_q\"],\n",
    "            stepwise=True,\n",
    "        )\n",
    "\n",
    "        model.fit(train_ts)\n",
    "        preds = model.predict(n_periods=GAP + HORIZON - 1)\n",
    "        predictions = np.round(np.exp(preds[-HORIZON:]))\n",
    "\n",
    "        test_week_list = range(min(test_filled.week), max(test_filled.week) + 1)\n",
    "        pred_df = pd.DataFrame(\n",
    "            {\"predictions\": predictions, \"store\": store, \"brand\": brand, \"week\": test_week_list, \"round\": r + 1,}\n",
    "        )\n",
    "        test_ts = test_filled.loc[(test_filled.store == store) & (test_filled.brand == brand)]\n",
    "\n",
    "        combined_df = pd.merge(pred_df, test_ts, on=[\"store\", \"brand\", \"week\"], how=\"left\")\n",
    "\n",
    "        result_df = result_df.append(combined_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Since `auto_arima` model makes consecutive forecasts from the last time point, we want to forecast the next `n_periods = GAP + HORIZON - 1` points, so that we can account for the GAP, as described in the data setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will use *mean absolute percentage error* or **MAPE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = MAPE(result_df.predictions, result_df.actuals) * 100\n",
    "sb.glue(\"MAPE\", metric_value)\n",
    "\n",
    "print(f\"MAPE of the forecasts across all rounds is {metric_value} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `MAPE` for each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_r = result_df.groupby(\"round\").apply(lambda x: MAPE(x.predictions, x.actuals) * 100)\n",
    "\n",
    "print(\"MAPE values for each forecasting round are:\")\n",
    "print(mape_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a model with `auto_arima` for a large number of time series, it is often difficult to examine each model individually (in a similar way we did for the single time series above). As `auto_arima` searches a restricted space of the models, defined by the range of `p` and `q` parameters, we often might not find an optimal model for each time series.\n",
    "\n",
    "Let's plot a few examples of forecasted results from the last round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 6\n",
    "min_week = 120\n",
    "sales = pd.read_csv(os.path.join(DATA_DIR, \"yx.csv\"))\n",
    "sales[\"move\"] = sales.logmove.apply(lambda x: round(math.exp(x)) if x > 0 else 0)\n",
    "\n",
    "result_df = result_df[result_df[\"round\"] == 1]\n",
    "\n",
    "result_df[\"move\"] = result_df.predictions\n",
    "plot_predictions_with_history(\n",
    "    result_df,\n",
    "    sales,\n",
    "    grain1_unique_vals=store_list,\n",
    "    grain2_unique_vals=brand_list,\n",
    "    time_col_name=\"week\",\n",
    "    target_col_name=\"move\",\n",
    "    grain1_name=\"store\",\n",
    "    grain2_name=\"brand\",\n",
    "    min_timestep=min_week,\n",
    "    num_samples=num_samples,\n",
    "    predict_at_timestep=145,\n",
    "    line_at_predict_time=True,\n",
    "    title=\"Prediction results for a few sample time series (predictions are made at week 145)\",\n",
    "    x_label=\"week\",\n",
    "    y_label=\"unit sales\",\n",
    "    random_seed=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting_env",
   "language": "python",
   "name": "forecasting_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
