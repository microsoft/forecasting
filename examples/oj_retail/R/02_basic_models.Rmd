---
title: Basic models
output: html_notebook
encoding: utf8
---

```{r, echo=FALSE, results="hide", message=FALSE}
library(tidyr)
library(dplyr)
library(tsibble)
library(feasts)
library(fable)
```

We fit some simple models to the orange juice data. One model is fit for each combination of store and brand.

- `mean`: This is just a simple mean.
- `naive`: A random walk model without any other components. This amounts to setting all forecast values to the last observed value.
- `drift`: This adjusts the `naive` model to incorporate a trend.
- `arima`: An ARIMA model with the parameter values estimated from the data.
- `ets`: An exponentially weighted model, again with parameter values estimated from the data.

Note that the model training process is embarrassingly parallel on 3 levels:

- We have multiple independent training datasets;
- For which we fit multiple independent models;
- Within which we have independent sub-models for each store and brand.

This lets us speed up the training significantly. While the `fable::model` function can fit multiple models in parallel, we will run it sequentially here and instead parallelise by dataset. This avoids contention for cores, and also results in the simplest code.

First, we fit the models that can innately handle missing values.

```{r}
source("../../../fclib_R/cluster.R")
source("../../../fclib_R/model_eval.R")

load("oj_data.Rdata")

cl <- make_cluster(libs=c("tidyr", "dplyr", "fable", "tsibble", "feasts"))

oj_modelset_basic <- parallel::parLapply(cl, oj_train, function(df)
{
    model(df,
        mean=MEAN(logmove),
        naive=NAIVE(logmove),
        drift=RW(logmove ~ drift()),
        arima=ARIMA(logmove ~ pdq() + PDQ(0, 0, 0)),
        .safely=FALSE
    )
})
oj_fcast_basic <- parallel::clusterMap(cl, get_forecasts, oj_modelset_basic, oj_test)

save(oj_modelset_basic, oj_fcast_basic, file="oj_model_basic.Rdata")

destroy_cluster(cl)

do.call(rbind, oj_fcast_basic) %>%
    mutate_at(-(1:3), exp) %>%
    eval_forecasts()
```

