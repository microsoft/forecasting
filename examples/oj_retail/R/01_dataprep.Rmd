---
title: Data preparation
output: html_notebook
---

_Copyright (c) Microsoft Corporation._<br/>
_Licensed under the MIT License._

In this notebook, we generate the datasets that will be used for model training and validating. 

The orange juice dataset comes from the bayesm package, and gives pricing and sales figures over time for a variety of orange juice brands in several stores in Florida. Rather than installing the entire package (which is very complex), we download the dataset itself from the GitHub mirror of the CRAN repository.

```{r, results="hide", message=FALSE}
# download the data from the GitHub mirror of the bayesm package source
ojfile <- tempfile(fileext=".rda")
download.file("https://github.com/cran/bayesm/raw/master/data/orangeJuice.rda", ojfile)
load(ojfile)
```

The dataset generation parameters are obtained from the file `ojdata_forecast_settings.yaml`; you can modify that file to vary the experimental setup. The settings are

- `N_SPLITS`: the number of splits to make.
- `HORIZON`: the forecast horizon for the test dataset for each split.
- `GAP`: the gap in weeks from the end of the training period to the start of the testing period; see below.
- `FIRST_WEEK`: the first week of data to use.
- `LAST_WEEK`: the last week of data to use.
- `START_DATE`: the actual calendar date for the start of the first week in the data.

As part of the modelling, we also compute a new column `maxpricediff`, the log-ratio of the price of this brand compared to the best competing price. A positive `maxpricediff` means this brand is cheaper than all the other brands, and a negative `maxpricediff` means it is more expensive.

A complicating factor is that the data does not include every possible combination of store, brand and date, so we have to pad out the missing rows with `complete`. In addition, one store/brand combination has no data beyond week 156; we therefore end the analysis at this week. We also do _not_ fill in the missing values in the data, as many of the modelling functions in the fable package can handle this innately via `interpolate`.

```{r, results="hide", message=FALSE}
library(tidyr)
library(dplyr)
library(tsibble)
library(feasts)
library(fable)

settings <- yaml::read_yaml(here::here("examples/oj_retail/R/forecast_settings.yaml"))
start_date <- as.Date(settings$START_DATE)
train_periods <- seq(to=settings$LAST_WEEK - settings$HORIZON - settings$GAP + 1,
                     by=settings$HORIZON,
                     length.out=settings$N_SPLITS)

oj_data <- orangeJuice$yx %>%
    complete(store, brand, week) %>%
    group_by(store, brand) %>%
    group_modify(~ {
        pricevars <- grep("price", names(.x), value=TRUE)
        thispricevar <- paste0("price", .y$brand)
        best_other_price <- do.call(pmin, .x[setdiff(pricevars, thispricevar)])
        .x$maxpricediff <- log(best_other_price/.x[[thispricevar]])
        .x
    }) %>%
    ungroup() %>%
    mutate(week=yearweek(start_date + week*7)) %>%  # do this separately because of tsibble/vctrs issues
    as_tsibble(index=week, key=c(store, brand))
```

Here are some glimpses of what the data looks like. The dependent variable is `logmove`, the logarithm of the total sales for a given brand and store, in a particular week.

```{r}
head(oj_data)
```

The time series plots for a small subset of brands and stores are shown below. It is clear that the statistical behaviour of the data varies by store and brand.

```{r}
library(ggplot2)

oj_data %>%
    filter(store < 10, brand < 5) %>%
    ggplot(aes(x=week, y=logmove)) +
        geom_line() +
        scale_x_date(labels=NULL) +
        facet_grid(vars(store), vars(brand), labeller="label_both")
```

Finally, we split the dataset into separate samples for training and testing. The schema used is broadly time series cross-validation, whereby we train a model on data up to time $t$, test it on data for times $t+1$ to $t+k$, then train on data up to time $t+k$, test it on data for times $t+k+1$ to $t+2k$, and so on. In this specific case study, however, we introduce a small extra piece of complexity based on discussions with domain experts. We train a model on data up to week $t$, then test it on week $t+2$ to $t+3$. Then we train on data up to week $t+2$, and test it on weeks $t+4$ to $t+5$, and so on. Thus there is always a gap of one week between the training and test samples. The reason for this is because in reality, inventory planning always takes some time; the gap allows store managers to prepare the stock based on the forecasted demand.

```{r}
subset_oj_data <- function(start, end)
{
    start <- yearweek(start_date + start*7)
    end <- yearweek(start_date + end*7)
    filter(oj_data, week >= start, week <= end)
}

oj_train <- lapply(train_periods, function(i) subset_oj_data(settings$FIRST_WEEK, i))
oj_test <- lapply(train_periods, function(i) subset_oj_data(i + settings$GAP, i + settings$GAP + settings$HORIZON - 1))

save(oj_train, oj_test, file=here::here("examples/oj_retail/R/oj_data.Rdata"))

head(oj_train[[1]])

head(oj_test[[1]])
```
