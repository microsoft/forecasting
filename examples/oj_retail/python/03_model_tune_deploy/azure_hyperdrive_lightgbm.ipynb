{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>\n",
    "\n",
    "<i> Created by Chenhui Hu </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning and  Deployment using Azure Machine Learning Service\n",
    "\n",
    "\n",
    "In this notebook, we perform hyperparameter tuning of a LightGBM retail sales forecast model using HyperDrive in Azure Machine Learning (AzureML). After the optimal hyperparameters are found, we further deploy the best model as a web service on Azure.\n",
    "\n",
    "To tune the hyperparameters, we carry out cross-validation with the Orange Juice data from week 40 to week 135. Specifically, we split the data into a training set and a validation set. Then, we train LightGBM models with different sets of hyperparameters on the training set and evaluate the accuracy of each model on the validation set. The set of hyperparameters which yield the best validation accuracy will be used to train forecast models when the data beyond week 135 is available, e.g., in the multi-round training examples provided in [examples/02_model](../02_model).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you need to start from a conda environment where AzureML SDK is installed. In our case, we can first activate `forecasting_env` environment by\n",
    "```\n",
    "conda activate forecasting_env\n",
    "```\n",
    "as we have installed AzureML SDK in this environment. Then, we can start the notebook via\n",
    "```\n",
    "jupyter notebook --no-browswers\n",
    "```\n",
    "In addition, you need to install and enable AzureML widget extension in your environment by running the following commands.\n",
    "```\n",
    "jupyter nbextension install --py --user azureml.widgets\n",
    "jupyter nbextension enable --py --user azureml.widgets\n",
    "```\n",
    "\n",
    "Besides, you need to create an AzureML workspace and download its configuration file (`config.json`) by following the instructions in [configuration.ipynb](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext blackcellmagic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing /data/anaconda/envs/forecasting_env/lib/python3.6/site-packages/azureml/widgets/static -> azureml_widgets\n",
      "Up to date: /data/home/chenhui/.local/share/jupyter/nbextensions/azureml_widgets/index.js\n",
      "Up to date: /data/home/chenhui/.local/share/jupyter/nbextensions/azureml_widgets/extension.js\n",
      "Up to date: /data/home/chenhui/.local/share/jupyter/nbextensions/azureml_widgets/packages/labextension/azureml_widgets-1.1.0.tgz\n",
      "- Validating: \u001b[32mOK\u001b[0m\n",
      "\n",
      "    To initialize this nbextension in the browser every time the notebook (or other app) loads:\n",
      "    \n",
      "          jupyter nbextension enable azureml.widgets --user --py\n",
      "    \n",
      "Enabling notebook extension azureml_widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install and enable AzureML widgets\n",
    "!jupyter nbextension install --py --user azureml.widgets\n",
    "!jupyter nbextension enable --py --user azureml.widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n",
      "Azure ML SDK Version:  1.0.85\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import azureml\n",
    "import requests\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from azureml.core import (\n",
    "    Experiment,\n",
    "    ScriptRunConfig,\n",
    ")\n",
    "from azureml.telemetry import set_diagnostics_collection\n",
    "from azureml.core.runconfig import (\n",
    "    RunConfiguration,\n",
    "    EnvironmentDefinition,\n",
    "    CondaDependencies,\n",
    ")\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.hyperdrive import (\n",
    "    BayesianParameterSampling,\n",
    "    HyperDriveConfig,\n",
    "    quniform,\n",
    "    uniform,\n",
    "    choice,\n",
    "    PrimaryMetricGoal,\n",
    ")\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import (\n",
    "    Model,\n",
    "    InferenceConfig,\n",
    ")\n",
    "from fclib.common.utils import git_repo_path\n",
    "from fclib.azureml.azureml_utils import (\n",
    "    get_or_create_workspace,\n",
    "    get_or_create_amlcompute,\n",
    ")\n",
    "from fclib.dataset.ojdata import download_ojdata, split_train_test\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "if cur_dir not in sys.path:\n",
    "    sys.path.append(cur_dir)\n",
    "from aml_scripts.utils import create_features\n",
    "\n",
    "# Opt-in diagnostics for better experience of future releases\n",
    "set_diagnostics_collection(send_diagnostics=True)\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use False if you've already downloaded and split the data\n",
    "DOWNLOAD_SPLIT_DATA = False #True\n",
    "\n",
    "# Get data directory\n",
    "DATA_DIR = os.path.join(git_repo_path(), \"ojdata\")\n",
    "\n",
    "# Forecasting settings\n",
    "N_SPLITS = 1\n",
    "HORIZON = 2\n",
    "GAP = 2\n",
    "FIRST_WEEK = 40\n",
    "LAST_WEEK = 138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace & Create an AzureML Experiment\n",
    "\n",
    "Initialize a [Machine Learning Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the workspace you created in the Prerequisites step. `get_or_create_workspace()` below creates a workspace object from the details stored in `config.json` that you have downloaded. We assume that you store this config file to a directory `./.azureml`. In case the existing workspace cannot be loaded, the following cell will try to create a new workspace with the subscription ID, resource group, and workspace name as specified in the beginning of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: chhamlws\n",
      "Azure region: westcentralus\n",
      "Resource group: chhamlwsrg\n"
     ]
    }
   ],
   "source": [
    "# Please specify the AzureML workspace attributes below if you want to create a new one.\n",
    "subscription_id = \"<subscription-id>\"\n",
    "resource_group = \"<resource-group>\"\n",
    "workspace_name = \"<workspace-name>\"\n",
    "workspace_region = \"<workspace-region>\"\n",
    "\n",
    "# Connect to a workspace\n",
    "ws = get_or_create_workspace(\n",
    "    config_path=\"./.azureml\",\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group=resource_group,\n",
    "    workspace_name=workspace_name,\n",
    "    workspace_region=workspace_region,\n",
    ")\n",
    "print(\n",
    "    \"Workspace name: \" + ws.name,\n",
    "    \"Azure region: \" + ws.location,\n",
    "    \"Resource group: \" + ws.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment\n",
    "exp = Experiment(workspace=ws, name=\"tune-lgbm-forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We need to download the Orange Juice data and split it into training and test sets. By default, the following cell will download and spit the data. If you've already done so, you may skip this part by switching `DOWNLOAD_SPLIT_DATA` to False. \n",
    "\n",
    "By passing `write_csv=True` to `split_train_test()` below, this function will write the training data and test data to three csv files: `train.csv`, `auxi.csv` and `test.csv`. The first two csv files contain the historical sales up to week 135 as well as auxiliary information such as future price and promotion. Here we assume that future price and promotion information up to a certain number of weeks ahead is predetermined and known. We will use these two files to implement cross-validation and search for the best model with HyperDrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_SPLIT_DATA:\n",
    "    download_ojdata(DATA_DIR)\n",
    "    split_train_test(\n",
    "        DATA_DIR,\n",
    "        n_splits=N_SPLITS,\n",
    "        horizon=HORIZON,\n",
    "        gap=GAP,\n",
    "        first_week=FIRST_WEEK,\n",
    "        last_week=LAST_WEEK,\n",
    "        write_csv=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Script Locally\n",
    "\n",
    "A good practice is to test the model training and validation script on your local machine before you run the hyperparameter tuning job on a remote compute. To run the script locally, we need to correctly specify the path of the Python interpreter that has been installed in `forecasting_env` conda environment. In what follows, the script `train_validate.py` trains a model on the training set with the input arguments as specified in `ScriptRunConfig()` and computes the accuracy of the model on the validation set. Here we evaluate the model accuracy using mean-absolute-percentage-error (MAPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Python interpreter path\n",
    "python_path = subprocess.check_output(\"which python\", shell=True)\n",
    "python_path = python_path.decode(\"utf-8\")[:-1]\n",
    "\n",
    "# Configure local, user managed environment\n",
    "run_config_user_managed = RunConfiguration()\n",
    "run_config_user_managed.environment.python.user_managed_dependencies = True\n",
    "run_config_user_managed.environment.python.interpreter_path = python_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the local scripts\n",
    "script_folder = \"./aml_scripts\"\n",
    "\n",
    "# Training script name and path\n",
    "train_script_name = \"train_validate.py\"\n",
    "train_script_path = os.path.join(script_folder, train_script_name)\n",
    "\n",
    "# Specify script run config\n",
    "src = ScriptRunConfig(\n",
    "    source_directory=\"./\",\n",
    "    script=train_script_path,\n",
    "    arguments=[\"--data-folder\", DATA_DIR, \"--bagging-fraction\", \"0.8\"],\n",
    "    run_config=run_config_user_managed,\n",
    ")\n",
    "run_local = exp.submit(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check job status\n",
    "run_local.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will wait until the local run finishes. Then, we print out the validation metric. Moreover, you can also use `run_local.get_details()` to get detailed information about this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAPE': 63.79837613183825}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "while run_local.get_status() != \"Completed\":\n",
    "    {}\n",
    "run_local.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Script on Remote Compute\n",
    "\n",
    "After validating model training script locally, we can create a remote compute and further test the script on the remote compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a CPU cluster as compute target\n",
    "\n",
    "In the next cell, we create an AmlCompute target with a specific cluster name, VM size, and maximum number of nodes if the cluster does not exist. Otherwise, we will reuse an existing one. For more options of VM sizes, you can check information in this [link](https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found compute target: cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "# VM Size\n",
    "vm_size = \"STANDARD_D2_V2\"\n",
    "# Maximum number of nodes of the cluster\n",
    "max_nodes = 4\n",
    "\n",
    "# Create a new AmlCompute if it does not exist or reuse an existing one\n",
    "compute_target = get_or_create_amlcompute(\n",
    "    workspace=ws,\n",
    "    compute_name=cluster_name,\n",
    "    vm_size=vm_size,\n",
    "    min_nodes=0,\n",
    "    max_nodes=max_nodes,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Docker environment\n",
    "\n",
    "The remote compute will need to create a [Docker image](https://docs.docker.com/get-started/) for running the script. The Docker image is an encapsulated environment with necessary dependencies installed. In the following cell, we specify the conda packages and Python version that are needed for running the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvironmentDefinition()\n",
    "env.python.user_managed_dependencies = False\n",
    "env.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=[\"pandas\", \"numpy\", \"scipy\", \"scikit-learn\", \"lightgbm\", \"joblib\"],\n",
    "    python_version=\"3.6.2\",\n",
    ")\n",
    "env.python.conda_dependencies.add_channel(\"conda-forge\")\n",
    "env.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to default datastore\n",
    "\n",
    "Each workspace comes with a default datastore. In the following, we upload the Orange Juice dataset to the workspace's default datastore, which will later be mounted on the cluster for model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastore type: AzureBlob\n",
      "Account name: chhamlws4931040064\n",
      "Container name: azureml-blobstore-f799a640-1ca3-4877-ad24-08eef7bd307e\n"
     ]
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(\n",
    "    \"Datastore type: \" + ds.datastore_type,\n",
    "    \"Account name: \" + ds.account_name,\n",
    "    \"Container name: \" + ds.container_name,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_cf8c449e8b1f468fa912bdeb311cd90a"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remote data path\n",
    "path_on_datastore = \"data\"\n",
    "ds.upload(\n",
    "    src_dir=DATA_DIR,\n",
    "    target_path=path_on_datastore,\n",
    "    overwrite=True,\n",
    "    show_progress=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AZUREML_DATAREFERENCE_f9d2a744355a4ce3aadabbb8697ed14a\n"
     ]
    }
   ],
   "source": [
    "# Get data reference object for the data path\n",
    "ds_data = ds.path(path_on_datastore)\n",
    "print(ds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create estimator\n",
    "\n",
    "Next, we will check if the remote compute target is successfully created by submitting a job to the target. This compute target will be used by HyperDrive for hyperparameter tuning later. Note that you may skip this part and directly go to [Tune Hyperparameters using HyperDrive](#tune-hyperparameters-using-hyperdrive) if you want.\n",
    "\n",
    "In the following cells, we first create an estimator to specify details of the job. Then we sumbit the job to the remote compute and check the status of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\"--data-folder\": ds_data.as_mount(), \"--bagging-fraction\": 0.8}\n",
    "est = Estimator(\n",
    "    source_directory=script_folder,\n",
    "    script_params=script_params,\n",
    "    compute_target=compute_target,\n",
    "    use_docker=True,\n",
    "    entry_script=train_script_name,\n",
    "    environment_definition=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to remote compute\n",
    "run_remote = exp.submit(config=est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check job status\n",
    "\n",
    "You can monitor the status of the remote run using the AzureML widgets. After the job is done, the following cell will display a dashboard similar as\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/20047467/76150936-67fc7c00-607d-11ea-9354-418bd5f733d6.png\" width=\"900\" height=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a47a2da5fa44f7a3357edaded37340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/tune-lgbm-forecast/runs/tune-lgbm-forecast_1584053683_fcc053f7?wsid=/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourcegroups/chhamlwsrg/workspaces/chhamlws\", \"run_id\": \"tune-lgbm-forecast_1584053683_fcc053f7\", \"run_properties\": {\"run_id\": \"tune-lgbm-forecast_1584053683_fcc053f7\", \"created_utc\": \"2020-03-12T22:54:47.100773Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"777da34e-21db-4ebf-b773-b899054ac4fa\", \"azureml.git.repository_uri\": \"git@github.com:microsoft/forecasting.git\", \"mlflow.source.git.repoURL\": \"git@github.com:microsoft/forecasting.git\", \"azureml.git.branch\": \"chenhui/hyperdrive_lightgbm\", \"mlflow.source.git.branch\": \"chenhui/hyperdrive_lightgbm\", \"azureml.git.commit\": \"ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d\", \"mlflow.source.git.commit\": \"ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d\", \"azureml.git.dirty\": \"True\", \"AzureML.DerivedImageName\": \"azureml/azureml_7842fd2c5e99a43f1cca1341b66a0ecb\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-03-12T22:56:48.526683Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/azureml-logs/55_azureml-execution-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt?sv=2019-02-02&sr=b&sig=80maDl5nHoPPDp9FC2EglPNMWcr86mnYq86PHV6brkY%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/azureml-logs/65_job_prep-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt?sv=2019-02-02&sr=b&sig=jtrlktf5Bq9M115Hb8tnDhNzohxxEwoVlVu%2FtoW2agY%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=IhvjiEQ8Kg%2Bx%2FOeQ2mWP1dBS%2BGurDKxsiCRmUAenbMs%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\", \"azureml-logs/75_job_post-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/azureml-logs/75_job_post-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt?sv=2019-02-02&sr=b&sig=ofphAqs4RvXV3zMxNN4mY2VHy201aokIcP3tg8qS5Mo%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\", \"azureml-logs/process_info.json\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=hK613yFznsDpZemKKyxF1LI%2BdMHeKTLEmTVMdWzmAg4%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\", \"azureml-logs/process_status.json\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=XK6BOaiJSnPbZp0mZGlGkubmobHI03%2FhW7GxmrbtXjg%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\", \"logs/azureml/136_azureml.log\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/logs/azureml/136_azureml.log?sv=2019-02-02&sr=b&sig=E%2FI%2Bl257wHLKO8ga9df74bFmXKayQbOEf8uUJjWeFY8%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=DxlssYsdi3qDB14DwmZ8UERG1e%2FhMSZqKXveqKdqZuM%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.tune-lgbm-forecast_1584053683_fcc053f7/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=9rhyZ0ExIMBynRfEp9aVXV4sdeZWRgWwzjWjf3CjE8o%3D&st=2020-03-12T22%3A46%3A56Z&se=2020-03-13T06%3A56%3A56Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_64205f30874f615b57c2717938d4039e14020b51244803148cd50a8d15cabd4c_d.txt\"], [\"logs/azureml/136_azureml.log\"]], \"run_duration\": \"0:02:01\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"MAPE\", \"run_id\": \"tune-lgbm-forecast_1584053683_fcc053f7\", \"categories\": [0], \"series\": [{\"data\": [63.79837613183825]}]}], \"run_logs\": \"2020-03-12 22:55:38,817|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'snapshotProject': True, 'only_in_process_features': True, 'skip_track_logs_dir': True}, track_folders: None, deny_list: None, directories_to_watch: []\\n2020-03-12 22:55:38,817|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: batchai\\n2020-03-12 22:55:38,824|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Failed to import pyspark with error: No module named 'pyspark'\\n2020-03-12 22:55:38,825|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2020-03-12 22:55:39,032|azureml._base_sdk_common.user_agent|DEBUG|Fetching client info from /root/.azureml/clientinfo.json\\n2020-03-12 22:55:39,032|azureml._base_sdk_common.user_agent|DEBUG|Error loading client info: [Errno 2] No such file or directory: '/root/.azureml/clientinfo.json'\\n2020-03-12 22:55:39,327|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7f5c2c621488> for run source azureml.scriptrun\\n2020-03-12 22:55:39,328|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-03-12 22:55:39,334|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:39,334|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2020-03-12 22:55:39,334|azureml.core.authentication|DEBUG|Time to expire 1814347.665203 seconds\\n2020-03-12 22:55:39,335|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:39,335|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:39,335|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:39,335|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:39,335|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:39,366|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:39,366|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:39,367|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:39,372|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:39,381|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:39,387|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:39,393|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:39,400|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:39,400|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-03-12 22:55:39,400|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-03-12 22:55:39,401|msrest.http_logger|DEBUG|Request URL: 'https://westcentralus.experiments.azureml.net/history/v1.0/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/chhamlwsrg/providers/Microsoft.MachineLearningServices/workspaces/chhamlws/experiments/tune-lgbm-forecast/runs/tune-lgbm-forecast_1584053683_fcc053f7'\\n2020-03-12 22:55:39,401|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-03-12 22:55:39,401|msrest.http_logger|DEBUG|Request headers:\\n2020-03-12 22:55:39,401|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-03-12 22:55:39,401|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-03-12 22:55:39,401|msrest.http_logger|DEBUG|    'x-ms-client-request-id': 'c3e6bdbf-91c9-41f1-a51e-a8014346a808'\\n2020-03-12 22:55:39,401|msrest.http_logger|DEBUG|    'request-id': 'c3e6bdbf-91c9-41f1-a51e-a8014346a808'\\n2020-03-12 22:55:39,401|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-stretch-sid) msrest/0.6.11 azureml._restclient/core.1.0.85'\\n2020-03-12 22:55:39,402|msrest.http_logger|DEBUG|Request body:\\n2020-03-12 22:55:39,402|msrest.http_logger|DEBUG|None\\n2020-03-12 22:55:39,402|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-03-12 22:55:39,402|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-03-12 22:55:39,402|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-03-12 22:55:39,402|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-03-12 22:55:39,443|msrest.http_logger|DEBUG|Response status: 200\\n2020-03-12 22:55:39,444|msrest.http_logger|DEBUG|Response headers:\\n2020-03-12 22:55:39,444|msrest.http_logger|DEBUG|    'Date': 'Thu, 12 Mar 2020 22:55:39 GMT'\\n2020-03-12 22:55:39,444|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-03-12 22:55:39,444|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-03-12 22:55:39,444|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-03-12 22:55:39,444|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-03-12 22:55:39,444|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-03-12 22:55:39,444|msrest.http_logger|DEBUG|    'x-ms-client-request-id': 'c3e6bdbf-91c9-41f1-a51e-a8014346a808'\\n2020-03-12 22:55:39,445|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-03-12 22:55:39,445|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-03-12 22:55:39,445|msrest.http_logger|DEBUG|    'x-request-time': '0.022'\\n2020-03-12 22:55:39,445|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-03-12 22:55:39,445|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-03-12 22:55:39,445|msrest.http_logger|DEBUG|Response content:\\n2020-03-12 22:55:39,446|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 50,\\n  \\\"rootRunId\\\": \\\"tune-lgbm-forecast_1584053683_fcc053f7\\\",\\n  \\\"experimentId\\\": \\\"3199ea18-1505-42f9-9092-777f27df73e5\\\",\\n  \\\"createdUtc\\\": \\\"2020-03-12T22:54:47.1007739+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"8157bc92-2d12-4bc9-9270-bab51c673493\\\",\\n    \\\"userPuId\\\": \\\"10033FFF97A21586\\\",\\n    \\\"userIdp\\\": null,\\n    \\\"userAltSecId\\\": null,\\n    \\\"userIss\\\": \\\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\\",\\n    \\\"userTenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\",\\n    \\\"userName\\\": \\\"Chenhui Hu\\\"\\n  },\\n  \\\"userId\\\": \\\"8157bc92-2d12-4bc9-9270-bab51c673493\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 7,\\n  \\\"runUuid\\\": \\\"bf22bf87-9c3f-4ad6-8602-8efc6c5c42d1\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"bf22bf87-9c3f-4ad6-8602-8efc6c5c42d1\\\",\\n  \\\"runId\\\": \\\"tune-lgbm-forecast_1584053683_fcc053f7\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-03-12T22:55:20.2693483+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"heartbeatEnabled\\\": false,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.tune-lgbm-forecast_1584053683_fcc053f7\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"777da34e-21db-4ebf-b773-b899054ac4fa\\\",\\n    \\\"azureml.git.repository_uri\\\": \\\"git@github.com:microsoft/forecasting.git\\\",\\n    \\\"mlflow.source.git.repoURL\\\": \\\"git@github.com:microsoft/forecasting.git\\\",\\n    \\\"azureml.git.branch\\\": \\\"chenhui/hyperdrive_lightgbm\\\",\\n    \\\"mlflow.source.git.branch\\\": \\\"chenhui/hyperdrive_lightgbm\\\",\\n    \\\"azureml.git.commit\\\": \\\"ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d\\\",\\n    \\\"mlflow.source.git.commit\\\": \\\"ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d\\\",\\n    \\\"azureml.git.dirty\\\": \\\"True\\\",\\n    \\\"AzureML.DerivedImageName\\\": \\\"azureml/azureml_7842fd2c5e99a43f1cca1341b66a0ecb\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"train_validate.py\\\",\\n  \\\"target\\\": \\\"cpu-cluster\\\",\\n  \\\"uniqueChildRunComputeTargets\\\": [],\\n  \\\"tags\\\": {},\\n  \\\"inputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://westcentralus.experiments.azureml.net/execution/v1.0/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/chhamlwsrg/providers/Microsoft.MachineLearningServices/workspaces/chhamlws/experiments/tune-lgbm-forecast/runId/tune-lgbm-forecast_1584053683_fcc053f7/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://westcentralus.experiments.azureml.net/execution/v1.0/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/chhamlwsrg/providers/Microsoft.MachineLearningServices/workspaces/chhamlws/experiments/tune-lgbm-forecast/runId/tune-lgbm-forecast_1584053683_fcc053f7/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-03-12 22:55:39,455|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-03-12 22:55:39,455|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '777da34e-21db-4ebf-b773-b899054ac4fa', 'azureml.git.repository_uri': 'git@github.com:microsoft/forecasting.git', 'mlflow.source.git.repoURL': 'git@github.com:microsoft/forecasting.git', 'azureml.git.branch': 'chenhui/hyperdrive_lightgbm', 'mlflow.source.git.branch': 'chenhui/hyperdrive_lightgbm', 'azureml.git.commit': 'ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d', 'mlflow.source.git.commit': 'ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d', 'azureml.git.dirty': 'True', 'AzureML.DerivedImageName': 'azureml/azureml_7842fd2c5e99a43f1cca1341b66a0ecb', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-03-12 22:55:39,455|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-03-12 22:55:39,456|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2020-03-12 22:55:39,456|azureml.WorkerPool|DEBUG|[START]\\n2020-03-12 22:55:39,456|azureml.SendRunKillSignal|DEBUG|[START]\\n2020-03-12 22:55:39,456|azureml.RunStatusContext|DEBUG|[START]\\n2020-03-12 22:55:39,456|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunContextManager.RunStatusContext|DEBUG|[START]\\n2020-03-12 22:55:39,456|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2020-03-12 22:55:39,456|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2020-03-12 22:55:39,456|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/chhamlws/651fd5f483fe4f24b67d38ba050acf2e/tune-lgbm-forecast_1584053683_fcc053f7/mounts/workspaceblobstore/azureml/tune-lgbm-forecast_1584053683_fcc053f7\\n2020-03-12 22:55:39,456|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-03-12 22:55:39,456|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /mnt/batch/tasks/shared/LS_root/jobs/chhamlws/651fd5f483fe4f24b67d38ba050acf2e/tune-lgbm-forecast_1584053683_fcc053f7/mounts/workspaceblobstore/azureml/tune-lgbm-forecast_1584053683_fcc053f7\\n2020-03-12 22:55:40,118|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:40,118|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:40,118|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:40,118|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:40,119|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:40,119|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:40,119|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://westcentralus.experiments.azureml.net.\\n2020-03-12 22:55:40,124|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:40,124|azureml._run_impl.run_history_facade|DEBUG|Created a static thread pool for RunHistoryFacade class\\n2020-03-12 22:55:40,129|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:40,134|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:40,140|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:40,145|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:55:40,145|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-03-12 22:55:40,146|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|Request URL: 'https://westcentralus.experiments.azureml.net/history/v1.0/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/chhamlwsrg/providers/Microsoft.MachineLearningServices/workspaces/chhamlws/experiments/tune-lgbm-forecast/runs/tune-lgbm-forecast_1584053683_fcc053f7'\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|Request headers:\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '4c8c2fc5-bd0d-4397-9014-1fa6755c94c6'\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|    'request-id': '4c8c2fc5-bd0d-4397-9014-1fa6755c94c6'\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-stretch-sid) msrest/0.6.11 azureml._restclient/core.1.0.85'\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|Request body:\\n2020-03-12 22:55:40,146|msrest.http_logger|DEBUG|None\\n2020-03-12 22:55:40,146|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-03-12 22:55:40,146|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-03-12 22:55:40,147|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-03-12 22:55:40,147|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-03-12 22:55:40,193|msrest.http_logger|DEBUG|Response status: 200\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|Response headers:\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|    'Date': 'Thu, 12 Mar 2020 22:55:40 GMT'\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '4c8c2fc5-bd0d-4397-9014-1fa6755c94c6'\\n2020-03-12 22:55:40,194|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-03-12 22:55:40,195|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-03-12 22:55:40,195|msrest.http_logger|DEBUG|    'x-request-time': '0.027'\\n2020-03-12 22:55:40,195|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-03-12 22:55:40,195|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-03-12 22:55:40,195|msrest.http_logger|DEBUG|Response content:\\n2020-03-12 22:55:40,195|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 50,\\n  \\\"rootRunId\\\": \\\"tune-lgbm-forecast_1584053683_fcc053f7\\\",\\n  \\\"experimentId\\\": \\\"3199ea18-1505-42f9-9092-777f27df73e5\\\",\\n  \\\"createdUtc\\\": \\\"2020-03-12T22:54:47.1007739+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"8157bc92-2d12-4bc9-9270-bab51c673493\\\",\\n    \\\"userPuId\\\": \\\"10033FFF97A21586\\\",\\n    \\\"userIdp\\\": null,\\n    \\\"userAltSecId\\\": null,\\n    \\\"userIss\\\": \\\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\\",\\n    \\\"userTenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\",\\n    \\\"userName\\\": \\\"Chenhui Hu\\\"\\n  },\\n  \\\"userId\\\": \\\"8157bc92-2d12-4bc9-9270-bab51c673493\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 7,\\n  \\\"runUuid\\\": \\\"bf22bf87-9c3f-4ad6-8602-8efc6c5c42d1\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"bf22bf87-9c3f-4ad6-8602-8efc6c5c42d1\\\",\\n  \\\"runId\\\": \\\"tune-lgbm-forecast_1584053683_fcc053f7\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-03-12T22:55:20.2693483+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"heartbeatEnabled\\\": false,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.tune-lgbm-forecast_1584053683_fcc053f7\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"777da34e-21db-4ebf-b773-b899054ac4fa\\\",\\n    \\\"azureml.git.repository_uri\\\": \\\"git@github.com:microsoft/forecasting.git\\\",\\n    \\\"mlflow.source.git.repoURL\\\": \\\"git@github.com:microsoft/forecasting.git\\\",\\n    \\\"azureml.git.branch\\\": \\\"chenhui/hyperdrive_lightgbm\\\",\\n    \\\"mlflow.source.git.branch\\\": \\\"chenhui/hyperdrive_lightgbm\\\",\\n    \\\"azureml.git.commit\\\": \\\"ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d\\\",\\n    \\\"mlflow.source.git.commit\\\": \\\"ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d\\\",\\n    \\\"azureml.git.dirty\\\": \\\"True\\\",\\n    \\\"AzureML.DerivedImageName\\\": \\\"azureml/azureml_7842fd2c5e99a43f1cca1341b66a0ecb\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"train_validate.py\\\",\\n  \\\"target\\\": \\\"cpu-cluster\\\",\\n  \\\"uniqueChildRunComputeTargets\\\": [],\\n  \\\"tags\\\": {},\\n  \\\"inputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://westcentralus.experiments.azureml.net/execution/v1.0/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/chhamlwsrg/providers/Microsoft.MachineLearningServices/workspaces/chhamlws/experiments/tune-lgbm-forecast/runId/tune-lgbm-forecast_1584053683_fcc053f7/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://westcentralus.experiments.azureml.net/execution/v1.0/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/chhamlwsrg/providers/Microsoft.MachineLearningServices/workspaces/chhamlws/experiments/tune-lgbm-forecast/runId/tune-lgbm-forecast_1584053683_fcc053f7/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-03-12 22:55:40,198|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-03-12 22:55:40,199|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '777da34e-21db-4ebf-b773-b899054ac4fa', 'azureml.git.repository_uri': 'git@github.com:microsoft/forecasting.git', 'mlflow.source.git.repoURL': 'git@github.com:microsoft/forecasting.git', 'azureml.git.branch': 'chenhui/hyperdrive_lightgbm', 'mlflow.source.git.branch': 'chenhui/hyperdrive_lightgbm', 'azureml.git.commit': 'ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d', 'mlflow.source.git.commit': 'ca9cfa3e655d9cf2fb7e1ef0bbb36a5d3d728c8d', 'azureml.git.dirty': 'True', 'AzureML.DerivedImageName': 'azureml/azureml_7842fd2c5e99a43f1cca1341b66a0ecb', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-03-12 22:55:40,199|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-03-12 22:56:09,340|azureml.core.authentication|DEBUG|Time to expire 1814317.659152 seconds\\n2020-03-12 22:56:26,557|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-03-12 22:56:26,559|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-03-12 22:56:26,571|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-03-12 22:56:26,993|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-03-12 22:56:26,993|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/chhamlws/651fd5f483fe4f24b67d38ba050acf2e/tune-lgbm-forecast_1584053683_fcc053f7/mounts/workspaceblobstore/azureml/tune-lgbm-forecast_1584053683_fcc053f7\\n2020-03-12 22:56:26,993|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /mnt/batch/tasks/shared/LS_root/jobs/chhamlws/651fd5f483fe4f24b67d38ba050acf2e/tune-lgbm-forecast_1584053683_fcc053f7/mounts/workspaceblobstore/azureml/tune-lgbm-forecast_1584053683_fcc053f7 to /mnt/batch/tasks/shared/LS_root/jobs/chhamlws/651fd5f483fe4f24b67d38ba050acf2e/tune-lgbm-forecast_1584053683_fcc053f7/mounts/workspaceblobstore/azureml/tune-lgbm-forecast_1584053683_fcc053f7\\n2020-03-12 22:56:26,994|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /mnt/batch/tasks/shared/LS_root/jobs/chhamlws/651fd5f483fe4f24b67d38ba050acf2e/tune-lgbm-forecast_1584053683_fcc053f7/mounts/workspaceblobstore/azureml/tune-lgbm-forecast_1584053683_fcc053f7\\n2020-03-12 22:56:26,994|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2020-03-12 22:56:26,994|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2020-03-12 22:56:26,994|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7|INFO|complete is not setting status for submitted runs.\\n2020-03-12 22:56:26,994|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-03-12 22:56:26,994|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-03-12 22:56:26,994|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-03-12 22:56:26,994|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-03-12 22:56:26,994|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-03-12 22:56:26,994|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-03-12 22:56:26,995|azureml.RunStatusContext|DEBUG|[STOP]\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [].\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-03-12 22:56:26,995|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-03-12 22:56:26,995|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Start]\\n2020-03-12 22:56:26,996|azureml.BatchTaskQueueAdd_1_Batches.WorkerPool|DEBUG|submitting future: _handle_batch\\n2020-03-12 22:56:26,996|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|Batch size 1.\\n2020-03-12 22:56:26,996|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch|DEBUG|Using basic handler - no exception handling\\n2020-03-12 22:56:26,996|azureml._restclient.clientbase.WorkerPool|DEBUG|submitting future: _log_batch\\n2020-03-12 22:56:26,997|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|Adding task 0__handle_batch to queue of approximate size: 0\\n2020-03-12 22:56:26,997|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.post_batch-async:False|DEBUG|[START]\\n2020-03-12 22:56:26,997|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.0__log_batch|DEBUG|Using basic handler - no exception handling\\n2020-03-12 22:56:26,999|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|Adding task 0__log_batch to queue of approximate size: 0\\n2020-03-12 22:56:26,998|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-03-12 22:56:26,997|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Stop] - waiting default timeout\\n2020-03-12 22:56:26,999|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-03-12 22:56:26,999|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[START]\\n2020-03-12 22:56:26,999|msrest.http_logger|DEBUG|Request URL: 'https://westcentralus.experiments.azureml.net/history/v1.0/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/chhamlwsrg/providers/Microsoft.MachineLearningServices/workspaces/chhamlws/experiments/tune-lgbm-forecast/runs/tune-lgbm-forecast_1584053683_fcc053f7/batch/metrics'\\n2020-03-12 22:56:27,000|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Overriding default flush timeout from None to 120\\n2020-03-12 22:56:27,000|msrest.http_logger|DEBUG|Request method: 'POST'\\n2020-03-12 22:56:27,000|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0__handle_batch)].\\n2020-03-12 22:56:27,000|msrest.http_logger|DEBUG|Request headers:\\n2020-03-12 22:56:27,000|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[START]\\n2020-03-12 22:56:27,000|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-03-12 22:56:27,000|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|Awaiter is BatchTaskQueueAdd_1_Batches\\n2020-03-12 22:56:27,001|msrest.http_logger|DEBUG|    'Content-Type': 'application/json-patch+json; charset=utf-8'\\n2020-03-12 22:56:27,001|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[STOP]\\n2020-03-12 22:56:27,001|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '65468b98-205d-429d-9241-cdce1b5f815e'\\n2020-03-12 22:56:27,001|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|\\n2020-03-12 22:56:27,001|msrest.http_logger|DEBUG|    'request-id': '65468b98-205d-429d-9241-cdce1b5f815e'\\n2020-03-12 22:56:27,001|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[STOP]\\n2020-03-12 22:56:27,001|msrest.http_logger|DEBUG|    'Content-Length': '341'\\n2020-03-12 22:56:27,001|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-03-12 22:56:27,002|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-stretch-sid) msrest/0.6.11 azureml._restclient/core.1.0.85 sdk_run'\\n2020-03-12 22:56:27,002|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-03-12 22:56:27,002|msrest.http_logger|DEBUG|Request body:\\n2020-03-12 22:56:27,002|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [AsyncTask(0__log_batch)].\\n2020-03-12 22:56:27,002|msrest.http_logger|DEBUG|{\\\"values\\\": [{\\\"metricId\\\": \\\"a5af65b4-9c58-4fba-8dbd-4014eb18c848\\\", \\\"metricType\\\": \\\"azureml.v1.scalar\\\", \\\"createdUtc\\\": \\\"2020-03-12T22:56:26.557074Z\\\", \\\"name\\\": \\\"MAPE\\\", \\\"description\\\": \\\"\\\", \\\"numCells\\\": 1, \\\"cells\\\": [{\\\"MAPE\\\": 63.79837613183825}], \\\"schema\\\": {\\\"numProperties\\\": 1, \\\"properties\\\": [{\\\"propertyId\\\": \\\"MAPE\\\", \\\"name\\\": \\\"MAPE\\\", \\\"type\\\": \\\"float\\\"}]}}]}\\n2020-03-12 22:56:27,002|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-03-12 22:56:27,002|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-03-12 22:56:27,002|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-03-12 22:56:27,002|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-03-12 22:56:27,323|msrest.http_logger|DEBUG|Response status: 200\\n2020-03-12 22:56:27,323|msrest.http_logger|DEBUG|Response headers:\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'Date': 'Thu, 12 Mar 2020 22:56:27 GMT'\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'Content-Length': '0'\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '65468b98-205d-429d-9241-cdce1b5f815e'\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'x-request-time': '0.300'\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|Response content:\\n2020-03-12 22:56:27,324|msrest.http_logger|DEBUG|\\n2020-03-12 22:56:27,326|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.post_batch-async:False|DEBUG|[STOP]\\n2020-03-12 22:56:27,503|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.0__log_batch.WaitingTask|DEBUG|[START]\\n2020-03-12 22:56:27,503|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.0__log_batch.WaitingTask|DEBUG|Awaiter is PostMetricsBatch\\n2020-03-12 22:56:27,503|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.0__log_batch.WaitingTask|DEBUG|[STOP]\\n2020-03-12 22:56:27,504|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|Waiting on task: 0__log_batch.\\n1 tasks left. Current duration of flush 0.0002334117889404297 seconds.\\nWaiting on task: 0__log_batch.\\n1 tasks left. Current duration of flush 0.25067663192749023 seconds.\\n\\n2020-03-12 22:56:27,504|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-03-12 22:56:27,504|azureml._SubmittedRun#tune-lgbm-forecast_1584053683_fcc053f7.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-03-12 22:56:27,504|azureml.SendRunKillSignal|DEBUG|[STOP]\\n2020-03-12 22:56:27,504|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[START]\\n2020-03-12 22:56:27,504|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[STOP]\\n2020-03-12 22:56:27,505|azureml.WorkerPool|DEBUG|[STOP]\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": true, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.85\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run_remote).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the validation metric after the job finishes. The validation metric should be the same as the one we obtained when the script was ran locally. For more details of the job, you can execute `run_remote.get_details()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAPE': 63.79837613183825}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get metric value after the job finishes\n",
    "while run_remote.get_status() != \"Completed\":\n",
    "    {}\n",
    "run_remote.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tune-hyperparameters-using-hyperdrive'></a>\n",
    "## Tune Hyperparameters using HyperDrive\n",
    "\n",
    "Now we are ready to tune the hyperparameters of the LightGBM forecast model by launching multiple runs on the cluster. In the following cell, we define the configurations of a HyperDrive job that does a parallel searching of the hyperparameter space using a Bayesian sampling method. HyperDrive also supports random sampling of the parameter space.\n",
    "\n",
    "It is recommended that the maximum number of runs should be greater than or equal to 20 times the number of hyperparameters being tuned, for best results with Bayesian sampling. Specifically, it should be no less than 180 in the following case. Nevertheless, we find that even with very small amount of runs Bayesian search can achieve decent performance. Thus, the maximum number of child runs of HyperDrive `max_total_runs` is set as `20` to reduce the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For best results with Bayesian Sampling we recommend using a maximum number of runs greater than or equal to 20 times the number of hyperparameters being tuned. Current value for max_total_runs:20. Recommendend value:180.\n"
     ]
    }
   ],
   "source": [
    "# Increase this value if you want to achieve better performance\n",
    "max_total_runs = 20\n",
    "script_params = {\"--data-folder\": ds_data.as_mount()}\n",
    "est = Estimator(\n",
    "    source_directory=script_folder,\n",
    "    script_params=script_params,\n",
    "    compute_target=compute_target,\n",
    "    use_docker=True,\n",
    "    entry_script=train_script_name,\n",
    "    environment_definition=env,\n",
    ")\n",
    "\n",
    "# Specify hyperparameter space\n",
    "ps = BayesianParameterSampling(\n",
    "    {\n",
    "        \"--num-leaves\": quniform(8, 128, 1),\n",
    "        \"--min-data-in-leaf\": quniform(20, 500, 10),\n",
    "        \"--learning-rate\": choice(\n",
    "            1e-4, 1e-3, 5e-3, 1e-2, 1.5e-2, 2e-2, 3e-2, 5e-2, 1e-1\n",
    "        ),\n",
    "        \"--feature-fraction\": uniform(0.2, 1),\n",
    "        \"--bagging-fraction\": uniform(0.1, 1),\n",
    "        \"--bagging-freq\": quniform(1, 20, 1),\n",
    "        \"--max-rounds\": quniform(50, 2000, 10),\n",
    "        \"--max-lag\": quniform(3, 40, 1),\n",
    "        \"--window-size\": quniform(3, 40, 1),\n",
    "    }\n",
    ")\n",
    "\n",
    "# HyperDrive job configuration\n",
    "htc = HyperDriveConfig(\n",
    "    estimator=est,\n",
    "    hyperparameter_sampling=ps,\n",
    "    primary_metric_name=\"MAPE\",\n",
    "    primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
    "    max_total_runs=max_total_runs,\n",
    "    max_concurrent_runs=4,\n",
    ")\n",
    "\n",
    "htr = exp.submit(config=htc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job finishes, you should see outputs from the AzureML widgets similar to the following. Note that you can rerun  `RunDetails(htr).show()` after the job finishes to get the updated results on the dashboard in case it is not automatically refreshed.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/20047467/76152567-e31a5e00-608e-11ea-90a8-3fdfeafeeb92.png\" width=\"900\" height=\"500\">\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/20047467/76152586-270d6300-608f-11ea-8f83-07fa7a8528f2.png\" width=\"900\" height=\"400\">\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/20047467/76152600-46a48b80-608f-11ea-90d2-297aabd7376a.png\" width=\"900\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33199f45f554379b2158a3cf544265e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/tune-lgbm-forecast/runs/HD_9d84746f-6a79-4a7a-908e-d87b22964781?wsid=/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourcegroups/chhamlwsrg/workspaces/chhamlws\", \"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781\", \"run_properties\": {\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781\", \"created_utc\": \"2020-03-12T22:56:49.285878Z\", \"properties\": {\"primary_metric_config\": \"{\\\"name\\\": \\\"MAPE\\\", \\\"goal\\\": \\\"minimize\\\"}\", \"resume_from\": \"null\", \"runTemplate\": \"HyperDrive\", \"azureml.runsource\": \"hyperdrive\", \"platform\": \"AML\", \"ContentSnapshotId\": \"777da34e-21db-4ebf-b773-b899054ac4fa\"}, \"tags\": {\"max_concurrent_jobs\": \"4\", \"max_total_jobs\": \"20\", \"max_duration_minutes\": \"10080\", \"policy_config\": \"{\\\"name\\\": \\\"DEFAULT\\\"}\", \"generator_config\": \"{\\\"name\\\": \\\"BAYESIANOPTIMIZATION\\\", \\\"parameter_space\\\": {\\\"--num-leaves\\\": [\\\"quniform\\\", [8, 128, 1]], \\\"--min-data-in-leaf\\\": [\\\"quniform\\\", [20, 500, 10]], \\\"--learning-rate\\\": [\\\"choice\\\", [[0.0001, 0.001, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.1]]], \\\"--feature-fraction\\\": [\\\"uniform\\\", [0.2, 1]], \\\"--bagging-fraction\\\": [\\\"uniform\\\", [0.1, 1]], \\\"--bagging-freq\\\": [\\\"quniform\\\", [1, 20, 1]], \\\"--max-rounds\\\": [\\\"quniform\\\", [50, 2000, 10]], \\\"--max-lag\\\": [\\\"quniform\\\", [3, 40, 1]], \\\"--window-size\\\": [\\\"quniform\\\", [3, 40, 1]]}}\", \"primary_metric_config\": \"{\\\"name\\\": \\\"MAPE\\\", \\\"goal\\\": \\\"minimize\\\"}\", \"platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://westcentralus.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/chhamlwsrg/providers/Microsoft.MachineLearningServices/workspaces/chhamlws/experiments/tune-lgbm-forecast\\\", \\\"SubscriptionId\\\": \\\"9086b59a-02d7-4687-b3fd-e39fa5e0fd9b\\\", \\\"ResourceGroupName\\\": \\\"chhamlwsrg\\\", \\\"WorkspaceName\\\": \\\"chhamlws\\\", \\\"ExperimentName\\\": \\\"tune-lgbm-forecast\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"train_validate.py\\\", \\\"arguments\\\": [\\\"--data-folder\\\", \\\"$AZUREML_DATAREFERENCE_f9d2a744355a4ce3aadabbb8697ed14a\\\"], \\\"target\\\": \\\"cpu-cluster\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": null, \\\"nodeCount\\\": 1, \\\"environment\\\": {\\\"name\\\": null, \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"EXAMPLE_ENV_VAR\\\": \\\"EXAMPLE_VALUE\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"project_environment\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", {\\\"pip\\\": [\\\"azureml-defaults\\\"]}, \\\"pandas\\\", \\\"numpy\\\", \\\"scipy\\\", \\\"scikit-learn\\\", \\\"lightgbm\\\", \\\"joblib\\\"], \\\"channels\\\": [\\\"conda-forge\\\"]}}, \\\"docker\\\": {\\\"enabled\\\": true, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": true}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1}, \\\"dataReferences\\\": {\\\"f9d2a744355a4ce3aadabbb8697ed14a\\\": {\\\"dataStoreName\\\": \\\"workspaceblobstore\\\", \\\"pathOnDataStore\\\": \\\"data\\\", \\\"mode\\\": \\\"mount\\\", \\\"overwrite\\\": false, \\\"pathOnCompute\\\": null}}, \\\"data\\\": {}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": 1}}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"777da34e-21db-4ebf-b773-b899054ac4fa\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"azureml.train.hyperdrive._search\\\", \\\"amlClientFunction\\\": \\\"search\\\", \\\"tenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\", \\\"amlClientRequestId\\\": \\\"4fa6510d-377f-4bb6-a15a-961718d469f3\\\", \\\"amlClientSessionId\\\": \\\"7199b43e-d8cc-4b14-9650-12626afd04a8\\\", \\\"subscriptionId\\\": \\\"9086b59a-02d7-4687-b3fd-e39fa5e0fd9b\\\", \\\"estimator\\\": \\\"Estimator\\\", \\\"samplingMethod\\\": \\\"BayesianOptimization\\\", \\\"terminationPolicy\\\": \\\"Default\\\", \\\"primaryMetricGoal\\\": \\\"minimize\\\", \\\"maxTotalRuns\\\": 20, \\\"maxConcurrentRuns\\\": 4, \\\"maxDurationMinutes\\\": 10080, \\\"computeTarget\\\": \\\"AmlCompute\\\", \\\"vmSize\\\": null}}}\", \"resume_child_runs\": \"null\", \"all_jobs_generated\": \"false\", \"cancellation_requested\": \"false\", \"progress_metadata_evaluation_timestamp\": \"\\\"2020-03-12T22:56:49.935537\\\"\", \"progress_metadata_digest\": \"\\\"0b8662c12533ee207ee3a56ee0aa772620e9d3309b2cf9b8e196a91caf8d2f24\\\"\", \"progress_metadata_active_timestamp\": \"\\\"2020-03-12T22:56:49.935537\\\"\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_0\": \"{\\\"--num-leaves\\\": 11, \\\"--min-data-in-leaf\\\": 440, \\\"--learning-rate\\\": 0.1, \\\"--feature-fraction\\\": 0.7613259286486747, \\\"--bagging-fraction\\\": 0.2163411144672402, \\\"--bagging-freq\\\": 9, \\\"--max-rounds\\\": 1970, \\\"--max-lag\\\": 10, \\\"--window-size\\\": 27}\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_1\": \"{\\\"--num-leaves\\\": 123, \\\"--min-data-in-leaf\\\": 350, \\\"--learning-rate\\\": 0.015, \\\"--feature-fraction\\\": 0.5649646269862425, \\\"--bagging-fraction\\\": 0.18224963385548815, \\\"--bagging-freq\\\": 14, \\\"--max-rounds\\\": 60, \\\"--max-lag\\\": 12, \\\"--window-size\\\": 20}\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_2\": \"{\\\"--num-leaves\\\": 116, \\\"--min-data-in-leaf\\\": 340, \\\"--learning-rate\\\": 0.03, \\\"--feature-fraction\\\": 0.730454396811419, \\\"--bagging-fraction\\\": 0.9969744946290616, \\\"--bagging-freq\\\": 8, \\\"--max-rounds\\\": 1360, \\\"--max-lag\\\": 38, \\\"--window-size\\\": 23}\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_3\": \"{\\\"--num-leaves\\\": 102, \\\"--min-data-in-leaf\\\": 310, \\\"--learning-rate\\\": 0.1, \\\"--feature-fraction\\\": 0.542476560701534, \\\"--bagging-fraction\\\": 0.4853538349000507, \\\"--bagging-freq\\\": 3, \\\"--max-rounds\\\": 370, \\\"--max-lag\\\": 18, \\\"--window-size\\\": 34}\", \"environment_preparation_status\": \"PREPARED\", \"prepare_run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_preparation\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_4\": \"{\\\"--num-leaves\\\": 76, \\\"--min-data-in-leaf\\\": 90, \\\"--learning-rate\\\": 0.1, \\\"--feature-fraction\\\": 0.3391062441611188, \\\"--bagging-fraction\\\": 0.9645906042705741, \\\"--bagging-freq\\\": 2, \\\"--max-rounds\\\": 1780, \\\"--max-lag\\\": 26, \\\"--window-size\\\": 35}\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_5\": \"{\\\"--num-leaves\\\": 71, \\\"--min-data-in-leaf\\\": 90, \\\"--learning-rate\\\": 0.001, \\\"--feature-fraction\\\": 0.8830007838541849, \\\"--bagging-fraction\\\": 0.915206384551007, \\\"--bagging-freq\\\": 5, \\\"--max-rounds\\\": 1210, \\\"--max-lag\\\": 3, \\\"--window-size\\\": 32}\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_6\": \"{\\\"--num-leaves\\\": 99, \\\"--min-data-in-leaf\\\": 190, \\\"--learning-rate\\\": 0.05, \\\"--feature-fraction\\\": 0.7119016404691647, \\\"--bagging-fraction\\\": 0.2713540593312328, \\\"--bagging-freq\\\": 20, \\\"--max-rounds\\\": 430, \\\"--max-lag\\\": 18, \\\"--window-size\\\": 4}\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_7\": \"{\\\"--num-leaves\\\": 57, \\\"--min-data-in-leaf\\\": 130, \\\"--learning-rate\\\": 0.1, \\\"--feature-fraction\\\": 0.8245071880289601, \\\"--bagging-fraction\\\": 0.7382806691497505, \\\"--bagging-freq\\\": 8, \\\"--max-rounds\\\": 1580, \\\"--max-lag\\\": 26, \\\"--window-size\\\": 5}\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_8\": \"{\\\"--num-leaves\\\": 39, \\\"--min-data-in-leaf\\\": 140, \\\"--learning-rate\\\": 0.0001, \\\"--feature-fraction\\\": 0.7880481252624965, \\\"--bagging-fraction\\\": 0.7299904942486131, \\\"--bagging-freq\\\": 10, \\\"--max-rounds\\\": 100, \\\"--max-lag\\\": 12, \\\"--window-size\\\": 10}\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/hyperdrive.txt\": \"https://chhamlws4931040064.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_9d84746f-6a79-4a7a-908e-d87b22964781/azureml-logs/hyperdrive.txt?sv=2019-02-02&sr=b&sig=xS6L4MYvhJ4aXL%2BkkJlGfEy1e6hLtDuwcBk5VohOBqM%3D&st=2020-03-12T22%3A51%3A55Z&se=2020-03-13T07%3A01%3A55Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/hyperdrive.txt\"]], \"run_duration\": \"0:05:06\", \"hyper_parameters\": {\"--num-leaves\": [\"quniform\", [8, 128, 1]], \"--min-data-in-leaf\": [\"quniform\", [20, 500, 10]], \"--learning-rate\": [\"choice\", [[0.0001, 0.001, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.1]]], \"--feature-fraction\": [\"uniform\", [0.2, 1]], \"--bagging-fraction\": [\"uniform\", [0.1, 1]], \"--bagging-freq\": [\"quniform\", [1, 20, 1]], \"--max-rounds\": [\"quniform\", [50, 2000, 10]], \"--max-lag\": [\"quniform\", [3, 40, 1]], \"--window-size\": [\"quniform\", [3, 40, 1]]}}, \"child_runs\": [{\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_2\", \"run_number\": 53, \"metric\": 30.95474901, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-03-12T22:57:56.130558Z\", \"end_time\": \"2020-03-12T23:00:23.194336Z\", \"created_time\": \"2020-03-12T22:57:22.686361Z\", \"created_time_dt\": \"2020-03-12T22:57:22.686361Z\", \"duration\": \"0:03:00\", \"hyperdrive_id\": \"9d84746f-6a79-4a7a-908e-d87b22964781\", \"arguments\": null, \"param_--num-leaves\": 116, \"param_--min-data-in-leaf\": 340, \"param_--learning-rate\": 0.03, \"param_--feature-fraction\": 0.730454396811419, \"param_--bagging-fraction\": 0.9969744946290616, \"param_--bagging-freq\": 8, \"param_--max-rounds\": 1360, \"param_--max-lag\": 38, \"param_--window-size\": 23, \"best_metric\": 30.95474901}, {\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_0\", \"run_number\": 54, \"metric\": 36.1459398, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-03-12T22:57:56.519377Z\", \"end_time\": \"2020-03-12T22:59:32.418241Z\", \"created_time\": \"2020-03-12T22:57:23.269794Z\", \"created_time_dt\": \"2020-03-12T22:57:23.269794Z\", \"duration\": \"0:02:09\", \"hyperdrive_id\": \"9d84746f-6a79-4a7a-908e-d87b22964781\", \"arguments\": null, \"param_--num-leaves\": 11, \"param_--min-data-in-leaf\": 440, \"param_--learning-rate\": 0.1, \"param_--feature-fraction\": 0.7613259286486747, \"param_--bagging-fraction\": 0.2163411144672402, \"param_--bagging-freq\": 9, \"param_--max-rounds\": 1970, \"param_--max-lag\": 10, \"param_--window-size\": 27, \"best_metric\": 30.95474901}, {\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_3\", \"run_number\": 55, \"metric\": 32.05156232, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-03-12T22:57:56.431258Z\", \"end_time\": \"2020-03-12T22:59:13.495605Z\", \"created_time\": \"2020-03-12T22:57:23.454922Z\", \"created_time_dt\": \"2020-03-12T22:57:23.454922Z\", \"duration\": \"0:01:50\", \"hyperdrive_id\": \"9d84746f-6a79-4a7a-908e-d87b22964781\", \"arguments\": null, \"param_--num-leaves\": 102, \"param_--min-data-in-leaf\": 310, \"param_--learning-rate\": 0.1, \"param_--feature-fraction\": 0.542476560701534, \"param_--bagging-fraction\": 0.4853538349000507, \"param_--bagging-freq\": 3, \"param_--max-rounds\": 370, \"param_--max-lag\": 18, \"param_--window-size\": 34, \"best_metric\": 30.95474901}, {\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_1\", \"run_number\": 56, \"metric\": 51.20737047, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-03-12T22:57:57.569924Z\", \"end_time\": \"2020-03-12T22:58:55.615148Z\", \"created_time\": \"2020-03-12T22:57:23.498201Z\", \"created_time_dt\": \"2020-03-12T22:57:23.498201Z\", \"duration\": \"0:01:32\", \"hyperdrive_id\": \"9d84746f-6a79-4a7a-908e-d87b22964781\", \"arguments\": null, \"param_--num-leaves\": 123, \"param_--min-data-in-leaf\": 350, \"param_--learning-rate\": 0.015, \"param_--feature-fraction\": 0.5649646269862425, \"param_--bagging-fraction\": 0.18224963385548815, \"param_--bagging-freq\": 14, \"param_--max-rounds\": 60, \"param_--max-lag\": 12, \"param_--window-size\": 20, \"best_metric\": 30.95474901}, {\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_4\", \"run_number\": 57, \"metric\": null, \"status\": \"Running\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-03-12T23:00:15.63181Z\", \"end_time\": \"\", \"created_time\": \"2020-03-12T22:59:27.465289Z\", \"created_time_dt\": \"2020-03-12T22:59:27.465289Z\", \"duration\": \"0:02:28\", \"hyperdrive_id\": \"9d84746f-6a79-4a7a-908e-d87b22964781\", \"arguments\": null, \"param_--num-leaves\": 76, \"param_--min-data-in-leaf\": 90, \"param_--learning-rate\": 0.1, \"param_--feature-fraction\": 0.3391062441611188, \"param_--bagging-fraction\": 0.9645906042705741, \"param_--bagging-freq\": 2, \"param_--max-rounds\": 1780, \"param_--max-lag\": 26, \"param_--window-size\": 35, \"best_metric\": null}, {\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_5\", \"run_number\": 58, \"metric\": null, \"status\": \"Running\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-03-12T23:00:05.845411Z\", \"end_time\": \"\", \"created_time\": \"2020-03-12T22:59:27.73657Z\", \"created_time_dt\": \"2020-03-12T22:59:27.73657Z\", \"duration\": \"0:02:27\", \"hyperdrive_id\": \"9d84746f-6a79-4a7a-908e-d87b22964781\", \"arguments\": null, \"param_--num-leaves\": 71, \"param_--min-data-in-leaf\": 90, \"param_--learning-rate\": 0.001, \"param_--feature-fraction\": 0.8830007838541849, \"param_--bagging-fraction\": 0.915206384551007, \"param_--bagging-freq\": 5, \"param_--max-rounds\": 1210, \"param_--max-lag\": 3, \"param_--window-size\": 32, \"best_metric\": null}, {\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_6\", \"run_number\": 59, \"metric\": 32.53197978, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-03-12T23:00:31.584748Z\", \"end_time\": \"2020-03-12T23:01:49.181464Z\", \"created_time\": \"2020-03-12T22:59:59.558989Z\", \"created_time_dt\": \"2020-03-12T22:59:59.558989Z\", \"duration\": \"0:01:49\", \"hyperdrive_id\": \"9d84746f-6a79-4a7a-908e-d87b22964781\", \"arguments\": null, \"param_--num-leaves\": 99, \"param_--min-data-in-leaf\": 190, \"param_--learning-rate\": 0.05, \"param_--feature-fraction\": 0.7119016404691647, \"param_--bagging-fraction\": 0.2713540593312328, \"param_--bagging-freq\": 20, \"param_--max-rounds\": 430, \"param_--max-lag\": 18, \"param_--window-size\": 4, \"best_metric\": 30.95474901}, {\"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_7\", \"run_number\": 60, \"metric\": null, \"status\": \"Running\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-03-12T23:01:34.243772Z\", \"end_time\": \"\", \"created_time\": \"2020-03-12T23:01:00.738959Z\", \"created_time_dt\": \"2020-03-12T23:01:00.738959Z\", \"duration\": \"0:00:54\", \"hyperdrive_id\": \"9d84746f-6a79-4a7a-908e-d87b22964781\", \"arguments\": null, \"param_--num-leaves\": 57, \"param_--min-data-in-leaf\": 130, \"param_--learning-rate\": 0.1, \"param_--feature-fraction\": 0.8245071880289601, \"param_--bagging-fraction\": 0.7382806691497505, \"param_--bagging-freq\": 8, \"param_--max-rounds\": 1580, \"param_--max-lag\": 26, \"param_--window-size\": 5, \"best_metric\": null}], \"children_metrics\": {\"categories\": [0], \"series\": {\"MAPE\": [{\"categories\": [53, 54, 55, 56, 59], \"mode\": \"markers\", \"name\": \"MAPE\", \"stepped\": false, \"type\": \"scatter\", \"data\": [30.954749012270483, 36.14593979500759, 32.05156231757025, 51.207370474510164, 32.5319797816904]}, {\"categories\": [53, 54, 55, 56, 59], \"mode\": \"lines\", \"name\": \"MAPE_min\", \"stepped\": true, \"type\": \"scatter\", \"data\": [30.954749012270483, 30.954749012270483, 30.954749012270483, 30.954749012270483, 30.954749012270483]}]}, \"metricName\": null, \"primaryMetricName\": \"MAPE\", \"showLegend\": false}, \"run_metrics\": [{\"name\": \"best_child_by_primary_metric\", \"run_id\": \"HD_9d84746f-6a79-4a7a-908e-d87b22964781\", \"categories\": [0], \"series\": [{\"data\": [{\"metric_name\": [\"MAPE\", \"MAPE\", \"MAPE\"], \"timestamp\": [\"2020-03-12 22:59:21.698189+00:00\", \"2020-03-12 22:59:52.807815+00:00\", \"2020-03-12 23:00:23.828392+00:00\"], \"run_id\": [\"HD_9d84746f-6a79-4a7a-908e-d87b22964781_1\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_3\", \"HD_9d84746f-6a79-4a7a-908e-d87b22964781_2\"], \"metric_value\": [51.207370474510164, 32.05156231757025, 30.954749012270483], \"final\": [false, false, false]}]}]}], \"run_logs\": \"[2020-03-12T22:56:49.378394][API][INFO]Experiment created\\r\\n[2020-03-12T22:56:50.055977][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space\\r\\n[2020-03-12T22:56:50.311354][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.\\r\\n[2020-03-12T22:56:51.3910845Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.\\r\\n[2020-03-12T22:57:21.8271711Z][SCHEDULER][INFO]Scheduling job, id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_0'\\r\\n[2020-03-12T22:57:21.8257235Z][SCHEDULER][INFO]The execution environment was successfully prepared.\\r\\n[2020-03-12T22:57:21.8299416Z][SCHEDULER][INFO]Scheduling job, id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_2'\\r\\n[2020-03-12T22:57:21.8288269Z][SCHEDULER][INFO]Scheduling job, id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_1'\\r\\n[2020-03-12T22:57:21.8315183Z][SCHEDULER][INFO]Scheduling job, id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_3'\\r\\n[2020-03-12T22:57:22.8377968Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_2'\\r\\n[2020-03-12T22:57:23.4352248Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_0'\\r\\n[2020-03-12T22:57:23.5872202Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_3'\\r\\n[2020-03-12T22:57:23.7546755Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_1'\\r\\n[2020-03-12T22:59:21.650459][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space\\r\\n[2020-03-12T22:59:21.841049][GENERATOR][INFO]Successfully sampled '2' jobs, they will soon be submitted to the execution target.\\r\\n[2020-03-12T22:59:25.9588438Z][SCHEDULER][INFO]Scheduling job, id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_4'\\r\\n[2020-03-12T22:59:25.9601714Z][SCHEDULER][INFO]Scheduling job, id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_5'\\r\\n[2020-03-12T22:59:27.5336915Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_4'\\r\\n[2020-03-12T22:59:27.8788721Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_5'\\r\\n[2020-03-12T22:59:51.664942][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\\r\\n[2020-03-12T22:59:51.980392][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\\r\\n[2020-03-12T22:59:58.2685623Z][SCHEDULER][INFO]Scheduling job, id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_6'\\r\\n[2020-03-12T22:59:59.6273398Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_6'\\r\\n[2020-03-12T23:00:51.797480][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\\r\\n[2020-03-12T23:00:51.937489][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\\r\\n[2020-03-12T23:01:00.0746851Z][SCHEDULER][INFO]Scheduling job, id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_7'\\r\\n[2020-03-12T23:01:00.8710458Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_9d84746f-6a79-4a7a-908e-d87b22964781_7'\\r\\n[2020-03-12T23:01:51.813570][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\\r\\n[2020-03-12T23:01:52.450821][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": true, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.85\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(htr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HD_9d84746f-6a79-4a7a-908e-d87b22964781_0': {'MAPE': 36.14593979500759},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_1': {'MAPE': 51.207370474510164},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_10': {'MAPE': 32.446632612512},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_11': {'MAPE': 42.02577284190877},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_12': {'MAPE': 35.61336578834721},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_13': {'MAPE': 73.79327663085509},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_14': {'MAPE': 73.37497628117707},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_15': {'MAPE': 33.68597795822114},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_16': {'MAPE': 31.91582713727422},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_17': {'MAPE': 40.2927561100981},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_18': {'MAPE': 41.11747407801938},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_19': {'MAPE': 33.17396641669122},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_2': {'MAPE': 30.954749012270483},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_3': {'MAPE': 32.05156231757025},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_4': {'MAPE': 35.08488369223651},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_5': {'MAPE': 48.63277184440147},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_6': {'MAPE': 32.5319797816904},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_7': {'MAPE': 35.71181071112947},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_8': {'MAPE': 77.8306790737987},\n",
       " 'HD_9d84746f-6a79-4a7a-908e-d87b22964781_9': {'MAPE': 32.5653025881632}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while htr.get_status() != \"Completed\":\n",
    "    {}\n",
    "htr.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model and its hyperparameter values can be retrieved as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--data-folder', '$AZUREML_DATAREFERENCE_f9d2a744355a4ce3aadabbb8697ed14a', '--num-leaves', '116', '--min-data-in-leaf', '340', '--learning-rate', '0.03', '--feature-fraction', '0.730454396811419', '--bagging-fraction', '0.996974494629062', '--bagging-freq', '8', '--max-rounds', '1360', '--max-lag', '38', '--window-size', '23']\n"
     ]
    }
   ],
   "source": [
    "best_run = htr.get_best_run_by_primary_metric()\n",
    "parameter_values = best_run.get_details()[\"runDefinition\"][\"arguments\"]\n",
    "print(parameter_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then register the folder (and all files in it) as a model named `lgbm-oj-forecast` under the workspace for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(\n",
    "    model_name=\"lgbm-oj-forecast\", model_path=\"outputs/model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model in ACI\n",
    "\n",
    "Now we are ready to deploy the model as a web service running in Azure Container Instance [ACI](https://azure.microsoft.com/en-us/services/container-instances/). Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in.\n",
    "\n",
    "### Create score.py\n",
    "\n",
    "First, we will create a scoring script that will be invoked by the web service call.\n",
    "\n",
    "* Note that the scoring script must have two required functions, `init()` and `run(input_data)`.\n",
    "    - In `init()` function, you typically load the model into a global object. This function is executed only once when the Docker container is started.\n",
    "    - In `run(input_data)` function, the model is used to predict a value based on the input data. The input and output to run typically use JSON as serialization and de-serialization format but you are not limited to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def init():\n",
    "    global bst\n",
    "    model_root = os.getenv(\"AZUREML_MODEL_DIR\")\n",
    "    # The name of the folder in which to look for LightGBM model files\n",
    "    lgbm_model_folder = \"model\"\n",
    "    bst = lgb.Booster(\n",
    "        model_file=os.path.join(model_root, lgbm_model_folder, \"bst-model.txt\")\n",
    "    )\n",
    "\n",
    "\n",
    "def run(raw_data):\n",
    "    columns = bst.feature_name()\n",
    "    data = np.array(json.loads(raw_data)[\"data\"])\n",
    "    test_df = pd.DataFrame(data=data, columns=columns)\n",
    "    # Make prediction\n",
    "    out = bst.predict(test_df)\n",
    "    return out.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create myenv.yml\n",
    "\n",
    "We also need to create an environment file so that Azure Machine Learning can install the necessary packages in the Docker image which are required by your scoring script. In this case, we need to specify packages `numpy`, `pandas`, and `lightgbm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\r\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\r\n",
      "\n",
      "# Details about the Conda environment file format:\r\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\r\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\r\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\r\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "  - azureml-defaults\n",
      "- numpy=1.16.2\n",
      "- pandas=0.23.4\n",
      "- lightgbm=2.3.0\n",
      "channels:\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cd = CondaDependencies.create()\n",
    "cd.add_conda_package(\"numpy=1.16.2\")\n",
    "cd.add_conda_package(\"pandas=0.23.4\")\n",
    "cd.add_conda_package(\"lightgbm=2.3.0\")\n",
    "cd.save_to_file(base_directory=\"./\", conda_file_path=\"myenv.yml\")\n",
    "\n",
    "print(cd.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to ACI\n",
    "\n",
    "We are almost ready to deploy. In the next cell, we first create the inference configuration and deployment configuration. Then, we deploy the model to ACI. This cell will run for several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running.............................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n",
      "CPU times: user 568 ms, sys: 141 ms, total: 709 ms\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_config = InferenceConfig(runtime=\"python\", entry_script=\"score.py\", conda_file=\"myenv.yml\")\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=1,\n",
    "    tags={\"name\": \"ojdata\", \"framework\": \"LightGBM\"},\n",
    "    description=\"LightGBM model on Orange Juice data\",\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws, name=\"lgbm-oj-svc\", models=[model], inference_config=inference_config, deployment_config=aciconfig\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: If something goes wrong with the deployment, you could look at the logs from the service by running this command `print(service.get_logs())`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://1df089bb-be2f-47bb-8f62-fa30f476fe43.westus.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the web service is successfully deployed, you will see a deployment in the Azure Machine Learning workspace on Azure portal\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/20047467/76572336-2100f300-6490-11ea-9467-83cd693d23c1.png\" width=\"900\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model\n",
    "\n",
    "Let's test the deployed model. We create a few test data points and send them to the web service hosted in ACI. Note here we are using the run API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions each of which represents the forecasted sales of a target store, brand in a given week as specified by `store, brand, week` in `used_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [10865.700075504421, 20498.24318392933, 7172.911679596602]\n"
     ]
    }
   ],
   "source": [
    "# Prepare features according to the input schema of the best model\n",
    "train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "max_lag = int(parameter_values[parameter_values.index(\"--max-lag\") + 1])\n",
    "lags = np.arange(2, max_lag + 1)\n",
    "window_size = int(parameter_values[parameter_values.index(\"--window-size\") + 1])\n",
    "used_columns = [\n",
    "    \"store\",\n",
    "    \"brand\",\n",
    "    \"week\",\n",
    "    \"week_of_month\",\n",
    "    \"month\",\n",
    "    \"deal\",\n",
    "    \"feat\",\n",
    "    \"move\",\n",
    "    \"price\",\n",
    "    \"price_ratio\",\n",
    "]\n",
    "GAP = 2\n",
    "features, train_end_week = create_features(\n",
    "    1, train_dir, lags, window_size, used_columns\n",
    ")\n",
    "test_fea = features[features.week >= train_end_week + GAP].reset_index(drop=True)\n",
    "test_fea.drop(\"move\", axis=1, inplace=True)\n",
    "\n",
    "# Pick a few test data points\n",
    "test_samples = json.dumps({\"data\": np.array(test_fea.iloc[:3]).tolist()})\n",
    "test_samples = bytes(test_samples, encoding=\"utf8\")\n",
    "\n",
    "# Predict using the deployed model\n",
    "result = service.run(input_data=test_samples)\n",
    "print(\"prediction:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also send raw HTTP request to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST to url http://1df089bb-be2f-47bb-8f62-fa30f476fe43.westus.azurecontainer.io/score\n",
      "\n",
      "input data: b'{\"data\": [[2.0, 1.0, 137.0, 4.0, 4.0, 0.0, 0.0, 0.0416446872, 1.1124927835293534, 12416.0, 28096.0, 15168.0, 20736.0, 31808.0, 25728.0, 43584.0, 5056.0, 20224.0, 6720.0, 5504.0, 3520.0, 9792.0, 13120.0, 13120.0, 17728.0, 8320.0, 5120.0, 6080.0, 7168.0, 34240.0, 7296.0, 9216.0, 5312.0, 6272.0, 6784.0, 6784.0, 7744.0, 6016.0, 9024.0, 5632.0, 39424.0, 4160.0, 13568.0, 13568.0, 13568.0, 21952.0, 15206.95652173913], [2.0, 1.0, 138.0, 5.0, 4.0, 1.0, 1.0, 0.03734375, 0.9420125411290402, 12416.0, 12416.0, 28096.0, 15168.0, 20736.0, 31808.0, 25728.0, 43584.0, 5056.0, 20224.0, 6720.0, 5504.0, 3520.0, 9792.0, 13120.0, 13120.0, 17728.0, 8320.0, 5120.0, 6080.0, 7168.0, 34240.0, 7296.0, 9216.0, 5312.0, 6272.0, 6784.0, 6784.0, 7744.0, 6016.0, 9024.0, 5632.0, 39424.0, 4160.0, 13568.0, 13568.0, 13568.0, 15346.08695652174], [5.0, 1.0, 137.0, 4.0, 4.0, 0.0, 0.0, 0.04671875, 1.3116650049850451, 14144.0, 15808.0, 19008.0, 22784.0, 24448.0, 33600.0, 54272.0, 5376.0, 33600.0, 5632.0, 7424.0, 5888.0, 8640.0, 11840.0, 12160.0, 13440.0, 8192.0, 6656.0, 6912.0, 8960.0, 69504.0, 7552.0, 7872.0, 7168.0, 7232.0, 8128.0, 8128.0, 5440.0, 6400.0, 5632.0, 5632.0, 50752.0, 6016.0, 4608.0, 5056.0, 11904.0, 21120.0, 17552.695652173912]]}'\n",
      "\n",
      "prediction: [10865.700075504421, 20498.24318392933, 7172.911679596602]\n"
     ]
    }
   ],
   "source": [
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, test_samples, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "print(\"\")\n",
    "print(\"input data:\", test_samples)\n",
    "print(\"\")\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "After finishing the tests, you can delete the ACI deployment with a simple delete API call as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading:\n",
    "\n",
    "\\[1\\] Training, hyperparameter tune, and deploy with TensorFlow: https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/tensorflow/deployment/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb <br>\n",
    "\n",
    "\\[2\\] AzureML HyperDrive package: https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting_env",
   "language": "python",
   "name": "forecasting_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
