{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n",
    "This notebook gives an example of how to train a LightGBM model to generate point forecasts of product sales in retail. We will train a LightGBM based model on the Orange Juice dataset.\n",
    "\n",
    "[LightGBM](https://github.com/Microsoft/LightGBM) is a gradient boosting framework that uses tree-based learning algorithms. [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is an ensemble technique in which models are added to the ensemble sequentially and at each iteration a new model is trained with respect to the error of the whole ensemble learned so far. More detailed information about gradient boosting can be found in this [tutorial paper](https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021/full). Using this technique, LightGBM achieves great accuracy in many applications. Apart from this, it is designed to be distributed and efficient with the following advantages:\n",
    "* Fast training speed and high efficiency.\n",
    "* Low memory usage.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data.\n",
    "\n",
    "Due to these advantages, LightGBM has been widely-used in a lot of [winning solutions](https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions) of machine learning competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import scrapbook as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fclib.common.utils import git_repo_path\n",
    "from fclib.models.lightgbm import predict\n",
    "from fclib.evaluation.evaluation_utils import MAPE\n",
    "from fclib.common.plot import plot_predictions_with_history\n",
    "from fclib.dataset.ojdata import download_ojdata, split_train_test\n",
    "from fclib.dataset.ojdata import FIRST_WEEK_START\n",
    "from fclib.feature_engineering.feature_utils import (\n",
    "    week_of_month,\n",
    "    df_from_cartesian_product,\n",
    "    combine_features,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"LightGBM version: {}\".format(lgb.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "In what follows, we define global settings related to the model and feature engineering. LightGBM supports both classification models and regression models. In our case, we set the objective function to `mape` which stands for mean absolute percentage error (MAPE) since we will build a regression model to predict product sales and evaluate the accuracy of the model using MAPE.\n",
    "\n",
    "Generally, we can adjust the number of leaves (`num_leaves`), the minimum number of data in each leaf (`min_data_in_leaf`), maximum number of boosting rounds (`num_rounds`), the learning rate of trees (`learning_rate`) and `early_stopping_rounds` (to avoid overfitting) in the model to get better performance. Besides, we can also adjust other supported parameters to optimize the results. [In this link](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst), a list of all the parameters is given. In addition, advice on how to tune these parameters can be found [in this url](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst).\n",
    "\n",
    "We will use historical weekly sales, date time information, and product information as input features to train the model. Prior sales are used as lag features and `lags` contains the lags where each number indicates the number of time steps (i.e., weeks) that we shift the data backwards to get the historical sales. We also use the average sales within a certain time window in the past as a moving average feature. `window_size` controls the size of the moving window. Apart from these parameters, we use `use_columns` and `categ_fea` to denote all other features that we leverage in the model and the categorical features, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use False if you've already downloaded and split the data\n",
    "DOWNLOAD_SPLIT_DATA = True\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = os.path.join(git_repo_path(), \"ojdata\")\n",
    "\n",
    "# Forecasting settings\n",
    "N_SPLITS = 1\n",
    "HORIZON = 2\n",
    "GAP = 2\n",
    "FIRST_WEEK = 40\n",
    "LAST_WEEK = 138\n",
    "\n",
    "# Parameters of LightGBM model\n",
    "params = {\n",
    "    \"objective\": \"mape\",\n",
    "    \"num_leaves\": 124,\n",
    "    \"min_data_in_leaf\": 340,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"feature_fraction\": 0.65,\n",
    "    \"bagging_fraction\": 0.87,\n",
    "    \"bagging_freq\": 19,\n",
    "    \"num_rounds\": 940,\n",
    "    \"early_stopping_rounds\": 125,\n",
    "    \"num_threads\": 16,\n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "# Lags, window size, and feature column names\n",
    "lags = np.arange(2, 20)\n",
    "window_size = 40\n",
    "used_columns = [\"store\", \"brand\", \"week\", \"week_of_month\", \"month\", \"deal\", \"feat\", \"move\", \"price\", \"price_ratio\"]\n",
    "categ_fea = [\"store\", \"brand\", \"deal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We need to download the Orange Juice data and split it into training and test sets. By default, the following cell will download and spit the data. If you've already done so, you may skip this part by switching `DOWNLOAD_SPLIT_DATA` to `False`.\n",
    "\n",
    "We store the training data and test data using dataframes. The training data includes `train_df` and `aux_df` with `train_df` containing the historical sales up to week 135 (the time we make forecasts) and `aux_df` containing price/promotion information up until week 138. Here we assume that future price and promotion information up to a certain number of weeks ahead is predetermined and known. The test data is stored in `test_df` which contains the sales of each product in week 137 and 138. Assuming the current week is week 135, our goal is to forecast the sales in week 137 and 138 using the training data. There is a one-week gap between the current week and the first target week of forecasting as we want to leave time for planning inventory in practice.\n",
    "\n",
    "The setting of the forecast problem are defined in `fclib.dataset.ojdata.split_train_test` function. We can change this setting (e.g., modify the horizon of the forecast or the range of the historical data) by passing different parameters to this functions. Below, we split the data into `n_splits=1` splits, using the forecasting settings listed above in the **Parameters** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_SPLIT_DATA:\n",
    "    download_ojdata(DATA_DIR)\n",
    "    train_df_list, test_df_list, aux_df_list = split_train_test(\n",
    "        DATA_DIR,\n",
    "        n_splits=N_SPLITS,\n",
    "        horizon=HORIZON,\n",
    "        gap=GAP,\n",
    "        first_week=FIRST_WEEK,\n",
    "        last_week=LAST_WEEK,\n",
    "        write_csv=True,\n",
    "    )\n",
    "\n",
    "    # Split returns a list, extract the dataframes from the list\n",
    "    train_df = train_df_list[0].reset_index()\n",
    "    test_df = test_df_list[0].reset_index()\n",
    "    aux_df = aux_df_list[0].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Next we extract a number of features from the data for training the forecasting model including\n",
    "* datetime features including week, week of the month, and month\n",
    "* historical weekly sales of each orange juice in recent weeks\n",
    "* average sales of each orange juice during recent weeks\n",
    "* other features including `store`, `brand`, `deal`, `feat` columns and price features\n",
    "\n",
    "Note that the logarithm of the unit sales is stored in a column named `logmove` both for `train_df` and `test_df`. We compute the unit sales `move` based on this quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_df[\"move\"] = train_df[\"logmove\"].apply(lambda x: round(math.exp(x)))\n",
    "print(train_df.head(3))\n",
    "print(\"\")\n",
    "train_df = train_df[[\"store\", \"brand\", \"week\", \"move\"]]\n",
    "\n",
    "# Create a dataframe to hold all necessary data\n",
    "store_list = train_df[\"store\"].unique()\n",
    "brand_list = train_df[\"brand\"].unique()\n",
    "week_list = range(FIRST_WEEK, LAST_WEEK + 1)\n",
    "d = {\"store\": store_list, \"brand\": brand_list, \"week\": week_list}\n",
    "data_grid = df_from_cartesian_product(d)\n",
    "data_filled = pd.merge(data_grid, train_df, how=\"left\", on=[\"store\", \"brand\", \"week\"])\n",
    "\n",
    "# Get future price, deal, and advertisement info\n",
    "data_filled = pd.merge(data_filled, aux_df, how=\"left\", on=[\"store\", \"brand\", \"week\"])\n",
    "\n",
    "# Create relative price feature\n",
    "price_cols = [\n",
    "    \"price1\",\n",
    "    \"price2\",\n",
    "    \"price3\",\n",
    "    \"price4\",\n",
    "    \"price5\",\n",
    "    \"price6\",\n",
    "    \"price7\",\n",
    "    \"price8\",\n",
    "    \"price9\",\n",
    "    \"price10\",\n",
    "    \"price11\",\n",
    "]\n",
    "data_filled[\"price\"] = data_filled.apply(lambda x: x.loc[\"price\" + str(int(x.loc[\"brand\"]))], axis=1)\n",
    "data_filled[\"avg_price\"] = data_filled[price_cols].sum(axis=1).apply(lambda x: x / len(price_cols))\n",
    "data_filled[\"price_ratio\"] = data_filled[\"price\"] / data_filled[\"avg_price\"]\n",
    "data_filled.drop(price_cols, axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "print(\"Number of missing rows is {}\".format(data_filled[data_filled.isnull().any(axis=1)].shape[0]))\n",
    "print(\"\")\n",
    "data_filled = data_filled.groupby([\"store\", \"brand\"]).apply(lambda x: x.fillna(method=\"ffill\").fillna(method=\"bfill\"))\n",
    "\n",
    "# Create datetime features\n",
    "data_filled[\"week_start\"] = data_filled[\"week\"].apply(lambda x: FIRST_WEEK_START + datetime.timedelta(days=(x - 1) * 7))\n",
    "data_filled[\"year\"] = data_filled[\"week_start\"].apply(lambda x: x.year)\n",
    "data_filled[\"month\"] = data_filled[\"week_start\"].apply(lambda x: x.month)\n",
    "data_filled[\"week_of_month\"] = data_filled[\"week_start\"].apply(lambda x: week_of_month(x))\n",
    "data_filled[\"day\"] = data_filled[\"week_start\"].apply(lambda x: x.day)\n",
    "data_filled.drop(\"week_start\", axis=1, inplace=True)\n",
    "\n",
    "# Create other features (lagged features, moving averages, etc.)\n",
    "features = data_filled.groupby([\"store\", \"brand\"]).apply(\n",
    "    lambda x: combine_features(x, [\"move\"], lags, window_size, used_columns)\n",
    ")\n",
    "train_fea = features[features.week <= max(train_df.week)].reset_index(drop=True)\n",
    "print(\"Maximum training week number is {}\".format(max(train_fea[\"week\"])))\n",
    "print(\"\")\n",
    "\n",
    "# Drop rows with NaN values\n",
    "train_fea.dropna(inplace=True)\n",
    "print(train_fea.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We then train a LightGBM model to predict the sales. After the model is trained, we apply it to generate forecasts for the target weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set\n",
    "dtrain = lgb.Dataset(train_fea.drop(\"move\", axis=1, inplace=False), label=train_fea[\"move\"])\n",
    "\n",
    "# Train GBM model\n",
    "print(\"Training LightGBM model...\")\n",
    "bst = lgb.train(params, dtrain, valid_sets=[dtrain], categorical_feature=categ_fea, verbose_eval=50)\n",
    "print(\"\")\n",
    "\n",
    "# Generate forecasts\n",
    "test_fea = features[(features.week >= min(test_df.week)) & (features.week <= max(test_df.week))].reset_index(drop=True)\n",
    "idx_cols = [\"store\", \"brand\", \"week\"]\n",
    "pred = predict(test_fea, bst, target_col=\"move\", idx_cols=idx_cols).sort_values(by=idx_cols).reset_index(drop=True)\n",
    "print(\"Prediction results:\")\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "To evaluate the model performance, we compute MAPE of the forecasts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate prediction accuracy\n",
    "test_df[\"actual\"] = test_df[\"logmove\"].apply(lambda x: round(math.exp(x)))\n",
    "test_df.drop(\"logmove\", axis=1, inplace=True)\n",
    "combined = pd.merge(pred, test_df, on=[\"store\", \"brand\", \"week\"], how=\"left\")\n",
    "metric_value = MAPE(combined[\"move\"], combined[\"actual\"]) * 100\n",
    "sb.glue(\"MAPE\", metric_value)\n",
    "print(\"MAPE of the forecasts is {}\".format(metric_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization\n",
    "\n",
    "In the next, we plot out the feature importance learned by the model and the forecast results of a few sample store-brand combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "ax = lgb.plot_importance(bst, max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample forecast results\n",
    "num_samples = 6\n",
    "min_week = 120\n",
    "sales = pd.read_csv(os.path.join(DATA_DIR, \"yx.csv\"))\n",
    "sales[\"move\"] = sales[\"logmove\"].apply(lambda x: round(math.exp(x)) if x > 0 else 0)\n",
    "\n",
    "plot_predictions_with_history(\n",
    "    combined,\n",
    "    sales,\n",
    "    grain1_unique_vals=store_list,\n",
    "    grain2_unique_vals=brand_list,\n",
    "    time_col_name=\"week\",\n",
    "    target_col_name=\"move\",\n",
    "    grain1_name=\"store\",\n",
    "    grain2_name=\"brand\",\n",
    "    min_timestep=min_week,\n",
    "    num_samples=num_samples,\n",
    "    predict_at_timestep=max(train_df.week),\n",
    "    line_at_predict_time=True,\n",
    "    title=\"Prediction results for a few sample time series (predictions are made at week 135)\",\n",
    "    x_label=\"week\",\n",
    "    y_label=\"unit sales\",\n",
    "    random_seed=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "\n",
    "\\[1\\] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems. 3146–3154.<br>\n",
    "\\[2\\] Alexey Natekin and Alois Knoll. 2013. Gradient boosting machines, a tutorial. Frontiers in neurorobotics, 7 (21). <br>\n",
    "\\[3\\] The parameters of LightGBM: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst <br>\n",
    "\\[4\\] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. 2018. CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363 (2018).<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting_env",
   "language": "python",
   "name": "forecasting_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
