{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA: Autoregressive Integrated Moving Average\n",
    "\n",
    "This notebook provides an example of how to train an ARIMA model to generate point forecasts of product sales in retail. We will train an ARIMA based model on the Orange Juice dataset.\n",
    "\n",
    "An ARIMA, which stands for AutoRegressive Integrated Moving Average, model can be created using an `ARIMA(p,d,q)` model within `statsmodels` library. In this notebook, we will be using an alternative library `pmdarima`, which allows us to automatically search for optimal ARIMA parameters, within a specified range. More specifically, we will be using `auto_arima` function within `pmdarima` to automatically discover the optimal parameters for an ARIMA model. This function wraps `ARIMA` and `SARIMAX` models of `statsmodels` library, that correspond to non-seasonal and seasonal model space, respectively.\n",
    "\n",
    "In an ARIMA model there are 3 parameters that are used to help model the major aspects of a times series: seasonality, trend, and noise. These parameters are:\n",
    "- **p** is the parameter associated with the auto-regressive aspect of the model, which incorporates past values.\n",
    "- **d** is the parameter associated with the integrated part of the model, which effects the amount of differencing to apply to a time series.\n",
    "- **q** is the parameter associated with the moving average part of the model.,\n",
    "\n",
    "If our data has a seasonal component, we use a seasonal ARIMA model or `ARIMA(p,d,q)(P,D,Q)m`. In that case, we have an additional set of parameters: `P`, `D`, and `Q` which describe the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model, and `m` refers to the number of periods in each season.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scrapbook as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "from fclib.common.utils import git_repo_path\n",
    "from fclib.common.plot import plot_predictions_with_history\n",
    "from fclib.evaluation.evaluation_utils import MAPE\n",
    "from fclib.dataset.ojdata import download_ojdata, split_train_test, complete_and_fill_df\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "np.set_printoptions(precision=2)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Next, we define global settings related to the model. We will use historical weekly sales data only, without any covariate features to train the ARIMA model. The model parameter ranges are provided in params. These are later used by the `auto_arima()` function to search the space for the optimal set of parameters. To increase the space of models to search over, increase the `max_p` and `max_q` parameters.\n",
    "\n",
    "> NOTE: Our data does not show a strong seasonal component (as demonstrated in data exploration example notebook), so we will not be searching over the seasonal ARIMA models. To search over the seasonal models, set `seasonal` to `True` and include `start_P`, `start_Q`, `max_P`, and `max_Q` parameters in the auto_arima() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use False if you've already downloaded and split the data\n",
    "DOWNLOAD_SPLIT_DATA = True\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = os.path.join(git_repo_path(), \"ojdata\")\n",
    "\n",
    "# Forecasting settings\n",
    "N_SPLITS = 1\n",
    "HORIZON = 2\n",
    "GAP = 2\n",
    "FIRST_WEEK = 40\n",
    "LAST_WEEK = 138\n",
    "\n",
    "# Parameters of ARIMA model\n",
    "params = {\n",
    "    \"seasonal\": False,\n",
    "    \"start_p\": 0,\n",
    "    \"start_q\": 0,\n",
    "    \"max_p\": 5,\n",
    "    \"max_q\": 5,\n",
    "    \"m\": 52,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We need to download the Orange Juice data and split it into training and test sets. By default, the following cell will download and spit the data. If you've already done so, you may skip this part by switching `DOWNLOAD_SPLIT_DATA` to `False`.\n",
    "\n",
    "We store the training data and test data using dataframes. The training data includes `train_df` and `aux_df` with `train_df` containing the historical sales up to week 135 (the time we make forecasts) and `aux_df` containing price/promotion information up until week 138. Here we assume that future price and promotion information up to a certain number of weeks ahead is predetermined and known. In our example, we will be using historical sales only, and will not be using the `aux_df` data. The test data is stored in `test_df` which contains the sales of each product in week 137 and 138. Assuming the current week is week 135, our goal is to forecast the sales in week 137 and 138 using the training data. There is a one-week gap between the current week and the first target week of forecasting as we want to leave time for planning inventory in practice.\n",
    "\n",
    "The setting of the forecast problem are defined in `fclib.dataset.ojdata.split_train_test` function. We can change this setting (e.g., modify the horizon of the forecast or the range of the historical data) by passing different parameters to this functions. Below, we split the data into `n_splits=1` splits, using the forecasting settings listed above in the **Parameters** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if DOWNLOAD_SPLIT_DATA:\n",
    "    download_ojdata(DATA_DIR)\n",
    "    train_df_list, test_df_list, _ = split_train_test(\n",
    "        DATA_DIR,\n",
    "        n_splits=N_SPLITS,\n",
    "        horizon=HORIZON,\n",
    "        gap=GAP,\n",
    "        first_week=FIRST_WEEK,\n",
    "        last_week=LAST_WEEK,\n",
    "        write_csv=True,\n",
    "    )\n",
    "\n",
    "    # Split returns a list, extract the dataframes from the list\n",
    "    train_df = train_df_list[0].reset_index()\n",
    "    test_df = test_df_list[0].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data preparation for the training and test set include the following steps:\n",
    "\n",
    "- The unit sales of orange juice are give in logarithmic scale. We will transfrom them back into the unit scale by applying `math.exp()`\n",
    "- Our time series data is not complete, since we have missing sales for some stores/products and weeks. We will fill in those missing values by propagating the last valid observation forward to next available value.\n",
    "\n",
    "Note that our time series are grouped by `store` and `brand`, while `week` represents a time step, and `move` represents the value to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first process the training data. Note that the training data runs from `FIRST_WEEK` to `LAST_WEEK - HORIZON - GAP + 1` as defined in Parameters section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Select only required columns\n",
    "train_df = train_df[[\"store\", \"brand\", \"week\", \"logmove\"]]\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the unit sales of the products are given in logarithmic scale. We will use this quantity for training the forecasting model, as it smooths out the time series, and results in better forecasting performance. We will convert the `logmove` to a unit scale for evaluation, for consistency across our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create a dataframe to hold all necessary data\n",
    "store_list = train_df[\"store\"].unique()\n",
    "brand_list = train_df[\"brand\"].unique()\n",
    "train_week_list = range(FIRST_WEEK, LAST_WEEK - (HORIZON - 1) - (GAP - 1))\n",
    "\n",
    "train_filled = complete_and_fill_df(train_df, stores=store_list, brands=brand_list, weeks=train_week_list)\n",
    "\n",
    "train_filled.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now process the test data. Note that the test data runs from `LAST_WEEK - HORIZON + 1` to `LAST_WEEK`. Note that we are converting unit sales below from logarithmic scale to the counts, as we will be using counts to calculate the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate prediction accuracy\n",
    "test_df[\"actuals\"] = test_df.logmove.apply(lambda x: round(math.exp(x)))\n",
    "test_df = test_df[[\"store\", \"brand\", \"week\", \"actuals\"]]\n",
    "\n",
    "test_week_list = range(LAST_WEEK - HORIZON + 1, LAST_WEEK + 1)\n",
    "test_filled = complete_and_fill_df(test_df, stores=store_list, brands=brand_list, weeks=test_week_list)\n",
    "\n",
    "test_filled.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We next train an ARIMA model for a single time series, for demonstration. We select `STORE=2` and `BRAND=6` and filter our data based on these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE = 2\n",
    "BRAND = 6\n",
    "\n",
    "train_ts = train_filled.loc[(train_filled.store == STORE) & (train_filled.brand == BRAND)]\n",
    "train_ts.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts = np.array(train_ts.logmove)\n",
    "\n",
    "model = auto_arima(\n",
    "    train_ts,\n",
    "    seasonal=params[\"seasonal\"],\n",
    "    start_p=params[\"start_p\"],\n",
    "    start_q=params[\"start_q\"],\n",
    "    max_p=params[\"max_p\"],\n",
    "    max_q=params[\"max_q\"],\n",
    "    stepwise=True,\n",
    ")\n",
    "\n",
    "model.fit(train_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the model summary. As seen from the summary, the selected ARIMA model is `(p=1, d=0, q=0)`. This is a relatively simple model, also referred to as first-order auto-regressive model. It indicates that the time series is stationary and can be predicted as a multiple of its own previous value, plus a constant.  This is an `ARIMA(1,0,0)+constant` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary contains a lot of information. The coefficient table in the middle provides the estimates for the weights of the respective p and q terms. Notice that the coefficient of the AR1 term has a low p-value (`P>|z|` column), indicating that this term is significant. It also shows that the constant term is significant with a low p-value.\n",
    "\n",
    "Next, let's also examine the diagnostics plot for the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_diagnostics(figsize=(10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the top left, the residual errors fluctuate around a mean of zero and have a uniform variance, which may indicate that there is no bias in prediction. The density plot in the top right suggests normal distribution with mean zero.\n",
    "\n",
    "The Correlogram, or the ACF plot, in the lower right shows the residual errors are not autocorrelated. Any detected autocorrelation in this plot suggests that there may be some pattern in the residual errors which are not explained in the model, so adding additional predictors to the model may be beneficial.\n",
    "\n",
    "In the bottom left, we do not see significant deviation of residuals from the red line, which indicates that the model is a good fit.\n",
    "\n",
    "Overall, based on the above, it seems to that the model is a good fit for this data.\n",
    "\n",
    "\n",
    "It is worth noting that selecting the best parameters for an ARIMA model can be challenging - somewhat subjective and time intesive, and should be done following a thorough data examination (seasonality, trend, bias). We use an `auto_arima()` function to search a provided space of parameters for the best model, mostly to demonstrate its usage and functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the predictions. Since auto_arima model makes consecutive forecasts from the last time point, we want to forecast the next `n_periods = GAP + HORIZON - 1` points, so that we can account for the GAP, as described in the data setup. As mentioned above, we are also transforming our predictions from logarithmic scale to counts, for calculating evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(n_periods=GAP + HORIZON - 1)\n",
    "\n",
    "predictions = np.round(np.exp(preds[-HORIZON:]))\n",
    "pred_df = pd.DataFrame({\"predictions\": predictions, \"store\": STORE, \"brand\": BRAND, \"week\": test_week_list})\n",
    "\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will use *mean absolute percentage error* or **MAPE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine actual units and predictions\n",
    "test_ts = test_filled.loc[(test_filled.store == STORE) & (test_filled.brand == BRAND)]\n",
    "\n",
    "combined = pd.merge(pred_df, test_ts, on=[\"store\", \"brand\", \"week\"], how=\"left\")\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = MAPE(combined.predictions, combined.actuals) * 100\n",
    "\n",
    "print(f\"MAPE of the forecasts is {metric_value} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training for all stores and brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run model training across all the stores and brands. We will re-run the same code to automatically search for the best parameters, simply wrapped in a for loop iterating over stores and brands.\n",
    "\n",
    "> NOTE: Since we are building a model for each time series sequentially, it will take ~11 minutes to iterate over 900+ time series for each store and brand. To execute the next cell faster, you can subset a list of stores in the below cell by uncommenting `store_list = store_list[0:20]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# store_list = store_list[0:10]\n",
    "result_df = pd.DataFrame(None, columns=[\"predictions\", \"store\", \"brand\", \"week\", \"actuals\"])\n",
    "\n",
    "print(\"Training ARIMA model...\")\n",
    "for store, brand in itertools.product(store_list, brand_list):\n",
    "\n",
    "    if brand == 1:\n",
    "        print(f\"Forecasting for store: {store}\")\n",
    "\n",
    "    train_ts = train_filled.loc[(train_filled.store == store) & (train_filled.brand == brand)]\n",
    "    train_ts = np.array(train_ts[\"logmove\"])\n",
    "\n",
    "    model = auto_arima(\n",
    "        train_ts,\n",
    "        seasonal=params[\"seasonal\"],\n",
    "        start_p=params[\"start_p\"],\n",
    "        start_q=params[\"start_q\"],\n",
    "        max_p=params[\"max_p\"],\n",
    "        max_q=params[\"max_q\"],\n",
    "        stepwise=True,\n",
    "    )\n",
    "\n",
    "    model.fit(train_ts)\n",
    "    preds = model.predict(n_periods=GAP + HORIZON - 1)\n",
    "    predictions = np.round(np.exp(preds[-HORIZON:]))\n",
    "\n",
    "    pred_df = pd.DataFrame({\"predictions\": predictions, \"store\": store, \"brand\": brand, \"week\": test_week_list})\n",
    "    test_ts = test_filled.loc[(test_filled.store == store) & (test_filled.brand == brand)]\n",
    "\n",
    "    combined_df = pd.merge(pred_df, test_ts, on=[\"store\", \"brand\", \"week\"], how=\"left\")\n",
    "\n",
    "    result_df = result_df.append(combined_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute `MAPE` for all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_value = MAPE(result_df.predictions, result_df.actuals) * 100\n",
    "sb.glue(\"MAPE\", metric_value)\n",
    "\n",
    "print(f\"MAPE of the forecasts is {metric_value} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a model with `auto_arima` for a large number of time series, it is often difficult to examine each model individually (in a similar way we did for the single time series above). As `auto_arima` searches a restricted space of the models, defined by the range of `p` and `q` parameters, we often might not find an optimal model for each time series.\n",
    "\n",
    "Let's plot a few examples of forecasted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 6\n",
    "min_week = 120\n",
    "sales = pd.read_csv(os.path.join(DATA_DIR, \"yx.csv\"))\n",
    "sales[\"move\"] = sales.logmove.apply(lambda x: round(math.exp(x)) if x > 0 else 0)\n",
    "\n",
    "result_df[\"move\"] = result_df.predictions\n",
    "plot_predictions_with_history(\n",
    "    result_df,\n",
    "    sales,\n",
    "    grain1_unique_vals=store_list,\n",
    "    grain2_unique_vals=brand_list,\n",
    "    time_col_name=\"week\",\n",
    "    target_col_name=\"move\",\n",
    "    grain1_name=\"store\",\n",
    "    grain2_name=\"brand\",\n",
    "    min_timestep=min_week,\n",
    "    num_samples=num_samples,\n",
    "    predict_at_timestep=max(train_df.week),\n",
    "    line_at_predict_time=True,\n",
    "    title=\"Prediction results for a few sample time series (predictions are made at week 135)\",\n",
    "    x_label=\"week\",\n",
    "    y_label=\"unit sales\",\n",
    "    random_seed=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting_env",
   "language": "python",
   "name": "forecasting_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
