{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n",
    "This notebook gives an example of how to train a LightGBM model to generate point forecasts of product sales in retail. We will train a LightGBM based model on the OrangeJuice dataset.\n",
    "\n",
    "[LightGBM](https://github.com/Microsoft/LightGBM) is a gradient boosting framework that uses tree-based learning algorithms. [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is an ensemble technique in which models are added to the ensemble sequentially and at each iteration a new model is trained with respect to the error of the whole ensemble learned so far. More detailed information about gradient boosting can be found in this [tutorial paper](https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021/full). Using this technique, LightGBM achieves great accuracy in many applications. Apart from this, it is designed to be distributed and efficient with the following advantages:\n",
    "* Fast training speed and high efficiency.\n",
    "* Low memory usage.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data.\n",
    "\n",
    "Due to these advantages, LightGBM has been widely-used in a lot of [winning solutions](https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions) of machine learning competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29) \n",
      "[GCC 7.3.0]\n",
      "LightGBM version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from forecasting_lib.evaluation.evaluation_utils import MAPE\n",
    "import forecasting_lib.dataset.retail.benchmark_settings as bs\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"LightGBM version: {}\".format(lgb.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Setting\n",
    "\n",
    "In what follows, we define global settings related to the model and feature engineering. LightGBM supports both classification models and regression models. In our case, we set the objective function to `mape` which stands for mean absolute percentage error (MAPE) since we will build a regression model to predict product sales and evaluate the accuracy of the model using MAPE.\n",
    "\n",
    "Generally, we can adjust the number of leaves (`num_leaves`), the minimum number of data in each leaf (`min_data_in_leaf`), maximum number of boosting rounds (`num_rounds`), the learning rate of trees (`learning_rate`) and `early_stopping_rounds` (to avoid overfitting) in the model to get better performance. Besides, we can also adjust other supported parameters to optimize the results. [In this link](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst), a list of all the parameters is given. In addition, advice on how to tune these parameters can be found [in this url](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst).\n",
    "\n",
    "We will use historical weekly sales, date time information, and product information as input features to train the model. Prior sales are used as lag features and `lags` contains the lags where each number indicates the number of time steps (i.e., weeks) that we shift the data backwards to get the historical sales. We also use the average sales within a certain time window in the past as a moving average feature. `window_size` controls the size of the moving window. Apart from these parameters, we use `use_columns` and `categ_fea` to denote all other features that we leverage in the model and the categorical features, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directories\n",
    "DATA_DIR = \"../../contrib/tsperf/OrangeJuice_Pt_3Weeks_Weekly/data\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "# Parameters of LightGBM model\n",
    "params = {\n",
    "    \"objective\": \"mape\",\n",
    "    \"num_leaves\": 124,\n",
    "    \"min_data_in_leaf\": 340,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"feature_fraction\": 0.65,\n",
    "    \"bagging_fraction\": 0.87,\n",
    "    \"bagging_freq\": 19,\n",
    "    \"num_rounds\": 940,\n",
    "    \"early_stopping_rounds\": 125,\n",
    "    \"num_threads\": 16,\n",
    "    \"verbose_eval\": 20,\n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "# Lags and categorical features\n",
    "lags = np.arange(2, 20)\n",
    "window_size = 40\n",
    "used_columns = [\"store\", \"brand\", \"week\", \"week_of_month\", \"month\", \"deal\", \"feat\", \"move\", \"price\", \"price_ratio\"]\n",
    "categ_fea = [\"store\", \"brand\", \"deal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from forecasting_lib.feature_engineering.feature_utils import week_of_month, df_from_cartesian_product, combine_features\n",
    "from forecasting_lib.models.lightgbm import make_predictions\n",
    "\n",
    "\n",
    "def plot_result(results, store, brand):\n",
    "    \"\"\"Plot out prediction results and actual sales.\n",
    "    \n",
    "    Args:\n",
    "        result (Dataframe): Input dataframe including predicted sales and actual sales\n",
    "        store (integer): store index\n",
    "        brand (integer): brand index\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    subset = results[(results.store == store) & (results.brand == brand)]\n",
    "    subset = subset[[\"week\", \"move\", \"actual\"]].set_index(\"week\")\n",
    "    ax = subset.plot()\n",
    "    ax.set_title(\"Forecast result of brand {} at store {}\".format(brand, store))\n",
    "    ax.set_ylabel(\"unit sales\")\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.legend(labels=[\"predicted\", \"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that future price and promotion information up to a certain number of weeks ahead is predetermined and known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store  brand  week   logmove  ...  deal  feat     profit  move\n",
      "0      2      1    40  9.018695  ...     1   0.0  37.992326  8256\n",
      "1      2      1    46  8.723231  ...     0   0.0  30.126667  6144\n",
      "2      2      1    47  8.253228  ...     0   0.0  30.000000  3840\n",
      "\n",
      "[3 rows x 20 columns]\n",
      "\n",
      "Number of missing rows is 6204\n",
      "\n",
      "Maximum training week number is 135\n",
      "\n",
      "    store  brand  week  ...  move_lag18  move_lag19    move_mean\n",
      "19      2      1    59  ...      8256.0      8256.0  7847.111111\n",
      "\n",
      "[1 rows x 29 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and predict for all forecast rounds\n",
    "\n",
    "r = 0\n",
    "# Load training data\n",
    "train_df = pd.read_csv(os.path.join(TRAIN_DIR, \"train_round_\" + str(r + 1) + \".csv\"))\n",
    "train_df[\"move\"] = train_df[\"logmove\"].apply(lambda x: round(math.exp(x)))\n",
    "print(train_df.head(3))\n",
    "print(\"\")\n",
    "train_df = train_df[[\"store\", \"brand\", \"week\", \"move\"]]\n",
    "\n",
    "# Create a dataframe to hold all necessary data\n",
    "store_list = train_df[\"store\"].unique()\n",
    "brand_list = train_df[\"brand\"].unique()\n",
    "week_list = range(bs.TRAIN_START_WEEK, bs.TEST_END_WEEK_LIST[r] + 1)\n",
    "d = {\"store\": store_list, \"brand\": brand_list, \"week\": week_list}\n",
    "data_grid = df_from_cartesian_product(d)\n",
    "data_filled = pd.merge(data_grid, train_df, how=\"left\", on=[\"store\", \"brand\", \"week\"])\n",
    "\n",
    "# Get future price, deal, and advertisement info\n",
    "aux_df = pd.read_csv(os.path.join(TRAIN_DIR, \"aux_round_\" + str(r + 1) + \".csv\"))\n",
    "data_filled = pd.merge(data_filled, aux_df, how=\"left\", on=[\"store\", \"brand\", \"week\"])\n",
    "\n",
    "# Create relative price feature\n",
    "price_cols = [\n",
    "    \"price1\",\n",
    "    \"price2\",\n",
    "    \"price3\",\n",
    "    \"price4\",\n",
    "    \"price5\",\n",
    "    \"price6\",\n",
    "    \"price7\",\n",
    "    \"price8\",\n",
    "    \"price9\",\n",
    "    \"price10\",\n",
    "    \"price11\",\n",
    "]\n",
    "data_filled[\"price\"] = data_filled.apply(lambda x: x.loc[\"price\" + str(int(x.loc[\"brand\"]))], axis=1)\n",
    "data_filled[\"avg_price\"] = data_filled[price_cols].sum(axis=1).apply(lambda x: x / len(price_cols))\n",
    "data_filled[\"price_ratio\"] = data_filled[\"price\"] / data_filled[\"avg_price\"]\n",
    "data_filled.drop(price_cols, axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "print(\"Number of missing rows is {}\".format(data_filled[data_filled.isnull().any(axis=1)].shape[0]))\n",
    "print(\"\")\n",
    "data_filled = data_filled.groupby([\"store\", \"brand\"]).apply(lambda x: x.fillna(method=\"ffill\").fillna(method=\"bfill\"))\n",
    "\n",
    "# Create datetime features\n",
    "data_filled[\"week_start\"] = data_filled[\"week\"].apply(\n",
    "    lambda x: bs.FIRST_WEEK_START + datetime.timedelta(days=(x - 1) * 7)\n",
    ")\n",
    "data_filled[\"year\"] = data_filled[\"week_start\"].apply(lambda x: x.year)\n",
    "data_filled[\"month\"] = data_filled[\"week_start\"].apply(lambda x: x.month)\n",
    "data_filled[\"week_of_month\"] = data_filled[\"week_start\"].apply(lambda x: week_of_month(x))\n",
    "data_filled[\"day\"] = data_filled[\"week_start\"].apply(lambda x: x.day)\n",
    "data_filled.drop(\"week_start\", axis=1, inplace=True)\n",
    "\n",
    "# Create other features (lagged features, moving averages, etc.)\n",
    "features = data_filled.groupby([\"store\", \"brand\"]).apply(\n",
    "    lambda x: combine_features(x, [\"move\"], lags, window_size, used_columns)\n",
    ")\n",
    "train_fea = features[features.week <= bs.TRAIN_END_WEEK_LIST[r]].reset_index(drop=True)\n",
    "print(\"Maximum training week number is {}\".format(max(train_fea[\"week\"])))\n",
    "print(\"\")\n",
    "\n",
    "# Drop rows with NaN values\n",
    "train_fea.dropna(inplace=True)\n",
    "print(train_fea.head(1))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-22-2f1a0b0101cc>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-2f1a0b0101cc>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    .sort_values(by=idx_cols)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting models...\")\n",
    "# Create training set\n",
    "dtrain = lgb.Dataset(train_fea.drop(\"move\", axis=1, inplace=False), label=train_fea[\"move\"])\n",
    "if r % 3 == 0:\n",
    "    # Train GBM model\n",
    "    print(\"Training model...\")\n",
    "    bst = lgb.train(params, dtrain, valid_sets=[dtrain], categorical_feature=categ_fea, verbose_eval=False)\n",
    "\n",
    "# Generate forecasts\n",
    "test_fea = features[features.week >= bs.TEST_START_WEEK_LIST[r]].reset_index(drop=True)\n",
    "idx_cols = [\"store\", \"brand\", \"week\"]\n",
    "pred = (\n",
    "    make_predictions(test_fea, bst, target_col=\"move\", idx_cols=idx_cols)\n",
    "    .sort_values(by=idx_cols)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "# Additional columns required by the submission format\n",
    "pred[\"round\"] = r + 1\n",
    "pred[\"weeks_ahead\"] = pred[\"week\"] - bs.TRAIN_END_WEEK_LIST[r]\n",
    "print(pred)\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate prediction accuracy\n",
    "test_df = pd.read_csv(os.path.join(TEST_DIR, \"test_round_\" + str(r + 1) + \".csv\"))\n",
    "test_df[\"actual\"] = test_df[\"logmove\"].apply(lambda x: round(math.exp(x)))\n",
    "test_df.drop(\"logmove\", axis=1, inplace=True)\n",
    "combined = pd.merge(pred, test_df, on=[\"store\", \"brand\", \"week\"], how=\"left\")\n",
    "metric_value = MAPE(combined[\"move\"], combined[\"actual\"]) * 100\n",
    "print(\"\")\n",
    "print(\"MAPE of current round is {}\".format(metric_value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Plot feature importances...\")\n",
    "ax = lgb.plot_importance(bst, max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecast results\n",
    "results = pd.concat([combined], axis=0)\n",
    "random.seed(2)\n",
    "for _ in range(5):\n",
    "    store = random.choice(results[\"store\"].unique())\n",
    "    brand = random.choice(results[\"brand\"].unique())\n",
    "    plot_result(results, store, brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "\n",
    "\\[1\\] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems. 3146â€“3154.<br>\n",
    "\\[2\\] Alexey Natekin and Alois Knoll. 2013. Gradient boosting machines, a tutorial. Frontiers in neurorobotics, 7 (21). <br>\n",
    "\\[3\\] The parameters of LightGBM: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst <br>\n",
    "\\[4\\] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. 2018. CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363 (2018).<br>\n",
    "\\[5\\] Scikit-learn. 2018. categorical_encoding. https://github.com/scikit-learn-contrib/categorical-encoding<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('forecast': conda)",
   "language": "python",
   "name": "python361064bitforecastconda28a2c7cb2c2344ae9086eefcd0f55e3e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
